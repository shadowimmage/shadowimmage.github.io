[{"categories":null,"contents":" I had to set up an Ubuntu VM (Virtual Machine) on my machine recently and because I also use Docker, I had to learn exactly how Hyper-V handles networking tasks between the host machine and the virtual machines you set up. Hyper-V, as far as I understand, has 3 main networking methods called virtual switches. There\u0026rsquo;s actually a 4th option that comes out of the box, which is something of a hybrid between an \u0026ldquo;Internal Switch\u0026rdquo; and an \u0026ldquo;External Switch\u0026rdquo;. The default switch is interesting, and not particularly intuitive, but after getting used to it, I find it really convenient, if a bit tedious to deal with. It is absolutely not something I recommend running anything in for long periods of time, or for anything upon which other people rely. It is, however, good for getting something set up quick and dirty - so long as you understand what it\u0026rsquo;s doing.\nFor the length of this article, it will be good to understand what a switch is, so read this Wikipedia article if you are unfamiliar.\nSo with that, here\u0026rsquo;s a quick overview of what the Hyper-V switches are and can do.\nFirst, there\u0026rsquo;s the Internal Switch. This is a virtual switch that allows Host\u0026lt;-\u0026gt;Guest connections with static IP addressing allowing inter-machine communication. The downside of this, however, is that the Guest OS is isolated from the wider internet that the Host may be connected to.\nSecond, there\u0026rsquo;s the External Switch. This is a virtual switch that is not connected to the Host directly, and consumes a physical host network interface. This interface is given over to the exclusive use of any VM set up to use the External Switch. The benefit of this is that all VMs on that switch can talk to each other, and can communicate to whatever is connected to that physical network, which may also be the same physical network as your Host (via an external switch, routing, etc. and separate physical network interface). If you have two network interfaces for your computer, then this may be a good solution for long term use. Other benefits include: Static IP addressing of your guest virtual machines, the VMs behave exactly as if they had their own wired (or wireless) network connection to the rest of your physical network, and internet connections are thus possible to and from the Guest machines. Basically, this is just like having real physically separate computers sitting on your desk, with the caveat that you need more than 1 physical network interface (which I don\u0026rsquo;t currently have).\nLastly, there\u0026rsquo;s the Private Switch. This is an interesting situation, and I can only think of a couple uses for it, but what it does is only allow VM\u0026lt;-\u0026gt;VM communication. There\u0026rsquo;s no way for Host\u0026lt;-\u0026gt;VM or Internet\u0026lt;-\u0026gt;VM connections with this switch. I think the only situation you\u0026rsquo;d want to use this is for VM-VM relay communication, where each VM has more than 1 virtual network connection enabled, such as a Router VM, Firewall VM, and other(s), where 1 VM is connected to the internet, relays traffic to an isolated Firewall VM, which then allows traffic through to even deeper VM(s) that are thus protected via layers of networking, without the need for a rack full of hardware. This might theoretically be a useful application for a clustered network simulation or something where everything is isolated from external interference, but that\u0026rsquo;s beyond my interest or skill.\nThe \u0026ldquo;Default Switch\u0026rdquo; - Getting Up and Running Without Configuring a Bunch of Stuff The thing that makes the Default Switch work well is that it is the only version of virtual switch that Hyper-V has that has NAT or Network Address Translation. The default switch behaves somewhat like a home router, allowing Guest VMs internet access, as well as Host\u0026lt;-\u0026gt;VM communication, without the hardware requirements or setup needs of an External Switch. It also does all the work for you, but also doesn\u0026rsquo;t allow you to take control of it. What this means is that you cannot set static IPs for your VMs. Hyper-V gives them DHCP addresses in a limited range, which is randomly selected in the 192.168.xxx.xxx/28 space. The Host can communicate with services running on a Guest at the given IP address that the VM is given, and the Guest(s) can communicate with the Host at it\u0026rsquo;s external IP address via NAT. Guests will get new random IP addresses every (or almost every) time the VM is rebooted, and each time the Host is rebooted, Hyper-V will choose a new random local address space to set up it\u0026rsquo;s DHCP range in, before passing those off to the guest VMs. You also can\u0026rsquo;t disable or remove the Default Switch (as far as I can tell) - but it won\u0026rsquo;t hurt anything being there. If you don\u0026rsquo;t use it, just ignore it.\nSo, how to get things talking to each other?\nStep 1: Gather information. You need to retrieve 2 things: Your Host computer\u0026rsquo;s IP address. This is what you computer is addressed according to your company / home network if another physical computer on that network were to ping it. For me, it\u0026rsquo;s 192.168.1.10 at home and 10.155.43.82 at work. Then you need to get the IP of your VM. In this case I just opened the ethernet settings in Ubuntu and checked the assigned IP address. You can also get this information from the Hyper-V manager window to see what IP your VM got assigned. At the time of taking notes, this was 192.168.18.187.\nStep 2: Set up services on VM: Say you want to run a test web server on your VM and access it from your Host\u0026rsquo;s browser. You\u0026rsquo;ll need to tell your server to bind to the VM IP at whatever port you want to use, for instance 192.168.18.187:8000. For a Django server, I\u0026rsquo;d run: python manage.py runserver 192.168.18.187:8000. Then (if it successfully starts up) you can go to your Host browser and go to that address and the Default Switch should route your traffic to the VM running that webserver.\nFrom the VM, if you want to communicate with the Host services (say, a Docker image running PgAdmin; or your PostgreSQL server that\u0026rsquo;s running on the Host), you can address them from the VM by using your Host\u0026rsquo;s external IP address. For the above example, my Django web server running in my Ubuntu VM, would communicate with my Host\u0026rsquo;s PostgreSQL database server at 10.155.43.82:5432.\nNext time you reboot things, just check your IP addresses, and reconfigure your settings before starting up services again.\n","permalink":"https://chasesawyer.dev/post/2019/05/hyper-v-local-networking/","tags":["post","hyperv","networking","virtualization"],"title":"Hyper-V Local Networking"},{"categories":null,"contents":" At the end of February 2019, Google released general access to the .dev top level domain. I had heard about this happening about a year ago, and am now the happy owner of two .dev domains! One of these is chasesawyer.dev and will soon be the new home of this site!\nBut how to get it set up? When you buy a domain name, nothing is really set up for them - going to those addresses doesn\u0026rsquo;t point to anything Firefox and Google will just say \u0026ldquo;Server not found.\u0026rdquo; So here\u0026rsquo;s how to change that.\nTheres a few things I want to set up for these domains, and I\u0026rsquo;m going to go through the steps for each. This is also part one of a series of posts on how to accomplish these tasks. Other parts will be linked when they\u0026rsquo;ve been figured out and done!\n Custom domain setup for this site, while maintaining hosting via Github Pages Email forwarding to ProtonMail Custom domain setup for my apps on Heroku  Some Quick Assumptions I\u0026rsquo;m going to be making some assumptions throughout this article, most of which can probably be translated to other platforms, but your mileage may vary.\n You\u0026rsquo;re using a .dev domain - part of the HSTS list (this will come up later) You bought your domain, are administering your domain through Google Domains (rather than godaddy or namecheap or something else - those are probably fine, I just went with Google Domains for expediency when they launched .dev)  GitHub Pages and Custom Domains This first one is pretty straightforward - you need to do two things:\n Determine if you want your GitHub Pages site to be at the root of your custom domain (ie, going to chasesawyer.dev without any prefixes) or if you want it to be at a subdomain (ie, blog.chasesawyer.dev or pages.chasesawyer.dev) Create a the appropriate DNS record on google domains (domains.google.com/m/registrar/{yourdomainname}/dns) under \u0026ldquo;Custom resource records.  For the former case (Apex domain), see setting up an apex domain. For this use case, the way that worked best for me was not by using forwarding, but by using A records in the Google Domains DNS settings for my domain name. For the latter case (subdomain), see setting up a custom subdomain  Update the custom domain option on your GitHub Pages repository to match the domain that you\u0026rsquo;re using. If you don\u0026rsquo;t do this, then your site won\u0026rsquo;t load properly, and, since the .dev domain can only be used via HTTPS, you\u0026rsquo;ll have security problems with your site! (more below) Wait for a little while - your DNS settings need some time to propagate through the DNS system, so that requests to {yourdomain}.dev will properly redirect to your GitHub Pages site. GitHub Pages also has to have time to set up a certificate for your new site.  Security, HTTPS, Certificates If, like me, you enjoy using GitHub Pages for hosting your site, you\u0026rsquo;ve enjoyed having your site having an https address, and all the security baked into GitHub Pages. When you are setting up sites on a .dev domain since it\u0026rsquo;s part of the HSTS preload list in most modern browsers, your whatever.dev site can only be loaded via https. This means that you\u0026rsquo;ll need a security certificate for the site you set up on .dev! But how do you do this if you\u0026rsquo;re using GitHub Pages? You don\u0026rsquo;t have access to their webserver, so the Let\u0026rsquo;s Encrypt certbot won\u0026rsquo;t work for you. But you need a security certificate for your domain!\nWhat isn\u0026rsquo;t really explained anywhere where I can see, is that this process of getting a security certificate through Let\u0026rsquo;s Encrypt for your GitHub Pages site at your custom domain is completely automated! That\u0026rsquo;s right! GitHub Pages just does it for you. This is why step 4 above takes a little while to complete - whatever mechanism at GitHub that serves up pages from your github.io pages site that gets set up on {your custom domain} needs to have a security certificate set up, and that system takes care of getting one through Let\u0026rsquo;s Encrypt.\nThis is also why I decided to go through with setting up this site with A DNS records, because for whatever reason, DNS forwarding didn\u0026rsquo;t work properly, and didn\u0026rsquo;t trigger whatever automated system that sets up a security certificate for your custom domain served through GitHub Pages.\nWill this cause problems later on? I am not sure yet. I haven\u0026rsquo;t set up subdomain records on Google Domains yet (like {something}.chasesawyer.dev) but that is gonna be the subject of my next post!\n","permalink":"https://chasesawyer.dev/post/2019/03/google-.dev-domains-github-pages-and-heroku-apps/","tags":["post","blog","dev","heroku","google","domains","dns","security","hsts"],"title":"Google .dev Domains, GitHub Pages, and Heroku Apps"},{"categories":null,"contents":" I have been working on the backend for a project that I\u0026rsquo;ve written about {{previously\u0026ndash;link}}. The established tools server that will be supporting my new React frontend app(s) will be using a backend built on Python 2.7 and Django 1.11, and thus I\u0026rsquo;ve had to remember how to get a development environment set up that will appropriately support the project running locally on my machine. I have a personal site that runs on the same version of Django, but with Python 3.6 as the underlying code base. Having used these tools previously, I decided that it was time to bump my personal Django instance up to something more modern and learn what it would take to get my apps-demos server (available at chase-sawyer-demos.herokuapp.com) up to Django 2.1 with Python 3.7 backing it up. I also wanted to keep some notes here on what the experience has been like setting up a legacy development environment from scratch after not having worked on a project for some time or picking up someone else\u0026rsquo;s project.\nBelow are some of my notes and experiences on having gone through this process.\nThe Work Environment and Long File Names I had a further complication with this project in that some of the dependencies required were projects maintained by other groups here, were clearly developed on a MacOS or Linux environment as the file names and directories for files within these projects were too long for Windows to handle. This meant that I could not successfully clone the codebase on to my usual development computer (Windows 10 based). I dug through some documentation for Windows that stated that it can technically support longer than 260 or so characters, and that the NTFS filesystem has no technical reason to restrict these files from existing. However, many programs and utilities baked into Windows still use the maximum path length limit (like explorer.exe - the Windows file manager). I decided that instead of trying to hack Windows into working so that I could get a working copy of the backend server that I am using to host the API and database connections for this project, I could just use a VM. I am already using Hyper-V as part of having Docker running on my dev environment, and Hyper-V has a nearly-one-click solution to installing an Ubuntu 18.04 LTS VM on Windows, so I set that up as my designated Python 2.7 + Django 1.11 development instance so that I could handle the long paths required for this project to run successfully.\nHaving set up an Ubuntu 18.04 LTS VM on my main Windows machine, I installed git, vscode, and virtualenv for Python, along with Python27 for Ubuntu so that I could get a working virtual environment set up for this project before I cloned in the existing backend project with all it\u0026rsquo;s dependencies and get things up and running before starting my new backend app within that framework.\nBut then I remembered something - Django is built around app reusability and so I went back to my app structure and moved it out into a super basic from-scratch Django project which allows me to work on any machine to build the app, without any dependence on the existing older project structure. This approach uses Django 1.11 to match the work environment\u0026rsquo;s Django version, but uses the Python 3.7 version that I have installed on most all of my development machines - this shouldn\u0026rsquo;t be too problematic, as Django 1.11 supports Python 3 and I have been taking steps to ensure that the coding I\u0026rsquo;m doing is Python 2 and 3 compatible. I\u0026rsquo;ll edit here if this turns out to be a bad idea.\n","permalink":"https://chasesawyer.dev/post/2019/02/maintaining-older-django-and-python-projects/","tags":["post","django","python","virtualenv"],"title":"Maintaining Older Django and Python Projects"},{"categories":null,"contents":" Author\u0026rsquo;s Note: this post has taken a long time to get written - so long, that I already have my 2018 Hactoberfest t-shirt and stickers! It\u0026rsquo;s content spans mid-October through December. I\u0026rsquo;ve done my best to make it a cohesive whole.\n Happy Hacktoberfest! This has been the second year that I\u0026rsquo;ve participated, and I\u0026rsquo;m looking forward to getting my t-shirt and stickers when they\u0026rsquo;re available from DigitalOcean. Really briefly, here\u0026rsquo;s a list of things that I\u0026rsquo;ve been working on and doing in the last few months since my last update.\nI\u0026rsquo;ve started working through the project prompts on The Odin Project - Still finding it difficult to stick with these self-directed learning sites. The advantage I think that The Odin Project has over Free Code Camp is that their projects are more easily shown off to other people. So far I\u0026rsquo;ve managed a couple project solutions without having to set up anything other than the GitHub repository where my fork\u0026rsquo;s code lives. The project code is all served from GitHub pages by having a gh-pages branch pointing to the desired live version of the page project.\nThe following have been completed:\n etch-a-sketch Rock-Paper-Scissors  I\u0026rsquo;ll admit, it\u0026rsquo;s not a terribly impressive list. But in my defense, work has come up with a way to keep me thoroughly engaged by giving me the capacity to pitch, prioritize, and execute upon projects of my own to improve processes and practices. What this has boiled down to is:\n 4 projects pitched in August/September 4 approved projects 1 active project in early development / preparation stage (described below) 3 projects scheduled / prioritized for execution following active project 1 developer (me)  Given the constraint of me as Project Manager, Lead Developer, and Architect, I\u0026rsquo;ve been pretty busy taking on one of the heaviest lifts I\u0026rsquo;ve ever engaged in professionally: bring a year-old React app up to date, with a database backend (from scratch), and analytics/live dashboards. You might think that this seems pretty easy, so let me explain further the current state-of-affairs.\nThe Active Project The react app is served statically from a generic managed LAMP server, which I have no access to. The server\u0026rsquo;s resources are behind the UW\u0026rsquo;s NetID authentication, but the actual React site has nothing to do with this, you simply must be authenticated in order to load the app at all.\nThere is no backend in any sense that I\u0026rsquo;ve ever thought of as a \u0026ldquo;backend\u0026rdquo;. It\u0026rsquo;s Google Sheets. All of it. Dynamic elements of the React site are all fields set up in various tabs of a single Google Sheets spreadsheet. Changing values in particular cells of the spreadsheet change the values displayed in the React app. The React app then uses those values to modify forms and dynamically change so that users can submit information about classroom checks (problems and details about them, or everything is fine). This information populates a \u0026ldquo;form responses\u0026rdquo; tab in that same spreadsheet, which then is checked by a formula in another tab, and that subsequently updates the React app.\nWhat black magic makes all this data go around without collapsing? Firebase. And Google Apps Scripts.\nSo Google Apps Scripts allow you to use Javascript to modify Google Docs, Sheets, etc. with custom functions and the like (such as macros). These scripts (can) have triggers, which can be time based (I\u0026rsquo;d say, cron-adjacent) or based on actions that happen with the associated Google Sheet or Doc. Firebase is a no-sql document object store with some really fancy real-time update functionality built into their libraries that provide some attractive quality-of-life features to app developers - specifically, updates to date in Firebase are automagically synced to all online clients, and any offline changes from clients are cached locally and later synced when that client goes back online.\nIn our case, Google Sheets holds all the data, any change to any part of the document triggers an Apps Scripts function that uploads all te watched cells to Firebase. Those changes are live-synced to any clients watching (clients being those with the main React app loaded). These triggers are dependent on user interaction. The opposite direction data-flow (the React app to Google Sheets) is not realtime. Or not quite. Form data filled out in the React app is uploaded to Firebase into a sort-of update queue. This is where data lives until cleared out by another Apps Script attached to this Google Sheet. This function has a time-based trigger that runs as often as Google allows: every minute of every day forever. Each minute (or so), this script checks the Firebase database (\u0026lsquo;queue\u0026rsquo;) to see if there\u0026rsquo;s been a change to the queue, and if so, it pulls that data and populates a new row of the form responses tab, then clears that data from the Firebase queue, which causes another update to Firebase, because that new incoming data changed the Sheets, which means the Firebase data needs updating, which will cause an update to any listening React clients.\nIt sounds complicated, and it is. Learning how everything was put together in the first place took some time to learn, and then maintaining it is another battle entirely. The whole apparatus depends on triggers associated with the Google Apps Scripts, which are directly attached to their relevant Google Apps document.\nSome more background: Google Apps scripts can be either standalone, or they can be attached to a document. If they are standalone, then they exist and behave much like other Google documents or spreadsheets (they show up like another ). They are little files that exist somewhere. If they are attached to a document, then those scripts do not actually have any independent identity. They are intrinsically linked to whatever document that they were created in. This means that they are not easily found for shared documents, like the ones that we are dealing with here. They also have a peculiar association with other Google Cloud applications and functions, in that they appear to be projects, and have similar properties, but you cannot see the engine or virtual machine they run within, nor are they cloud functions per se, and their API usage doesn\u0026rsquo;t necessarily show up in the Cloud Console. (This could just be my misunderstanding how these things work, but this has been my observation thus far). To edit scripts, you need to find them in the Google Apps Scripts developers console (G Suite Developer Hub), or by first finding the document that they\u0026rsquo;re attached to and then opening that document\u0026rsquo;s scripts.\nNow, about those triggers: until very recently there was no way (that I could find) to find the triggers associated with a shared document/spreadsheet attached apps script, unless you were the user that created that trigger. Now in the last couple of weeks (since November 20th or so) they allow other users that can see the shared document/script to see that there are triggers, but not who that user is or a whole lot of detail about the trigger parameters. (YMMV - this is taken from the perspective of a user within an organization (the UW) where documents are shared amongst many users (within one of our Team Drives)). When I first began looking into taking over this project, I couldn\u0026rsquo;t see any triggers attached to any of the functions that were associated with our master Google spreadsheet, so to me, it was clear that these functions were happening, but I couldn\u0026rsquo;t see how. This led to a failure for our crew one night, since (for reasons unknown) the magical invisible triggers just stopped working. Now, this probably wasn\u0026rsquo;t the case, but since I could never see the triggers responsible for all the data moving back and forth, I also couldn\u0026rsquo;t see that they weren\u0026rsquo;t there or if they were broken in some other way. I had anticipated this, and the resolution was just to re-create these triggers under my own UW Google account.\nGoing forward, this is fine, but it brings me back to another fundamental flaw in this kind of system: Transitioning these resources over to a new team member/user. Triggers can\u0026rsquo;t be moved/transferred/shared/etc. And similarly Scripts that are associated within a Googel Doc or Sheet are intrinsically tied to it. They can\u0026rsquo;t move away from that document, and if that document is deleted, then the Script associated with it goes too. Making your scripts separately from your G Suite applications seems like a wise decision from the outset, but that wasn\u0026rsquo;t immediately clear when these scripts were initially made. Of course copy-paste makes moving their content to local files or to independent Scripts on Google would ostensibly be a valid way of breaking this interdependency, it\u0026rsquo;s still no Git. And of course today I used this very functionality to create two Slack Custom Integrations that run within this script framework with daily triggers to create some automated notifications for staff. These scripts are trivial so I feel no particular attachment to making sure they don\u0026rsquo;t disappear suddenly, but on the other hand, for something more mission-critical, it\u0026rsquo;s more concerning.\nLong term I\u0026rsquo;m going to work out a way of potentially deploying these Apps Scripts programmatically, so that they can live somewhere else and be deployed through some kind of automated system? (looking at you Github Actions\u0026hellip;)\nOngoing Development Ok, so here\u0026rsquo;s a small list of the things that need to happen (completely out of order, some already completed): Write a project charter, and try to anticipate the speed at which decisions will be made within a bureaucratic government organization during the holiday season). Update Create-React-App (and React), all their dependencies, the Google Firebase library, and any other utility libraries. Audit the list of packages in package.json and determine what can stay and what is extraneous. Add eslint and settle on some rules to bring some order to the code base. Update React components to be more modular - break out what needs separation, cull redundancy, and prepare the app that is using Firebase as it\u0026rsquo;s backend ready to switch over to a SQL database for a backend. Build out the architecture of the SQL database. Build an API between the database and the app. Build a Tableau dashboard and report set that pull from the SQL database. Build in Business Logic everywhere to ensure the core data in the SQL database is and remains valid into the future. Clean up and transition legacy data over to the SQL Database. Decide on what flavor of Database to have in the first place. Figure out how to get the app to be integrated into the UW NetID (Shibboleth) SSO, and use information about the users to control what users can and cannot do, including who can manage data in the SQL database (via a manager view within the app). Build a manager interface within the app.\nIt\u0026rsquo;s an ambitious project, but one that I feel like I have all the pieces for, and the actual plumbing of all the parts and figuring out how they all go together is the most fun and enjoyable part of all of this.\n","permalink":"https://chasesawyer.dev/post/2018/10/long-time-update-new-projects/","tags":["post","update","uw","cloud"],"title":"Long Time Update; New Projects"},{"categories":null,"contents":" When developing an application or project with Node.js, debugging is an important aspect to getting to the bottom of issues. If you use an IDE (integrated development environment) or code editor that has built-in or supported debugging capacities, such as Visual Studio Code, then you have that ability already. But if you are someone who\u0026rsquo;s developing with a more basic code editor - such as Sublime Text, then you can still debug your Node.js application, using Chrome Developer Tools as your debugger, as if you were debugging a live website.\nBackground - Node Debugging Node creates a server and runs your application. When debugging, you need a client to talk to the server to tell it when to stop execution, catch errors, and to set/catch breakpoints in the code. This is called an inspector client, and that interface is what is presented to you in the Chrome Developer Tools as well as when using the debugger mode in IDEs.\nSet up For this, you can use any Node server app. We\u0026rsquo;ll be launching the app from the command line, and then open up Chrome and connect to the running Node server, which will be running in debug mode. Initiating the app in this way, it won\u0026rsquo;t actually begin execution until the inspector client (Chrome) connects to Node server.\nStart Node in Debug mode If you normally start your Node project server with something like\nnode app.js Open a terminal in your Node project folder, and start it with the command\nnode --inspect-brk app.js You should then be greeted with a message like this:\nDebugger listening on ws://127.0.0.1:9229/9b3ec4f2-b590-4372-b34c-1s4affc3a345 For help see https://nodejs.org/en/docs/inspector Then open chrome to the address chrome://inspect\nFrom here, you should see your Node instance listed, including the file path to the app that\u0026rsquo;s running. Click the \u0026ldquo;inspect\u0026rdquo; link below the file directory shown, and Chrome DevTools will open up. In the console, you should now see\nDebugger attached. At this point, your code still hasn\u0026rsquo;t actually begun running yet. DevTools has attached to the Node debugging instance, and has frozen your code at the very beginning of app.js. DevTools will now let you explore your code, and insert breakpoints - places where you want code to halt - before it starts executing your code.\nFrom here you can begin to debug your code to your heart\u0026rsquo;s content, and dig into pesky bugs that might be plaguing you. Using an interactive debugger is also very useful for catching the state of your app when it crashes (runs into an exception) as the debugger will halt at that point, and you can inspect the state of your variables and dig deeper into what may have gone wrong to cause the exception.\nFor a more in-depth guide on debugging and how to navigate Chrome\u0026rsquo;s DevTools, check out the links below:\n https://developers.google.com/web/tools/chrome-devtools/ https://nodejs.org/en/docs/guides/debugging-getting-started/  For VS Code:\n https://code.visualstudio.com/Docs/editor/debugging  As a side note, most modern browsers have debugging tools, and they all behave pretty similarly, with the exception of mobile browsers - mobile browsers (to my knowledge) don\u0026rsquo;t come with the ability to debug the code running on them. But if you are debugging a mobile page, then you can run a page as if in a mobile browser from within Chrome or Firefox by opening the DevTools and entering Responsive Design mode. Check out https://developer.mozilla.org/en-US/docs/Tools/Responsive_Design_Mode for Mozilla\u0026rsquo;s implementation of this tool, and how it can be helpful in quickly testing your site\u0026rsquo;s reaction to small screens.\n","permalink":"https://chasesawyer.dev/post/2018/06/debugging-node-applications/","tags":["post","blog","node","javascript","debugging"],"title":"Debugging Node Applications"},{"categories":null,"contents":"","permalink":"https://chasesawyer.dev/post/2018/06/file-paths-in-node/","tags":["post","blog","express","javascript","node","filesystem","debugging"],"title":"File Paths in Node"},{"categories":null,"contents":"","permalink":"https://chasesawyer.dev/post/2018/06/exercise-tracker-full-stack-app/","tags":["sample","javascript","freecodecamp","mongodb","express","pug"],"title":"Exercise Tracker Full-Stack App"},{"categories":null,"contents":"Word processors are great for creating documents with a lot of rich formatting, but all that stuff can be a huge distraction. Code editors on the other hand are amazingly good for writing - the writing where the only thing that actually matters is the content, and not the format.\nWhile it\u0026rsquo;s probably been hashed out and written about a lot by other people, the one thing, the killer aspect of it for me, is the ability to scroll the page down, so that whenever you want, you can always return to that feeling of having a blank canvas to write upon. I really love the blank page, and I love writing on something fresh, and so far, code editors are the only things that I\u0026rsquo;ve seen do this. The word processors - think Google Docs, Microsoft Word, etc. - all stop you at the bottom of the current virtual piece of paper you\u0026rsquo;re writing on and won\u0026rsquo;t let you progress until you flow over onto the next page. Now, of course there\u0026rsquo;s hacks around this, including just inserting a page break or two at the end of whatever it is you\u0026rsquo;re working on. However, this has always felt really artificial to me. Plus there\u0026rsquo;s always those page breaks and virtual pieces of paper that physically break apart your writing. It can be distracting.\nBut to be fair to word processors, I don\u0026rsquo;t think anyone writes their blog posts in them, and I don\u0026rsquo;t think that anyone writes their papers that are meant to be printed in a blog editor. There are always exceptions, of course, such as those that use LaTex for all their typesetting needs (I don\u0026rsquo;t really know much about LaTex other than that it exists and that it is very useful for document formatting, and printing math equations).\n","permalink":"https://chasesawyer.dev/post/2018/06/code-editors-vs.-word-processors/","tags":["post","blog","nonesense"],"title":"Code Editors vs. Word Processors"},{"categories":null,"contents":"","permalink":"https://chasesawyer.dev/post/2018/06/new-free-code-camp-cert/","tags":["post","blog","update","certificate"],"title":"New Free Code Camp Cert"},{"categories":null,"contents":" Repair Task Tracker RTT is a full stack app that addresses the needs of a computer hardware management process, allowing the tracking and resolution of issues/problems with the hardware, as well as the configuration and components of each major hardware item. RTT is meant to be a back-of-house tool, replacing paper tickets and spreadsheets. The goal of this project was to implement a GraphQL app, with useful data, allowing a seamless user experience as they operate through the app, and data is downloaded and uploaded in the background.\nDevelopement Status This project is ongoing development. Progress details are in the project readme.\nTechnologies RTT is a single-page-app built in Vue.js and served from a Python/Django server running within a Heroku dyno (server instance). The data records are stored in a PostgreSQL database, retrieved with a GraphQL implementation using Apollo and Graphene.\nDemo The backend database schema has been completed; the frontend is still under development. The latest snapshot is live on heroku.\n  Display of all open issues in the database\n      Detail page and update fields for selected issue\n     ","permalink":"https://chasesawyer.dev/project/rttapp/","tags":["project","python","heroku","vue","javascript","postgresql","graphene","apollo","graphql"],"title":"RTTApp"},{"categories":null,"contents":" The Keys App was my first major project that sought to solve a problem with managing key checkouts without requiring a cumbersome customer database / sign up form. This would target an institution that mostly catered to internal customers, such as a university\u0026rsquo;s AV department and it\u0026rsquo;s instructors. It largely replicates a paper-form-based system, with an added layer of data validation and control (emails, phone numbers must be in a valid format; keys can only be returned by their original owners; keys can\u0026rsquo;t be checked out twice; etc.). Each checkout and check-in is recorded and tracked. All changes in the database leave a history.\nThis project was built entirely upon Django, with some Bootstrap for front-end styling. The app runs as a multi-part form with several steps and each is validated before progress is allowed. The backend leverages the built-in Django admin console and allows navigating and editing all records in the system, while also maintaining data integrity.\nTechnologies The Keys App is a full stack project built with\n Python/Django PostgreSQL Bootstrap FontAwesome  Database Schema [To come]\nDemo Hosted on Heroku: https://chase-sawyer-demos.herokuapp.com/keysApp/\n    ","permalink":"https://chasesawyer.dev/project/keysapp/","tags":["project","demo","django","python","postgresql","heroku"],"title":"KeysApp"},{"categories":null,"contents":" I spent my sick day today designing a new logo for the site using Inkscape. I knew that it had to be something that was unique, so I went online looking around for resources on designing and producing a logo. I know that SVGs are great, because you will never have scaling issues - if you need a larger version, you can simply export it at higher resolution. Or lower. Or whatever. I found some sites for inspiration as well, since I couldn\u0026rsquo;t think of exactly what I was going for initially.\nThe new design: logobook.com has been really helpful in looking at what kinds of designs you can do with different features or lettering and Logo Design Love was really helpful in getting my head into a good space for designing a logo. Other than that, the first few iterations of designs were just pen-and-paper drawings in my notebook.\nTools Designing a vector graphic is relatively easy if you have a vector graphic editing program. I have known about Inkscape for a while, and this is what I ultimately used. Another tool I found while looking around for vector graphic tools is a browser-based (or downloadable) tool called Vectr. I didn\u0026rsquo;t use Vectr much, but it looks useful if you need really simple tools and don\u0026rsquo;t want to get something as heavyweight as Inkscape. That being said, using Inkscape and it\u0026rsquo;s exceptional grid and guide snapping tools really made designing this logo easy.\nGetting it online In order to fully implement use of the logo, there\u0026rsquo;s several modifications that I had to make, including converting the .svg file to an .ico or .png format that would be browser friendly. Modern browsers support .svg files for page graphics, but most still don\u0026rsquo;t support the format in the favicon space. Only Internet Explorer requires the format be a .ico for the favicon, and most support .png, but in different levels of quality, depending on the browser and age, and if it\u0026rsquo;s a mobile or a desktop browser (not to mention things like consoles with browsers baked in). I didn\u0026rsquo;t know any of that information starting out though, and instead got a good amount of information from this stackoverflow question.\nChanges to Head \u0026lt;!-- new favicon --\u0026gt; \u0026lt;link rel=\u0026#34;apple-touch-icon\u0026#34; sizes=\u0026#34;180x180\u0026#34; href=\u0026#34;{{ \u0026#34;img/apple-touch-icon.png\u0026#34; | absURL }}\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;icon\u0026#34; type=\u0026#34;image/png\u0026#34; sizes=\u0026#34;32x32\u0026#34; href=\u0026#34;{{ \u0026#34;img/favicon-32x32.png\u0026#34; | absURL }}\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;icon\u0026#34; type=\u0026#34;image/png\u0026#34; sizes=\u0026#34;16x16\u0026#34; href=\u0026#34;{{ \u0026#34;img/favicon-16x16.png\u0026#34; | absURL }}\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;manifest\u0026#34; href=\u0026#34;{{ \u0026#34;img/site.webmanifest\u0026#34; | absURL }}\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;mask-icon\u0026#34; href=\u0026#34;{{ \u0026#34;img/safari-pinned-tab.svg\u0026#34; | absURL }}\u0026#34; color=\u0026#34;#5bbad5\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;msapplication-TileColor\u0026#34; content=\u0026#34;#da532c\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;theme-color\u0026#34; content=\u0026#34;#ffffff\u0026#34;\u0026gt; \u0026lt;!-- end favicon --\u0026gt; This required some minor modifications to the Hugo theme that I\u0026rsquo;ve been building off of by changing part of the navbar to include the new monogram svg. Getting the sizing just right required some custom css, which also had to be included in the html head. I also changed the Hugo params in the config file to reflect that the avatar is an avatar, not an icon, and instead use the icon parameter for the file location of the monogram.\nChanges to Nav \u0026lt;div class=\u0026#34;navbar-header\u0026#34;\u0026gt; + \u0026lt;a href=\u0026#34;{{ \u0026#34;\u0026#34; | absLangURL }}\u0026#34;\u0026gt; + \u0026lt;img class=\u0026#34;navbar-brand\u0026#34; id=\u0026#34;site-logo\u0026#34; src=\u0026#34;{{ .Site.Params.logo | absURL }}\u0026#34; alt=\u0026#34;{{ .Site.Title }}\u0026#34; /\u0026gt; + \u0026lt;/a\u0026gt;  \u0026lt;button type=\u0026#34;button\u0026#34; class=\u0026#34;navbar-toggle\u0026#34; data-toggle=\u0026#34;collapse\u0026#34; data-target=\u0026#34;#main-navbar\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;sr-only\u0026#34;\u0026gt;{{ i18n \u0026#34;toggleNavigation\u0026#34; }}\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026#34;icon-bar\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026#34;icon-bar\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026#34;icon-bar\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;/button\u0026gt; \u0026lt;a class=\u0026#34;navbar-brand\u0026#34; href=\u0026#34;{{ \u0026#34;\u0026#34; | absLangURL }}\u0026#34;\u0026gt;{{ .Site.Title }}\u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; ","permalink":"https://chasesawyer.dev/post/2018/05/logo-design/","tags":["post","hugo","update"],"title":"Logo Design"},{"categories":null,"contents":"","permalink":"https://chasesawyer.dev/post/2018/05/file-metadata-microservice/","tags":["sample","freecodecamp","javascript","demo"],"title":"File Metadata Microservice"},{"categories":null,"contents":"","permalink":"https://chasesawyer.dev/post/2018/05/image-search-abstraction-layer/","tags":["sample","freecodecamp","javascript","mongodb","demo"],"title":"Image Search Abstraction Layer"},{"categories":null,"contents":"The challenge here was to create a URL Shortener microservice. It uses a database to associate a short url ID with the original url, and once created, the microservice will redirect visitors of the short URL to the original URL.\nExample creation input:\n https://fcc-challenges.herokuapp.com/shortener/new/https://www.google.com https://fcc-challenges.herokuapp.com/shortener/new/http://foo.com:80  Example creation output:\n{ \u0026#34;original_url\u0026#34;:\u0026#34;http://foo.com:80\u0026#34;, \u0026#34;short_url\u0026#34;:\u0026#34;https://fcc-challenges.herokuapp.com/shortener/8170\u0026#34; } Usage:\n https://fcc-challenges.herokuapp.com/shortener/2871  Will redirect to:\n https://www.google.com/  Demo View this code live on Heroku at fcc-challenges.herokuapp.com/shortener/\u0026hellip;\n URL Creation: new/https://www.google.com Retrieval: 2871  Code View on GitHub\u0026nbsp;\u0026nbsp; Shortener // URL Shortener (part1) - Short URL Creator exports.new = function (req, res) { var resData = { original_url: \u0026#39;invalid URL\u0026#39;, short_url: null } resData.short_url = req.hostname + \u0026#39;/shortener/\u0026#39; // console.log(req.url)  var url = req.url.slice(5) // console.log(req.url.slice(5))  if (validUrl.isUri(url)) { resData.original_url = url var collection = req.app.dbConn.getDB().collection(shortUrlCollection) var lastDoc = collection.find().sort({ index: -1 }).limit(1) lastDoc.project({_id: 0, index: 1}).toArray(function (err, documents) { if (err) console.error(err) var insertIndex = 1 if (documents.length \u0026gt; 0) { // console.log(documents[0].index);  insertIndex += documents[0].index } collection.insertOne({ index: insertIndex, url: resData.original_url }, function(err, r) { if (err) console.error(err) resData.short_url += insertIndex res.json(resData) }) }) } else { //end valid url section  res.json(resData) } }  Resolver // URL Shortener (part 2) - Short URL resolver/redirector exports.getId = function (req, res) { if (req.params.id) { var collection = req.app.dbConn.getDB().collection(shortUrlCollection) var shortDestDoc = collection.find({ index: parseInt(req.params.id) }).project({ _id: 0, url: 1 }).toArray(function (err, documents) { if (err) console.error(err) if (documents.length \u0026gt; 0) { res.redirect(documents[0].url) } else { res.end(\u0026#39;Invalid short URL id.\u0026#39;) } }) } else { res.end(JSON.stringify({\u0026#39;error\u0026#39;:\u0026#39;invalid URL\u0026#39;})) } } ","permalink":"https://chasesawyer.dev/post/2018/05/url-shortener-microservice/","tags":["sample","freecodecamp","javascript","mongodb","demo"],"title":"Url Shortener Microservice"},{"categories":null,"contents":"The goal for this one is to get return to the user the IP address, language, and operating system of the browser/user making the request.\nCode View on GitHub\u0026nbsp;\u0026nbsp; // whoami.js // FreeCodeCamp // Backend Challenge 2 - Get requesting client IP Address exports.who = function (req, res) { var resData = { ipaddress: null, language: null, software: null } resData.ipaddress = req.ip if (req.header(\u0026#39;accept-language\u0026#39;)) { resData.language = req.header(\u0026#39;accept-language\u0026#39;).split(\u0026#39;,\u0026#39;)[0] } if (req.header(\u0026#39;user-agent\u0026#39;)) { var userAgent = req.header(\u0026#39;user-agent\u0026#39;) var lParenIndex = userAgent.indexOf(\u0026#39;(\u0026#39;) var rParenIndex = userAgent.indexOf(\u0026#39;)\u0026#39;) if (lParenIndex \u0026gt; -1 \u0026amp;\u0026amp; rParenIndex \u0026gt; -1) { resData.software = userAgent.substr(lParenIndex+1, rParenIndex-lParenIndex) } } res.json(resData) }  Demo View this code live on Heroku at fcc-challenges.herokuapp.com/api/whoami\n","permalink":"https://chasesawyer.dev/post/2018/05/request-header-parser-microservice/","tags":["sample","javascript","freecodecamp","demo"],"title":"Request Header Parser Microservice"},{"categories":null,"contents":"","permalink":"https://chasesawyer.dev/post/2018/05/timestamp-microservice/","tags":["post","sample","freecodecamp","demo"],"title":"Timestamp Microservice"},{"categories":null,"contents":"","permalink":"https://chasesawyer.dev/post/2018/05/hugo-gotchas/","tags":["post","fyi","note","help"],"title":"Hugo Gotchas"},{"categories":null,"contents":"I woke up late today, so I didn\u0026rsquo;t have time to make coffee. And instead of going to my usual place where I know there\u0026rsquo;s going to be great coffee available, I decided to get a doughnut and some coffee from the doughnut shop. The first warning that this wasn\u0026rsquo;t going to go well was when I got my travel mug back and there was coffee all over the outside of the mug. All day since then I\u0026rsquo;ve been drinking it slowly (8 hours) and it\u0026rsquo;s just been terrible. It\u0026rsquo;s really made me sad, because I paid money for this coffee, and this is Seattle. I could have gone to the Starbucks down the block from this doughnut shop, but instead decided not to in favor of making one stop, rather than two on my way into work.\nI should have just waited longer and gone to my usual place.\n","permalink":"https://chasesawyer.dev/post/2018/05/bad-coffee/","tags":["blog","coffee","fwp","short"],"title":"Bad Coffee"},{"categories":null,"contents":" Built a new site, leaving Jekyll for Hugo, for my github.io page.\nChanges Moved all my old Jekyll files to a new subdirectory in order to maintain access to the old code and posts, and then transition all the content over to the main Hugo site as settings come together. One major advantage of Hugo is that the build process is super fast, so I\u0026rsquo;ve been looking at how to integrate CircleCI with Hugo builds. Fortunately the CircleCI engineering blog has a post on how to get set up doing automated builds via CircleCI.\nCircleCI One of the caveats of using Github Pages for hosting my static content is that I have to have the site content in the master branch. I had already branched my project to start working with Hugo to the hugo branch, with the intent that this branch would eventually merge back with master. After reading a bit more about how Hugo works, it seems that the hugo branch is going to be a permanent feature, with master being overwritten with the content of the /public output from the Hugo build process. The hugo branch will remain indefinitely as the new default branch on the site repository.\nOn the other hand, this will work out fine, as it will let me ser CircleCI up to pull the current hugo branch when updated, and then upon successful build, will be able to push the resulting build back up to the repository\u0026rsquo;s master branch. This automated build process is perfect for what I\u0026rsquo;m hoping to achieve, and will let me get more experience with CircleCI, which I haven\u0026rsquo;t touched in months!\n","permalink":"https://chasesawyer.dev/post/2018/05/new-static-site-with-hugo/","tags":["post","Hugo","update"],"title":"New Static Site with Hugo"},{"categories":null,"contents":"This project was built for Classroom Technologies \u0026amp; Events at the University of Washington.\nView on GitHub\u0026nbsp;\u0026nbsp;   Background The University has been updating their course scheduling from an onsite instance of R25 (unable to link because documentation doesn\u0026rsquo;t exist on the internet anymore) - a product from CollegeNET. Part of that process involved moving the scheduling database off-site. This allowed for a new integration to our Slack instance that could contact the R25 web service, now hosted on the CollegeNET servers, rather than secured locally on one of the UW\u0026rsquo;s servers.\nTimeline I came up with the idea for this project around April 18th, 2018, after talking to some colleagues about transitioning other internal tools that leveraged the local SQL database over to the R25 web service after the system transitioned to the cloud. Our department had recently been granted access to the web service, and I had spent the earlier part of the month translating Python scripts that had been digesting local SQL results to talking to the R25 web service and digesting XML instead. I spent about a week researching the possible integrations between Slack and external apps, and decided to use a Slack Slash Command for the integration\u0026rsquo;s implementation. The first version of the app was released on May 4th.\nImplementation Full details on the README\nTo quickly get up and running I started out with AWS Lambda and getting up to speed with what was necessary to get an AWS Lambda function working on the web. This led me to Serverless as a way of accelerating development and management of AWS resources.\nThe first version of the integration used only one Lambda function, but in order to echo commands back to the user in a channel and also respond later asynchronously, two Lambda functions are required. This is because each Lambda function can only reply to the request / trigger that started it once. If the Lambda function replies to Slack to acknowledge receipt of the command, then it won\u0026rsquo;t be able to reply later once the R25 data has been retrieved and processed. If the Lambda function waits until it\u0026rsquo;s gathered and processed all the R25 data, then it might miss the 3-second window to acknowledge the Slack command, and even if it does reply within the time frame, the confirmation of the command happens after the results are returned to Slack, making the confirmation show up not only late, but out of order. Thus two Lambda functions are required: one to parse and return acknowledge the command and another to query and process the R25 web service data.\nScreenshots Invoking   Invocation\n    Help   Help\n    ","permalink":"https://chasesawyer.dev/project/slackr25bot/","tags":["project","aws","lambda","sns","slack","UW"],"title":"SlackR25Bot - UW"},{"categories":null,"contents":"I\u0026rsquo;ve been making a lot of progress on the python-LEDSerialController project. There\u0026rsquo;s been a lot to learn about how to run the original command line script with a GUI frontend. I chose to use Tk since it\u0026rsquo;s baked into Python already, and there\u0026rsquo;s nothing to configure to get it working. It doesn\u0026rsquo;t look nearly as nice as something that would come out of using a more advanced UI toolkit, but it\u0026rsquo;s also had a lower bar to entry, despite some drawbacks with Tk\u0026rsquo;s documentation. Googling around for solutions to problems as they arise has proved to be effective though.\nGetting this project to run smoothly has been a challenge because I\u0026rsquo;m using one thread to accomplish everything, which comes with the restriction that nothing can be blocking (at least not for long) without causing UI lag (bad) or causing responsiveness on the LED controller (Arduino) to lag. This is further complicated by the way that I\u0026rsquo;ve built up the Arduino code to allow it to perform animations with the LED strips and check in on the serial buffer.\nAll of this combines to create the following conditions:\n The Arduino only checks in with the computer when it\u0026rsquo;s ready for a new command in between running pattern animations, which depends on the interval setting for the last command that was sent The UI only updates while the Python thread is not blocked or doing anything long-running Updating the Arduino requires checking the computer\u0026rsquo;s serial buffer to see if the Arduino has signaled that it\u0026rsquo;s ready for the next command.  Disadvantages of doing things this way:\n A pattern animation, such as the animated rainbow, will only run for a single cycle, and then stop. More to the point, without any serial input from the host computer, the Arduino will cease to update any LEDs at all. The host computer must continually update the Arduino (controller) with what to do next, including, telling it to do the same thing over again.  Advantages of doing things this way:\n Any updates to state on the Python host program can update the controller with new information as soon as it\u0026rsquo;s ready UI to Controller update delay is minimal UI can remain responsive while the controller is busy and not ready for communication  Timing on the Tk application can be tricky in this situation: checking the serial input as often as possible can push CPU usage to 100%, and accomplishes nothing productive since the Arduino won\u0026rsquo;t be ready for updates that frequently. On the other hand, checking too infrequently will lead to stuttering in continuous patterns, but will leave more time on the host PC for keeping the UI responsive.\nThe way I solved this was to ensure that any blocking actions are only executed when absolutely necessary. In this instance, I can use the pyserial in_waiting property to know when the Arduino has sent data that needs to be checked:\ndef serial_has_waiting(self): \u0026#34;\u0026#34;\u0026#34;Return true if there is serial data in the input buffer - non-Blocking\u0026#34;\u0026#34;\u0026#34; return self.cmdMessenger.comm.in_waiting != 0 Using that method allows me to avoid going into my incoming data handling code before there\u0026rsquo;s anything in the input buffer to read:\ndef getCommandSet(self, src): receivedCmdSet = None logging.debug(src + \u0026#39;: getCommand...\u0026#39;) while (self.cmdMessenger.comm.in_waiting == 0): # blocking - here as a final check before self.c.receive() time.sleep(0.1) receivedCmdSet = self.c.receive() logging.debug(src + \u0026#39;: getCommand complete.\u0026#39;) if (receivedCmdSet[0] == \u0026#34;CMDERROR\u0026#34;): logging.error(\u0026#34;CMDERROR: \u0026#34; + receivedCmdSet[1][0]) logging.debug(receivedCmdSet) return receivedCmdSet The final piece is making a method that will be called for every cycle of the Tk mainloop() - first in main:\nif __name__ == \u0026#39;__main__\u0026#39;: try: if setup(): pre_run_commands() app.after(500, update_controller) # HERE app.mainloop() except KeyboardInterrupt: # Called when user ends process with CTRL+C stop() And within update_controller():\ndef update_controller(): \u0026#34;\u0026#34;\u0026#34;Check the LED Controller, and issue, or re-issue a command as needed\u0026#34;\u0026#34;\u0026#34; if LEDController.serial_has_waiting(): LEDController.repeat() app.after(75, update_controller) # HERE The important part here is the amount of time (in milliseconds) that the app.after() is given for the next check. Initially from main, I have it set at 500ms, since we want to get things going and allow a little time for the UI to get started before we start the serial communication with the Arduino in earnest. Later on, it\u0026rsquo;s reduced to 75ms in update_controller() so that we can have enough time to update the UI, not overburden the CPU, and also be sure to catch input from the Arduino relatively quickly (within about 75ms, which is pretty fast). This balance is fast enough that animations on the Arduino don\u0026rsquo;t perceptibly have a delay in between iterations.\nI might tune these values more, but for now things seem to be running well enough that I can focus on further development of the UI and implementing more advanced actions through the Tk GUI, and eventually (long term) start building out connections to other applications and APIs that would allow the LED strip to react to events from other applications or web services.\n","permalink":"https://chasesawyer.dev/post/2017/06/python-tk-ui-notes-project-update/","tags":["python","gui","update"],"title":"Python Tk UI Notes, Project Update"},{"categories":null,"contents":"","permalink":"https://chasesawyer.dev/post/2017/05/learning-jekyll-and-getting-started-with-github-pages/","tags":["jekyll","update","learning"],"title":"Learning Jekyll and Getting Started with Github Pages"},{"categories":null,"contents":"We have a hammer drill that has a vacuum collector attachment, and we wanted a collector that could use different kinds of filters. I took the existing dust collector and designed an exact replacement that would take any kind of 3M respirator filter as the filter element. This allows using different kinds of filters and for the filters to be replaced. The parts involved were manually reverse-engineered on paper with the device and a set of good digital calipers. The replacement parts were designed in Autodesk Fusion360 and 3D printed on a Prusa i3 MK2.\nReverse Engineering a Device Digital calipers are infinitely helpful when trying to reverse engineer or copy a physical object with 3D CAD and 3D Printing. Dimension all the parts on a drawing and then use those measurements to create the 3D model in CAD. This was a challenging project, and one of the most complex models that I\u0026rsquo;ve ever had to engineer. The result was a device that was a perfect 1:1 fit in the overall vacuum body.\nChallenges The original filter device didn\u0026rsquo;t need to have any moving parts, and the filter was build into the wall between the chamber and the outlet. For my design I had to get a round filter to attach to a rectangular shaft, and also position the connection point somewhere where the filter could be folded around into the rectangular space of the dust chamber. Using a lofted profile allowed me to blend the rectangular outlet port into a circular tube to connect with the filter body. This worked well and maintained equal or greater volume between the filter connection point and the outlet port (no pinch point for the airflow).\nThe other challenge was designing a sliding joint that would hopefully be tight enough to seal, keeping the dust contained within the chamber and maintaining vacuum pressure. It took several test prints with small cut-away sections of the sliding joint between the two halves to get the tolerances just right to be tight without being loose or impossible to slide. This was definitely helped by making sure that the 3D printer was well calibrated and consistent throughout the entire print.\nDrawings   Reference measurements taken from the original device\n      Reference measurements taken from a sample filter scan\n     Results Below is a rendering of the 3D model from Fusion360. Unfortunately I neglected to take any photos of the finished 3D print.\n    ","permalink":"https://chasesawyer.dev/project/dustcollector/","tags":["project","3d printing","uw","fusion360"],"title":"Dust Collector - UW"},{"categories":null,"contents":" Summary   Hello   I\u0026rsquo;m a software engineer living in Seattle, Washington, currently working for Academic Technologies at the University of Washington.\nMy interests span across industries and disciplines, from art to computer science, and games to business intelligence. I\u0026rsquo;ve built my skill set from a position of curiosity, using an analytical approach to problems and their possible solutions, always learning whenever I have the opportunity. I love to learn. I solve difficult problems by combining my learning and experience into novel approaches and solutions.\nI\u0026rsquo;m really interested in security, privacy, and ensuring that data is protected. Encryption is fascinating and I think that it should be accessible and usable by everyone to protect themselves and their data. I also love diving into data and using it to build automation and business intelligence tools.\n🛠 Skills Languages, Tools, \u0026amp; Operating Systems Python   Java  Javascript  Node   git  virtualenv  Linux  Windows  Arduino    Databases MySQL  MongoDB   SQL Server  PostgreSQL   GraphQL    Frameworks Vue   React   Django    BI Tools Tableau   Knime    Architecture Serverless   IP Networking  RESTful APIs   Cloud Platforms \u0026amp; Technologies AWS   Google Cloud Platform   Heroku   Continuous Integration / Continuous Delivery    Experience  Senior Computer Specialist  University of Washington   May 2016 - Present  Seattle, WA     Create business intelligence solutions and tools, informing service improvement. Develop software tools to automate processes, saving hundreds of hours of staff time. Manage communications and outreach campaigns with wide customer base. Manage and maintain 400+ desktop and laptop computers.     Program Support Supervisor  University of Washington   Aug 2013 - May 2016  Seattle, WA   \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;row\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;exp-description\u0026#34;\u0026gt; \u0026lt;p\u0026gt; Managed a team of help desk analysts, including hiring and training. Performed Tier 2 and 3 support and maintenance of HD AV Systems and infrastructure while providing excellent customer support and service. Certified in Biamp Audio DSPs and Crestron DigitalMedia.   \n Lead Help Desk Analyst  University of Washington   Apr 2009 - Jun 2013  Seattle, WA         Education  BFA, 3D4M  University of Washington School of Art   2008 - 2013  Seattle, WA     Undergraduate degree culminating in a synthesis of computer science and art in the form of live drawing robots within the gallery space and finished works displayed in the gallery. Coursework involved (but was not limited to) CS fundamentals, data structures and algorithms, networking, databases, studio art, sculpture, and acoustics.     Certificates  APIs and Microservices  Free Code Camp   2018       Experience building APIs and Microservices. Check out my samples and projects sections for examples.  Free Code Camp Certificate    Additional Details Check me out on Linkedin\u0026nbsp;\u0026nbsp;\n","permalink":"https://chasesawyer.dev/page/about/","tags":null,"title":"About Me"},{"categories":null,"contents":" This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ` [outputs] home = [\u0026quot;HTML\u0026quot;, \u0026quot;JSON\u0026quot;] \\`\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ` ... \u0026quot;contents\u0026quot;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026quot;tags\u0026quot;:{{ .Params.tags | jsonify }}{{end}}, \u0026quot;categories\u0026quot; : {{ .Params.categories | jsonify }}, ... \\`\nEdit fuse.js options to Search static/js/search.js ` keys: [ \u0026quot;title\u0026quot;, \u0026quot;contents\u0026quot;, \u0026quot;tags\u0026quot;, \u0026quot;categories\u0026quot; ] \\`\n","permalink":"https://chasesawyer.dev/search/","tags":null,"title":"Search Results"}]