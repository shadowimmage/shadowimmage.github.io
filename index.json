[{"categories":null,"contents":"This is a guide on setting up Python/Django apps to run on a Windows server using IIS as the webserver. I'll go over the specifics below. We're starting things off with the following assumptions:\n Windows Server is installed somewhere and running with a static IP and domain name all set. Server SSL Certificate has already been provisioned and set up. (Optional but extremely recommended to run HTTPS) (not specifically necessary) any SSO setup/shibboleth stuff has already been set up. (This is if you want to leverage SSO login, etc.) Everything is running 64-bit architecture.  Python Install the latest Python version using the GUI Windows x64 installer downloaded from the python.org. As of writing, the latest version available is 3.8.2.\nMake the following settings changes to the Python installation (we're going for a minimal installation with just the Python core and a few niceties):\n Check option for \u0026ldquo;Add Python 3.8 to PATH\u0026rdquo; Click \u0026ldquo;Customize Installation\u0026rdquo; Deselect all options except pip. click Next Check \u0026ldquo;Install for all users\u0026rdquo; Deselect \u0026ldquo;Create shortcuts for installed applications\u0026rdquo; Check \u0026ldquo;Add Python to environment variables\u0026rdquo; Check \u0026ldquo;Precompile standard library\u0026rdquo; (not specifically necessary, but doesn't hurt anything) NO \u0026ldquo;debugging symbols\u0026rdquo; or \u0026ldquo;debug binaries\u0026rdquo; (this is supposed to be a prod environment, after all) Change the Installation directory: C:\\Python38\\ Install.  Virtualenv Once Python's installation is complete open an administrative terminal/Powershell window (winkey+x, a) and complete the following:\n Note: If any of the following commands come back with something like \u0026ldquo;command not found\u0026rdquo; double-check that C:\\Python38\\ and C:\\Python38\\Scripts\\ are in the system PATH environment variable (run $Env:Path in powershell). If you had the terminal window before installing Python, close and re-open it. Or just add the Python directories to the system PATH manually.\n  upgrade pip   python -m pip install --upgrade pip\n install virtualenv   pip install virtualenv\n IIS Setup / Prerequisites IIS needs to be installed, with the CGI option. Once that is installed there should be a directory C:\\inetpub\\wwwroot\\\nMethod 1  Open \u0026ldquo;Windows Features\u0026rdquo; (search for \u0026ldquo;Windows Features\u0026rdquo; \u0026gt; \u0026ldquo;Turn Windows Features on or off\u0026rdquo; should be the result or Run (winkey+r) \u0026gt; optionalfeatures.exe \u0026ndash; If that doesn't do anything, try Method 2 Below. Select IIS feature, with the additional options highlighted as in the below image.  CGI HTTP Redirection Request Monitor   OK    Windows Features options may vary, features available may vary, etc. etc.\n   Method 2  Open the Server Manager Click \u0026ldquo;Manage\u0026rdquo; Click \u0026ldquo;Add Roles and Features\u0026rdquo; Go through the wizard and ensure that all the features listed in Method 1 are selected, specifically IIS services and especially CGI, HTTP Redirection, and Request Monitor. The items are the same as above in Method 1, but are organized a little differently.    Server Manager Screenshot Select Manage, then Add Roles and Features, then the wizard shown at 3Ô∏è‚É£ should show\n   Test Finally, test that the IIS server installation worked and that you can browse to http://localhost on a browser, and that you get the default IIS page.\nSet Up Django Application Directory and Virtual Environment for Python Here's where we set up the application folder that will host our Django application and all the required Python libraries to support that application without installing anything globally. This will make sure that if there's any other Python apps that need to run on the server or be served by the server, there won't be dependency version conflicts.\n Create an application folder to host your application. I wanted my app to be served from \u0026lt;webserver_root_url\u0026gt;/app so I put my application folder at c:\\inetpub\\wwwroot\\app\\. NOTE this does not necessarily have to be in your inetpub/wwwroot folder, it's just a bit easier to do it this way. Open an elevated console within the \\app directory (typically will need to be elevated to do things in this directory because security stuff) Create a virtual environment with virtualenv   \u0026gt; virtualenv venv\n This should create a directory called \u0026ldquo;venv\u0026rdquo; in \\app\\. Activate the virtual environment so that any Python or Pip commands work against it instead of the global Python environment.   \u0026gt; .\\venv\\Scripts\\activate\n Copy in your Django application, including your requirements.txt file. Install python dependencies from requirements:   \u0026gt; pip install -r requirements.txt\n We'll need the latest wfastcgi python package too (in case it's not in your requirements, since it's not needed to run the development server):   pip install wfastcgi\n Note: if you run into problems with wfastcgi not working, or are getting errors like \u0026ldquo;the fastcgi process exited unexpectedly\u0026rdquo; then try to force wfastcgi to upgrade:\n pip install wfastcgi --upgrade\n Set Up Django Site in IIS IIS has specific requirements around how a site is set up, in order for it to work properly. Specifically, each site must have at least 1 application and each application must have at least 1 virtual directory. This page from Microsoft Docs has detailed information on the requirements for a site to publish correctly on IIS.\n Open IIS manager (winkey+r) Run \u0026gt; inetmgr Select the Server and from the main page, double-click \u0026ldquo;FastCGI Settings\u0026rdquo; \u0026ldquo;Add Application\u0026rdquo; Fill out the settings dialog accordingly:  \u0026ldquo;Full Path\u0026rdquo;: Where your virtual environment's python.exe lives (such as C:\\inetpub\\wwwroot\\app\\venv\\Scripts\\python.exe) \u0026ldquo;Arguments\u0026rdquo;: path to wfastcgi.py which should also be in the virtual environment directory: C:\\inetpub\\wwwroot\\app\\venv\\Lib\\site-packages\\wfastcgi.py In the \u0026ldquo;FastCGI Settings\u0026rdquo; section, under \u0026ldquo;General \u0026gt; Environment Variables\u0026rdquo; click on the \u0026ldquo;(Collection)\u0026rdquo; line, then on the little ellipsis (\u0026quot;[\u0026hellip;]\u0026quot;) button, which will allow entering Environment Variables specific to the runtime environment that Django will be running in when a request comes into the web server.  In the \u0026ldquo;EnvironmentVariables Collection Editor\u0026rdquo; window: Add: Name: DJANGO_SETTINGS_MODULE Value: whatever matches up to your setting in wsgi.py. For me this was server.environment Add: Name: PYTHONPATH Value: C:\\inetpub\\wwwroot\\app Add: Name: WSGI_HANDLER Value: django.core.wsgi.get_wsgi_application() If you want WSGI Logging: Add: Name: WSGI_LOG Value: wherever you want logs to be written. I put: C:\\inetpub\\logs\\WSGI\\app.log (this file can get verbose, consider removing this once you've made sure the application is working well)  ‚ö† WARNING - READ THIS ‚ö†: You must make sure that the local server's worker processes have write permission on this file or it's directory. If you do not, wfastcgi/python will crash out and IIS will throw 500 server errors. I spent days fighting with this. The easiest fix is to manually create the file C:\\inetpub\\logs\\WSGI\\app.log and then edit the security permissions on that file, granting full write permission to the local server group \u0026ldquo;IIS_IUSRS\u0026rdquo;.        This should correctly set up the environment for FastCGI to be able to run the Django application (assuming that the paths above match to where you're working from). Note (1): For DJANGO_SETTINGS_MODULE I used server.environment - this matches my environment, since I have /app/server/environment.py and environment.py lists out which server settings should be loaded. Note (2): All of the above settings for Environment Variables are case sensitive.\nClose the Environment Variables window and the FastCGI Settings windows. On the left-hand pane of IIS Manager, under \u0026ldquo;connections\u0026rdquo; where the server we're working on, expand the server, and under \u0026ldquo;Default Web Site\u0026quot;, there should be a listing of directories that are in wwwroot\\. Here, we'll convert \\app\\ into an application (right-click on the directory, then select \u0026ldquo;convert to application\u0026rdquo;) - Click OK on the \u0026ldquo;Add Application\u0026rdquo; window that pops up. Open Handler Mappings for the application Click \u0026ldquo;Add module mapping\u0026rdquo; and enter the following settings:  Path: * Module (dropdown): \u0026ldquo;FastCgiModule\u0026rdquo; Executable: type in: C:\\inetpub\\wwwroot\\app\\venv\\Scripts\\python.exe|C:\\inetpub\\wwwroot\\app\\venv\\Lib\\site-packages\\wfastcgi.py Name: \u0026ldquo;Django Handler\u0026rdquo; or \u0026ldquo;Python FastCGI handler\u0026rdquo; or whatever - it's just a friendly name for the mapping. Click \u0026ldquo;Request Restrictions\u0026rdquo;  Deselect \u0026ldquo;Invoke handler only if\u0026hellip; mapped to:\u0026rdquo; Verbs: All verbs Access: Script   Click OK Click OK When a popup asks \u0026ldquo;Do you want to create a FastCGI Application for this executable?\u0026rdquo; click \u0026ldquo;No\u0026rdquo; as that has already been handled / set up.   The handler should now show in the list of Handler Mappings. Click \u0026ldquo;View Ordered List\u0026hellip;\u0026rdquo; on the right, and move the newly created handler to the top of the list. This will ensure that the python handler is the first one considered for all requests to this application.  Restart your IIS website and it should now be working where the Django application should be reachable at http://localhost/app/ (assuming your Django site has a page listed there).\n IIS restart commands:\n\u0026gt; iisreset /stop\n\u0026gt; net start e3svc\n Configure Django and IIS Static Files The Django development server automatically handles serving static files when working on your computer, but now that it's in a production environment, we need to collect the static files to a directory and then tell IIS to serve them from that directory. Most details on serving static files, as well as handling additional details should be found on Django's documentation site: static files deployment.\nSettings Django's settings need to be modified to include the options STATIC_ROOT and STATIC_URL.\nSTATIC_ROOT is used to tell Django's collectstatic command where in the filesystem to place the found static files. This location could, in theory, be anywhere on the filesystem, but it's good practice to keep these files in a location that makes sense in terms of compartmentalization and context. I put my files inside the project folder next to manage.py.\n1 2 3  # settings.py or prod.py or wherever your production settings may be... STATIC_ROOT = \u0026#39;/inetpub/wwwroot/app/static/\u0026#39; # Windows - assumes C as root; don\u0026#39;t have to explicitly say \u0026#34;C:\u0026#34; STATIC_URL = \u0026#39;/app/static/\u0026#39;   Move Files Run the collectstatic management command from the project directory.\nActivate the virtualenv:\n .\\venv\\scripts\\activate\n Run the command:\n python manage.py collectstatic\n Say \u0026ldquo;yes\u0026rdquo; to the prompt from the collectstatic management command to confirm the directory you want to copy static files to.\nSet Up a Virtual Directory for IIS to Serve the Static Files IIS needs to know where these files are located and how to serve them up when browsers request them. THe name of a virtual directory must match the value of the STATIC_URL setting in Django's settings.py, absent the beginning and trailing slashes. For this sample, the url is app/static.\n Open IIS Manager On the left pane, under ‚ÄúConnections\u0026rdquo; expand the server‚Äôs tree Expand the ‚ÄúSites‚Äù folder Right-Click the web site your app lives in (for me, I put everything in \u0026ldquo;default web site\u0026rdquo;) Click \u0026ldquo;Add Virtual Directory\u0026rdquo; Enter the following values:   Alias: static\nPhysical Path: C:\\inetpub\\wwwroot\\app\\static\n   static virtual directory in IIS Your \u0026#34;static\u0026#34; folder should now have a shortcut-looking icon on it, as shown here.\n   Configure Handler Mappings for Static Files  Select the \u0026ldquo;static\u0026rdquo; virtual directory Open \u0026ldquo;Handler Mappings\u0026rdquo; On the right side, select \u0026ldquo;View Ordered List\u0026hellip;\u0026rdquo; Move the \u0026ldquo;StaticFile\u0026rdquo; handler to the top of the list by selecting it, then on the right under \u0026ldquo;Actions\u0026rdquo; click \u0026ldquo;Move Up\u0026rdquo; until the handler is above all others. If IIS warns you about diverging from inheriting settings, click OK - this is what we want to do.  At this point Django app(s) should be available and serving from IIS at /app or /app/admin from your webserver, with all the static assets and CSS loaded properly. If not, go back over the Static Files settings, and make sure that the static assets collected by collectstatic correctly found and placed all the files you're relying on in the correct location.\nShibboleth / SSO / Remote-User IIS / Shibboleth The Shibboleth service needs to be installed and configured on the webserver. Once installed and configured, the path to the API / App / Site must be listed in shibboleth's configuration file Shibboleth2.xml. By default this file can be found in C:\\opt\\shibboleth-sp\\etc\\shibboleth\\.\nDjango Configuration A few things need to be added to middleware and authentication backends to enable use of the remote user environment variable set by shibboleth in IIS for purposes of authenticating users to Django / Apps.\nIn prod.py (or wherever production settings are stored, like settings.py) add the following to allow reading of REMOTE_USER from the request:\n1 2 3 4 5 6 7 8 9  MIDDLEWARE = MIDDLEWARE + [ # ADDDED REMOTE USER MIDDLEWARE FOR SHIBBOLETH AUTH \u0026#39;django.contrib.auth.middleware.PersistentRemoteUserMiddleware\u0026#39;, ] AUTHENTICATION_BACKENDS = [ \u0026#39;django.contrib.auth.backends.RemoteUserBackend\u0026#39;, \u0026#39;django.contrib.auth.backends.ModelBackend\u0026#39;, #Fallback ]   Note that in this case PersistentRemoteUserMiddleware is appended to the end of the MIDDLEWARE list, which is imported from base.py. If your configuration has other middleware that depends on specific ordering, then this solution may not be optimal for all cases.\nThere are also two flavors of Remote User Middleware - the generic and the .Persistent... variety. For more details on use see Authenticating using REMOTE_USER.\nThanks and Resources Below is a listing of all the tabs I had open for reference when figuring this out and writing this.\n Microsoft Docs: How to Use HTTP Detailed Errors in IIS Stack Overflow: python.exe the fastcgi process exited unexpectedly DigitalOcean community: How to Deploy Python WSGI Applications Using uWSGI Web Server with Nginx Microsoft Docs: Configure Python web apps for IIS Django Documentation: How Django processes a request Nitin Nain: Setting up Django on Windows IIS Server Microsoft Docs: About Sites, Applications, and Virtual Directories in IIS 7 and Above  ","permalink":"https://chasesawyer.dev/post/2020/03/django-api-apps-on-windows-iis/","tags":["post","iis","django","python","windows","tutorial"],"title":"Django API Apps on Windows IIS"},{"categories":null,"contents":"I had a problem when I moved my Raspberry Pi over to using a 4K display, which is just about the only resolution that professional displays come in these days, as manufacturers have all moved on to 4K as a standard. The problem was with overscanning and the display not having the ability to correctly scale the image signal coming from the Raspberry Pi, which was still a 1920x1080 signal.\nThere's a collection of settings in config.txt on the Raspberry Pi that can mitigate this problem:\n disable_overscan overscan_left overscan_right overscan_top overscan_bottom  For me, I deleted all the explicit overscan settings, and enabled the disable_overscan setting.\nHere's my fixed up config.txt:\n1 2 3 4 5 6 7  enable_uart=1 disable_overscan=1 disable_splash=1 dtparam=i2c_arm=on dtparam=spi=on dtparam=audio=on gpu_mem=32   More settings and info can be found here: https://www.opentechguides.com/how-to/article/raspberry-pi/28/raspi-display-setting.html\n","permalink":"https://chasesawyer.dev/post/2020/01/raspberry-pi-hdmi-overscan/","tags":["post","balena","raspberrypi","short","note"],"title":"Raspberry Pi HDMI Overscan"},{"categories":null,"contents":"New app in progress! Hopefully it won't take months to complete. Should be pretty simple. Especially since this app is going to be primarily for me - and I'm not going to worry too too much about the UI elements. Thinking this is going to be primarily a django based app with templates and model-based views like the keys app.\nCurrent State: Developing database models\nPurpose Provide myself and whoever wants it a place to track their car maintenance, and to make sure that they know when the next scheduled service for a particular maintenance item is due. For example, say you have a vehicle who's differential fluid needs to be checked every 60000 miles, this app would let you enter a Maintenance Item of \u0026ldquo;differential\u0026rdquo; with an interval of 60000(miles). Then the app can use the last service record relevant to the differential, the mileage of that service, and the current mileage to determine if the differential service or inspection is due.\nSchema  Cars  Manufacturer Model Year   Maintenance Items [MI]  (FK/multiple) Car Item  Title Mileage Interval (opt) Time Interval (opt)   Interval Type  ? Time : Mileage : Both   Service Type (Replace or Inspect)   Maintenance Record [MR]  (FK) Car (FK) Maintenance Item Current Mileage (as of service date) Service Date (timestamp)   Regular Checks  (FK) Car Current Mileage Fuel Economy (estimate?) Oil Level Comments    Setting up a new vehicle To set up a new vehicle, we'd enter a new manufacturer and model, then go into the maintenance items table and add all the various items that need to be checked and/or replaced.\nIntervals Business logic will check to make sure that either a date or a mileage interval is selected and that the interval type selection matches (time, mileage, or \u0026lsquo;both\u0026rsquo;). When analyzing a vehicle for service items due, the logic can use the setting in Maintenance Items to determine - based on most recent Maintenance Records, the current date, and most recent Regular Check (for non-service mileage) - whether or not service for that particular item is due (or due soon?).\n ","permalink":"https://chasesawyer.dev/post/2019/07/car-maintenance-app/","tags":["post","project","heroku","django","app","dev"],"title":"Car Maintenance App"},{"categories":null,"contents":"Fair warning: This is a work in progress, and I'm still working out the details for this project.\nGithub Repository\nI have started to work on getting a digital signage solution set up where I can set up a Raspberry Pi3 to display information on a screen, and that has no local interaction (no UI, running headless without any keyboard/mouse). I wanted a solution to control updates to the app, which is where BalenaCloud comes in (along with their handy OS for the Raspberry Pi). I also wanted to figure out a way to develop the app that would display on the screen locally on my Windows machine, and still be able to push the same app to the Raspberry Pi.\nThe biggest stumbling block or at least the steepest part of the learning curve for this project for me is that I'm deploying several new tools in ways that I've never done before. For one thing, I've never used Electron to create an app. I've never used Balena or BalenaCloud. And I still don't completely understand Docker core images and how they're created or how you might go about creating your own. On top of that, the architecture (which?) for a Raspberry Pi is different from the architecture (which?) on a Windows PC, which means that I can't run the same docker image on my PC as I need to run on the Pi, but have to either use a completely different core image with the same resources, or just trust that the Electron app will run the same on both (I haven't worked out all the bugs yet). Another block I had to get through was that most of the examples for running Electron on a Pi3 are about a year or more old, and there's been several updates to multiple components since then, and I want to use the latest versions of all the software if possible. Finally, I had some trouble finding an exact example from someone else that had this exact use case, and so I am taking pieces of lots of examples and putting them together to hopefully end up with a final product that achieves my goals.\nProject Overview! Briefly, this is what I want this thing to do:\n Connect to an API and load data  API is REST-ful API uses tokens for app authentication API token is unrestricted and needs to be secret (it has read/write permission)   Transform and resolve multiple data aspects through multiple API requests  API has endpoints for schedule, users, locations, etc.   Display data on a screen in a semi-public space  Screen is wall-mounted 24x7 uptime 1080p Installation location is in an office, but not monitored, can be accessed any time by any staff   Refresh data periodically  Use cache/local database to avoid excessive API calls Update as often as 5 minutes, or as infrequently as every hour   Display status info  Iconography to display online/offline state, last update time, clock, etc.   Be graceful with errors  What does an API rejection look like vs offline? (use HTTP status codes, probably)   Allow remote management (BalenaCloud or OpenBalena)  Git repository watch CI/CD integration Slack integration Display endpoint remote reboot, shutdown, wipe, etc.    BalenaCloud allows up to ten endpoints (managed display devices) for free. Since this project only needs to support 1 display at the moment, this is perfect. If I end up needing more than ten displays, then I'll probably set up OpenBalena, rather than pay for a cloud plan.\nMany of the dockerfile examples from balena / resin.io are kind of out of date, so I'm going to be re-working a dockerfile from scratch. I also am using electron.js v5, which has an issue with sandboxing the main chrome process that I have found a workaround (pass Electron executable the --no-sandbox option) for, so that's good. I now have a working dockerfile that will build on the pi (or on balenaCloud) and display an electron app. The dockerfile is below.\nBasic Config Support tools Since I'm using balena / balenaCloud, I installed the balena-cli - I had trouble with getting it working from npm and so I just went with the \u0026lsquo;download and extract to a location and add that to your system path\u0026rsquo; method (standalone zip package) (similar to how I use Hugo), since I didn't want to use the executable installer for a simple tool like this. Other than that, I'm using Windows 10, Visual Studio Code, and Postman. Postman is really helpful for exploring and testing the target API service, since it lets me run calls without needing all the code set up first. Typically I use Postman to mock all the example calls I'm going to make to the target API, and then translate those over into code (usually Python, but for this it's in Node, same idea though).\nDockerfile 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52  # Specify balena\u0026#39;s maintained core image for Raspberry Pi3, Node 10.16, and Ubuntu BionicFROMbalenalib/raspberrypi3-ubuntu-node:10.16-bionic# Install necessary modules to support Electron.js runtime, including xorg display and supporting librariesRUN apt-get update \u0026amp;\u0026amp; apt-get install -y --no-install-recommends \\  apt-utils \\  clang \\  xserver-xorg-core \\  xserver-xorg-input-all \\  xserver-xorg-video-fbdev \\  xorg \\  libxcb-image0 \\  libxcb-util1 \\  xdg-utils \\  libdbus-1-dev \\  libgtk2.0-dev \\  libnotify-dev \\  libgnome-keyring-dev \\  libgconf2-dev \\  libasound2-dev \\  libcap-dev \\  libcups2-dev \\  libxtst-dev \\  libxss1 \\  libnss3-dev \\  libsmbclient \\  libssh-4 \\  fbset \\  libexpat-dev \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/*# Set app working directoryWORKDIR/usr/src/app# Move package.json to app dir for dependency installationCOPY ./package.json .RUN npm install \u0026amp;\u0026amp; npm cache clean --force \u0026amp;\u0026amp; rm -rf /tmp/*# Copy over app source codeCOPY . .# SystemdENV INITSYSTEM on# set Xorg and FLUXBOX preferencesRUN mkdir ~/.fluxboxRUN echo \u0026#34;xset s off\u0026#34; \u0026gt; ~?.fluxbox/startup \u0026amp;\u0026amp; echo \u0026#34;xserver-command=X -s 0 dpms\u0026#34; \u0026gt;\u0026gt; ~/.fluxbox/startup# Set xserver to runRUN echo \u0026#34;#!/bin/bash\u0026#34; \u0026gt; /etc/X11/xinit/xserverrc \\  echo \u0026#34;\u0026#34; \u0026gt;\u0026gt; /etc/X11/xinit/xserverrc \\  echo \u0026#39;exec /usr/bin/X -s 0 dpms -nocursor -nolisten tcp \u0026#34;$@\u0026#34;\u0026#39; \u0026gt;\u0026gt; /etc/X11/xinit/xserverrc# Start Electron app using a scriptCMD [\u0026#34;bash\u0026#34;, \u0026#34;/usr/src/app/start.sh\u0026#34;]  Main start.sh The electron app / executable is started and displayed through the connected screen using the following shell script. This runs in one of the dedicated docker containers running on the Pi3, so similar to running a GUI app on a desktop from within a docker container, you need to tell it how to connect the output visuals to the display.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  #!/bin/bash export URL_LAUNCHER_NODE=1 export NODE_ENV=production # By default Docker gives 64MB of shared memory, but to display heavy pages we need more: umount /dev/shm \u0026amp;\u0026amp; mount -t tmpfs shm /dev/shm # use the locally installed electron module, rather than any that might be installed globally. # this also gives control to package.json as to which exact version of electron to use. # Below also sets an X instance with ONLY electronjs running, rather than a full desktop environment # saving a lot of resources (especially since this is for a headless display without any UI). rm /tmp/.X0-lock \u0026amp;\u0026gt;/dev/null || true # Set whether we\u0026#39;re using the PI TFT screen, rotation, etc. and start X else, using HDMI output, just start X if [ ! -c /dev/fb1 ] \u0026amp;\u0026amp; [ \u0026#34;TFT\u0026#34; = \u0026#34;1\u0026#34; ]; then modprobe spi-bcm2708 || true modprobe fbtft_device name=pitft verbose=0 rotate=${TFT_ROTATE:-0} || true sleep 1 mknod /dev/fb1 c $(cat /sys/class/graphics/fb1/dev | tr \u0026#39;:\u0026#39; \u0026#39; \u0026#39;) || true FRAMEBUFFER=/dev/fb1 startx /usr/src/app/node_modules/electron/dist/electron /usr/src/app --enable-logging --no-sandbox else startx /usr/src/app/node_modules/electron/dist/electron /usr/src/app --enable-logging --no-sandbox   Electron App Notes This app is really basic, since it only connects to one API and blindly presents that information on to a screen - there's no user input to handle, no other cycles beyond updating the screen every minute with new information from the API and perhaps displaying things like the current time. Since it doesn't need to handle a lot of complex items, I'm not including anything exceptional, like a full framework like Vue or React, since these are somewhat overblown for what I need on this project. The most I'm including for presentation is a minified compiled version of Bootstrap.\nThe trick is scheduling the screen refresh and how it's supposed to handle different states in terms of what the API returns, network status, etc. To handle this I'm implementing a supervisory style main loop that deals with handling the cached data, loads up the environment variables, and reacts to differing application states from the environment (network link up/down/connected, API response codes - such as how to handle a 503 code).\nFiguring out where to put these elements (looping to refresh data, handling response codes, etc.) was a little tricky, since I couldn't decide if this should be something in the renderer.js file or main.js. The answer came from here (mdn). I was considering using setInterval() to create a never-ending loop, but instead decided to go with a recursive loop that never really ends, but recursively calls setTimeout(), with changing values of timeout intervals reacting to changes in app state, such as receiving a non-200 HTML status from the remote API, and using a multiplier on the recursive timeout to wait longer and longer between API requests, in the hopes of eventually receiving a good response again. So normally each data refresh is every 30 seconds. If an API call fails to return or returns a non-200 code, then the next call will wait 60 seconds, then 120 seconds, then 240, and so on. After a successful response, the state returns to a 30-second interval. The idea for this is partially inspired by Exponential Backoff, used in TCP.\nRenderer.js - Starting up Starting up the loop for API requests is pretty simple. Once the page is loaded, fire up a couple recursive loops to handle 2 main things: 1.) the clock a the bottom of the screen and 2.) the API data fetch.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  function handleReload(interval, initialInterval) { // display the active reload interval in the page footer  const reloadSpan = document.getElementById(\u0026#39;reload-interval\u0026#39;); reloadSpan.innerHTML = `Reloading every ${interval / 1000}s`; setTimeout(() =\u0026gt; { getSchedule().then((result) =\u0026gt; { if (result.status === 200) { const target = document.getElementById(\u0026#39;content\u0026#39;); target.innerHTML = markupResults(result.data).join(\u0026#39;\u0026#39;); // markupResults returns an array of HTML elements  handleReload(initialInterval, initialInterval); // resets interval to initial state  } else { // didn\u0026#39;t receive a 200 code :( wait a bit longer before trying next time  handleReload(interval * 2, initialInterval); } }); }, interval); } function showClock() { setTimeout(() =\u0026gt; { const target = document.getElementById(\u0026#39;clock-row\u0026#39;); target.innerHTML = markupClock(); showClock(); }, 1000); } window.onload = () =\u0026gt; { const initialInterval = 30000; // milliseconds - TODO: make this an Environment Variable  handleReload(initialInterval, initialInterval); showClock(); };   As soon as the page on the electron app has finished its initial load, the window.onload handler starts both recursive loops. This process continues indefinitely.\nDeployment  Set up the Raspberry Pi on the BalenaCloud dashboard, download and flash the BalenaOS image to the microSD card that will run the Pi. Update application- or device-level environment variables. These will allow devices in production to access the API using the key provided through the environment variables. This also allows a unified and quick location to update the key should it be changed or compromised. Note: updating environment variables will cause endpoint devices to reboot. Push the code in the repository to Balena using the balena-cli - this will use the project's Dockerfile to build the application image and then distribute it to all devices assigned to the application on the BalenaCloud Dashboard. Wait for the code to download onto the target device(s) and begin running.  Local Development Runs fine with npm start which will run a local electron session, loading the code and opening a window on the desktop. Using dotenv, environment variables stored in a .env file will be loaded into the main process. When deployed via balena, these environment variables will not be loaded from the .env file (which should contain and store secrets that are not committed to source control) but from the environment variables loaded into the balena console (which are pushed down to target devices within scope).\nThe .env file should contain API keys and any other things specific to this particular implementation that might change over time, but that wouldn't require any kind of code change / recompilation of the docker image.\n","permalink":"https://chasesawyer.dev/post/2019/06/docker-balena-electron-raspberry-pi-digital-signage/","tags":["post","balenacloud","electron","js","javascript","raspberrypi","docker"],"title":"Docker-Balena-Electron-Raspberry Pi Digital Signage"},{"categories":null,"contents":"I'm building a web infrastructure project that's based around the project verbose-equals-true (referred to as VET from now on), which sets out to create a set of services to support modern web apps, using several Docker based images to collect everything into separate concerns. I like the philosophy behind the project, and it looks well thought out, however, as things are always changing in this landscape and nobody has the same development environment, there's always going to be stumbling blocks. It's also my goal to use this other project as more of a framework or set of guidelines that should work, and then branch out from there, changing things as I go along so that it works for me.\nSo with credit to the original creator of this project, Brian Caffey, here are some of my notes on what I found difficult, what problems I ran into, and the things I changed.\nOperating Systems The project is built in a Linux environment, which has excellent support for Docker, but it's not my primary environment to be working in. I use Windows primarily, and all of my computers are Windows boxes. However, I have some Linux environments set up in Hyper-V already, so I'm using Linux in Windows to implement this Docker project (what could possibly go wrong? Networking, mostly).\nDNS This is in the Docker documentation here, but not in the VET documentation: (quoted below from docs.docker.com)\n Troubleshoting for Linux users\n\u0026hellip;\nDNS settings\nDNS misconfigurations can generate problems with pip. You need to set your own DNS server address to make pip work properly. You might want to change the DNS settings of the Docker daemon. You can edit (or create) the configuration file at /etc/docker/daemon.json with the dns key, as following:\n{ \u0026quot;dns\u0026quot;: [\u0026quot;your_dns_address\u0026quot;, \u0026quot;8.8.8.8\u0026quot;] }\n[\u0026hellip;]\nBefore proceeding, save daemon.json and restart the docker service. sudo service docker restart\n The example they give above has you set the IP address(s) for your local DNS provider, with a fallback address to Google's public DNS. I set my DNS address list to use my local DNS first, falling back to 1.1.1.1 (Cloudflare) and finally 8.8.8.8 for Google's public DNS.\nThe line in the VET docs that gave me trouble was when it says to first run this line in the terminal:\nsudo docker-compose run backend django-admin.py startproject backend .\nThis line will spin up a docker container that will try to run pip install for the required packages in requirements.txt which failed for me because the docker container didn't know where to look for DNS resolution, and so pip couldn't find and install any requisite packages.\nAfter setting up my DNS setting for the Docker daemon as specified above and restarting the docker service, pip install worked as expected üôå.\nCI The VET docs on have you use the GitLab built-in CI offering, which, if I used GitLab at all, would probably be really easy! But I don't use GitLab and don't really want to get set up with another code repository when I already have GitHub set up and have integration with CircleCI already set up. So here's my modifications to the project to get set up with GitHub and CircleCI.\nNot really knowing anything about how GitLab's CI systems work, it's hard to translate what it's doing to what needs to be done with CircleCI, so this is maybe not the best solution, but it does work, and I believe that it does the thing it's supposed to do.\nWorking Version of CircleCI Config Here's the first version of the CircleCI configuration file that successfully passed all tests and was able to store testing data in a way that CircleCI could store and parse in results.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69  #.circleci/config.yml version: 2.1 # CircleCI jobs: lint_test_coverage: working_directory: ~/project # this is the default docker: # The first image listed is the primary image and runs all commands - image: circleci/python:3.6 environment: TEST_DATABASE_URL: postgresql://postgres@localhost/circle_test?sslmode=disable DJANGO_SETTINGS_MODULE: backend.settings-circleci # Subsequent images listed run on a common network with the primary image - image: circleci/postgres:9.6.9 environment: # these settings affect how the test database is going to be set up - use to connect later POSTGRES_USER: postgres POSTGRES_DB: circle_test POSTGRES_PASSWORD: \u0026#34;\u0026#34; steps: - checkout - run: mkdir test-reports - restore_cache: key: deps1-{{ .Branch }}-{{ checksum \u0026#34;backend/requirements.txt\u0026#34; }} - run: name: Install Python Dependencies command: | python3 -m venv .venv . .venv/bin/activate pip install -r backend/requirements.txt # Save installed environment dependencies in cache for later steps/jobs - save_cache: key: deps1-{{ .Branch }}-{{ checksum \u0026#34;backend/requirements.txt\u0026#34; }} paths: - \u0026#34;.venv\u0026#34; - run: name: install dockerize command: wget https://github.com/jwilder/dockerize/releases/download/$DOCKERIZE_VERSION/dockerize-linux-amd64-$DOCKERIZE_VERSION.tar.gz \u0026amp;\u0026amp; sudo tar -C /usr/local/bin -xzvf dockerize-linux-amd64-$DOCKERIZE_VERSION.tar.gz \u0026amp;\u0026amp; rm dockerize-linux-amd64-$DOCKERIZE_VERSION.tar.gz environment: DOCKERIZE_VERSION: v0.3.0 - run: name: Wait for db command: dockerize -wait tcp://localhost:5432 -timeout 1m - run: name: Linting test command: | . .venv/bin/activate cd backend flake8 --output-file=../test-reports/flake8.txt - run: name: coverage test command: | . .venv/bin/activate cd backend pytest --cov --junitxml=../test-reports/pytest - store_artifacts: path: test-reports/ destination: tr1 - store_test_results: path: test-reports/ workflows: version: 2 lint_test: jobs: - lint_test_coverage: filters: branches: only: master   Issue 1: DB Credentials Need to make sure that the postgres configuration passed to the secondary docker image sets up a user and database name that django is later going to connect to. In this case this is called out here:\n14 15 16 17 18  - image: circleci/postgres:9.6.9 environment: POSTGRES_USER: postgres POSTGRES_DB: circle_test POSTGRES_PASS: \u0026#34;\u0026#34;   Which needs to be loaded into django's settings from settings-circleci.py\n1 2 3 4 5 6 7 8 9 10 11 12  from .settings import * # noqa DATABASES = { \u0026#39;default\u0026#39;: { \u0026#39;ENGINE\u0026#39;: \u0026#39;django.db.backends.postgresql_psycopg2\u0026#39;, \u0026#39;NAME\u0026#39;: \u0026#39;circle_test\u0026#39;, \u0026#39;USER\u0026#39;: \u0026#39;postgres\u0026#39;, \u0026#39;PASSWORD\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;HOST\u0026#39;: \u0026#39;localhost\u0026#39;, \u0026#39;PORT\u0026#39;: \u0026#39;5432\u0026#39;, }, }   Which are loaded when pytest runs because of the environment variable on the main docker image that is running the tests. I also declared the TEST_DATABASE_URL environment variable there, but decided not to make use of it in my settings-circleci.py file, finding that it seems to work better to declare the individual parts, rather than to use the URL string.\nIssue #2: Difference in Caching Methods The source documentation for GitLab lets you declare a cache with a path. With CircleCI it's a little more complicated. For caching things like python dependencies, you need to add steps bracketing the dependency installation, with the same naming scheme for the cache. CircleCI leys you define the naming convention to use as the \u0026ldquo;key\u0026rdquo; for the cache file, so for any project make sure the restore_cache and save_cache use the same key generation method. In this case (from the circleci examples) we use the git branch name ({{ .Branch }}) and the checksum of the requirements file ({{ checksum \u0026quot;backend/requirements.txt\u0026quot; }}) so that we can maintain separate caches based on the required dependencies, and what git branch is actively being tested.\nIssue #3: Translating before_script For GitLab, this runs the pip installation of project requirements. This is basically the same, only in this case, we need to define a virtual environment (that can later be backed up) with venv, then install the dependencies there, much like what we would achieve with local development.\nThese lines:\n25 26 27 28 29 30  - run: name: Install Python Dependencies command: | python3 -m venv .venv . .venv/bin/activate pip install -r backend/requirements.txt   achieve the goal of installing project dependencies, which in the next step are cached for use on the next run (potentially).\nIssue #4: Database not Ready In preliminary test runs, I ran into the issue that django was complaining about the database not being set up. To solve this, and to ensure that the database will always be ready, I install dockerize and use it to wait for port 5432 on localhost to be ready (with a max time limit of 1 minute) in order to be sure that our postgres container is running and accepting connections. This could also be achieved with a curl command, but this works fine too.\nIssue #5: Remember where you're working I ran into a lot of failed runs with CircleCI simply because I wasn't calling commands in the right location, trying to change into directories that didn't exist (because I was already there) or tried to save things into directories that were incorrectly named or not in the places I expected them to be in.\nWorking Direcory CircleCI defaults to ~/project for the current working directory. This is where code gets checked out to and is where the console is pointing when it starts any - run: stanza. You can change the working directory to something else that makes sense if you want to. Some trouble I ran into was trying to cd in to project (which doesn't exist as a subdirectory of project for this project).\nSaving Test Results Early on I create the folder test-reports (line is - run: mkdir test-reports) right after the checkout step. This folder is at ~project/test-reports/, right at the top level of the project folder, alongside all my top level code folders and files when they're checked out.\nDuring my Linting step and Test step, I tried to save the results to ./test-reports/{whatever} - but if you look closely, in those two steps, I've cd'd down to the backend folder (~/project/backend/) which does not contain a test-reports folder! So my tests failed because they could not write to that direcory. After testing I realized my mistake and changed ./test-reports/{whatever} to ../test-reports/{whatever}.\nA single dot, but an important one.\nIssue #6: pytest Won't Set Up Django This was completely missed in the VET documentation, and I'm pretty sure I didn't miss anything in there that I was supposed to do, but installing pytest as part of the requirements.txt alone is insufficient to get pytest to get Django working properly to run tests against. For that you need to also add pytest-django to the project requirements. After I aded that package, Django would come up, connect to the test database and run the simple test that I have defined. Success!\nHere's my full requirements.txt after that change:\n1 2 3 4 5 6  Django==2.2.1 psycopg2==2.8.2 flake8==3.7 pytest pytest-django # \u0026lt;- Add this pytest-cov   Conclusion of Project Setup That's it for this post, and ends the alterations I needed to make for the Project Setup phase of the Verbose Equals True clone I'm making. Up next is the Backend API additions to Django, which will also include my first deep dive into Django REST Framework - since in the past I've only really used plain responses, and Apollo/GraphQl for my Django APIs.\n","permalink":"https://chasesawyer.dev/post/2019/05/docker-infrastructure-project/","tags":["post","docker","django","python","infrastructure"],"title":"Docker Infrastructure Project"},{"categories":null,"contents":"I had to set up an Ubuntu VM (Virtual Machine) on my machine recently and because I also use Docker, I had to learn exactly how Hyper-V handles networking tasks between the host machine and the virtual machines you set up. Hyper-V, as far as I understand, has 3 main networking methods called virtual switches. There's actually a 4th option that comes out of the box, which is something of a hybrid between an \u0026ldquo;Internal Switch\u0026rdquo; and an \u0026ldquo;External Switch\u0026rdquo;. The default switch is interesting, and not particularly intuitive, but after getting used to it, I find it really convenient, if a bit tedious to deal with. It is absolutely not something I recommend running anything in for long periods of time, or for anything upon which other people rely. It is, however, good for getting something set up quick and dirty - so long as you understand what it's doing.\nFor the length of this article, it will be good to understand what a switch is, so read this Wikipedia article if you are unfamiliar.\nSo with that, here's a quick overview of what the Hyper-V switches are and can do.\nFirst, there's the Internal Switch. This is a virtual switch that allows Host\u0026lt;-\u0026gt;Guest connections with static IP addressing allowing inter-machine communication. The downside of this, however, is that the Guest OS is isolated from the wider internet that the Host may be connected to.\nSecond, there's the External Switch. This is a virtual switch that is not connected to the Host directly, and consumes a physical host network interface. This interface is given over to the exclusive use of any VM set up to use the External Switch. The benefit of this is that all VMs on that switch can talk to each other, and can communicate to whatever is connected to that physical network, which may also be the same physical network as your Host (via an external switch, routing, etc. and separate physical network interface). If you have two network interfaces for your computer, then this may be a good solution for long term use. Other benefits include: Static IP addressing of your guest virtual machines, the VMs behave exactly as if they had their own wired (or wireless) network connection to the rest of your physical network, and internet connections are thus possible to and from the Guest machines. Basically, this is just like having real physically separate computers sitting on your desk, with the caveat that you need more than 1 physical network interface (which I don't currently have).\nLastly, there's the Private Switch. This is an interesting situation, and I can only think of a couple uses for it, but what it does is only allow VM\u0026lt;-\u0026gt;VM communication. There's no way for Host\u0026lt;-\u0026gt;VM or Internet\u0026lt;-\u0026gt;VM connections with this switch. I think the only situation you'd want to use this is for VM-VM relay communication, where each VM has more than 1 virtual network connection enabled, such as a Router VM, Firewall VM, and other(s), where 1 VM is connected to the internet, relays traffic to an isolated Firewall VM, which then allows traffic through to even deeper VM(s) that are thus protected via layers of networking, without the need for a rack full of hardware. This might theoretically be a useful application for a clustered network simulation or something where everything is isolated from external interference, but that's beyond my interest or skill.\nThe \u0026ldquo;Default Switch\u0026rdquo; - Getting Up and Running Without Configuring a Bunch of Stuff The thing that makes the Default Switch work well is that it is the only version of virtual switch that Hyper-V has that has NAT or Network Address Translation. The default switch behaves somewhat like a home router, allowing Guest VMs internet access, as well as Host\u0026lt;-\u0026gt;VM communication, without the hardware requirements or setup needs of an External Switch. It also does all the work for you, but also doesn't allow you to take control of it. What this means is that you cannot set static IPs for your VMs. Hyper-V gives them DHCP addresses in a limited range, which is randomly selected in the 192.168.xxx.xxx/28 space. The Host can communicate with services running on a Guest at the given IP address that the VM is given, and the Guest(s) can communicate with the Host at it's external IP address via NAT. Guests will get new random IP addresses every (or almost every) time the VM is rebooted, and each time the Host is rebooted, Hyper-V will choose a new random local address space to set up it's DHCP range in, before passing those off to the guest VMs. You also can't disable or remove the Default Switch (as far as I can tell) - but it won't hurt anything being there. If you don't use it, just ignore it.\nSo, how to get things talking to each other?\nStep 1: Gather information. You need to retrieve 2 things: Your Host computer's IP address. This is what you computer is addressed according to your company / home network if another physical computer on that network were to ping it. For me, it's 192.168.1.10 at home and 10.155.43.82 at work. Then you need to get the IP of your VM. In this case I just opened the ethernet settings in Ubuntu and checked the assigned IP address. You can also get this information from the Hyper-V manager window to see what IP your VM got assigned. At the time of taking notes, this was 192.168.18.187.\nStep 2: Set up services on VM: Say you want to run a test web server on your VM and access it from your Host's browser. You'll need to tell your server to bind to the VM IP at whatever port you want to use, for instance 192.168.18.187:8000. For a Django server, I'd run: python manage.py runserver 192.168.18.187:8000. Then (if it successfully starts up) you can go to your Host browser and go to that address and the Default Switch should route your traffic to the VM running that webserver.\nFrom the VM, if you want to communicate with the Host services (say, a Docker image running PgAdmin; or your PostgreSQL server that's running on the Host), you can address them from the VM by using your Host's external IP address. For the above example, my Django web server running in my Ubuntu VM, would communicate with my Host's PostgreSQL database server at 10.155.43.82:5432.\nNext time you reboot things, just check your IP addresses, and reconfigure your settings before starting up services again.\n","permalink":"https://chasesawyer.dev/post/2019/05/hyper-v-local-networking/","tags":["post","hyperv","networking","virtualization"],"title":"Hyper-V Local Networking"},{"categories":null,"contents":"At the end of February 2019, Google released general access to the .dev top level domain. I had heard about this happening about a year ago, and am now the happy owner of two .dev domains! One of these is chasesawyer.dev and will soon be the new home of this site!\nBut how to get it set up? When you buy a domain name, nothing is really set up for them - going to those addresses doesn't point to anything Firefox and Google will just say \u0026ldquo;Server not found.\u0026rdquo; So here's how to change that.\nTheres a few things I want to set up for these domains, and I'm going to go through the steps for each. This is also part one of a series of posts on how to accomplish these tasks. Other parts will be linked when they've been figured out and done!\n Custom domain setup for this site, while maintaining hosting via Github Pages Email forwarding to ProtonMail Custom domain setup for my apps on Heroku  Some Quick Assumptions I'm going to be making some assumptions throughout this article, most of which can probably be translated to other platforms, but your mileage may vary.\n You're using a .dev domain - part of the HSTS list (this will come up later) You bought your domain, are administering your domain through Google Domains (rather than godaddy or namecheap or something else - those are probably fine, I just went with Google Domains for expediency when they launched .dev)  GitHub Pages and Custom Domains This first one is pretty straightforward - you need to do two things:\n Determine if you want your GitHub Pages site to be at the root of your custom domain (ie, going to chasesawyer.dev without any prefixes) or if you want it to be at a subdomain (ie, blog.chasesawyer.dev or pages.chasesawyer.dev) Create a the appropriate DNS record on google domains (domains.google.com/m/registrar/{yourdomainname}/dns) under \u0026ldquo;Custom resource records.  For the former case (Apex domain), see setting up an apex domain. For this use case, the way that worked best for me was not by using forwarding, but by using A records in the Google Domains DNS settings for my domain name. For the latter case (subdomain), see setting up a custom subdomain   Update the custom domain option on your GitHub Pages repository to match the domain that you're using. If you don't do this, then your site won't load properly, and, since the .dev domain can only be used via HTTPS, you'll have security problems with your site! (more below) Wait for a little while - your DNS settings need some time to propagate through the DNS system, so that requests to {yourdomain}.dev will properly redirect to your GitHub Pages site. GitHub Pages also has to have time to set up a certificate for your new site.  Security, HTTPS, Certificates If, like me, you enjoy using GitHub Pages for hosting your site, you've enjoyed having your site having an https address, and all the security baked into GitHub Pages. When you are setting up sites on a .dev domain since it's part of the HSTS preload list in most modern browsers, your whatever.dev site can only be loaded via https. This means that you'll need a security certificate for the site you set up on .dev! But how do you do this if you're using GitHub Pages? You don't have access to their webserver, so the Let's Encrypt certbot won't work for you. But you need a security certificate for your domain!\nWhat isn't really explained anywhere where I can see, is that this process of getting a security certificate through Let's Encrypt for your GitHub Pages site at your custom domain is completely automated! That's right! GitHub Pages just does it for you. This is why step 4 above takes a little while to complete - whatever mechanism at GitHub that serves up pages from your github.io pages site that gets set up on {your custom domain} needs to have a security certificate set up, and that system takes care of getting one through Let's Encrypt.\nThis is also why I decided to go through with setting up this site with A DNS records, because for whatever reason, DNS forwarding didn't work properly, and didn't trigger whatever automated system that sets up a security certificate for your custom domain served through GitHub Pages.\nWill this cause problems later on? I am not sure yet. I haven't set up subdomain records on Google Domains yet (like {something}.chasesawyer.dev) but that is gonna be the subject of my next post!\n","permalink":"https://chasesawyer.dev/post/2019/03/google-.dev-domains-github-pages-and-heroku-apps/","tags":["post","blog","dev","heroku","google","domains","dns","security","hsts"],"title":"Google .dev Domains, GitHub Pages, and Heroku Apps"},{"categories":null,"contents":"I have been working on the backend for a project that I've written about {{previously\u0026ndash;link}}. The established tools server that will be supporting my new React frontend app(s) will be using a backend built on Python 2.7 and Django 1.11, and thus I've had to remember how to get a development environment set up that will appropriately support the project running locally on my machine. I have a personal site that runs on the same version of Django, but with Python 3.6 as the underlying code base. Having used these tools previously, I decided that it was time to bump my personal Django instance up to something more modern and learn what it would take to get my apps-demos server (available at chase-sawyer-demos.herokuapp.com) up to Django 2.1 with Python 3.7 backing it up. I also wanted to keep some notes here on what the experience has been like setting up a legacy development environment from scratch after not having worked on a project for some time or picking up someone else's project.\nBelow are some of my notes and experiences on having gone through this process.\nThe Work Environment and Long File Names I had a further complication with this project in that some of the dependencies required were projects maintained by other groups here, were clearly developed on a MacOS or Linux environment as the file names and directories for files within these projects were too long for Windows to handle. This meant that I could not successfully clone the codebase on to my usual development computer (Windows 10 based). I dug through some documentation for Windows that stated that it can technically support longer than 260 or so characters, and that the NTFS filesystem has no technical reason to restrict these files from existing. However, many programs and utilities baked into Windows still use the maximum path length limit (like explorer.exe - the Windows file manager). I decided that instead of trying to hack Windows into working so that I could get a working copy of the backend server that I am using to host the API and database connections for this project, I could just use a VM. I am already using Hyper-V as part of having Docker running on my dev environment, and Hyper-V has a nearly-one-click solution to installing an Ubuntu 18.04 LTS VM on Windows, so I set that up as my designated Python 2.7 + Django 1.11 development instance so that I could handle the long paths required for this project to run successfully.\nHaving set up an Ubuntu 18.04 LTS VM on my main Windows machine, I installed git, vscode, and virtualenv for Python, along with Python27 for Ubuntu so that I could get a working virtual environment set up for this project before I cloned in the existing backend project with all it's dependencies and get things up and running before starting my new backend app within that framework.\nBut then I remembered something - Django is built around app reusability and so I went back to my app structure and moved it out into a super basic from-scratch Django project which allows me to work on any machine to build the app, without any dependence on the existing older project structure. This approach uses Django 1.11 to match the work environment's Django version, but uses the Python 3.7 version that I have installed on most all of my development machines - this shouldn't be too problematic, as Django 1.11 supports Python 3 and I have been taking steps to ensure that the coding I'm doing is Python 2 and 3 compatible. I'll edit here if this turns out to be a bad idea.\n","permalink":"https://chasesawyer.dev/post/2019/02/maintaining-older-django-and-python-projects/","tags":["post","django","python","virtualenv"],"title":"Maintaining Older Django and Python Projects"},{"categories":null,"contents":" Author's Note: this post has taken a long time to get written - so long, that I already have my 2018 Hactoberfest t-shirt and stickers! It's content spans mid-October through December. I've done my best to make it a cohesive whole.\n Happy Hacktoberfest! This has been the second year that I've participated, and I'm looking forward to getting my t-shirt and stickers when they're available from DigitalOcean. Really briefly, here's a list of things that I've been working on and doing in the last few months since my last update.\nI've started working through the project prompts on The Odin Project - Still finding it difficult to stick with these self-directed learning sites. The advantage I think that The Odin Project has over Free Code Camp is that their projects are more easily shown off to other people. So far I've managed a couple project solutions without having to set up anything other than the GitHub repository where my fork's code lives. The project code is all served from GitHub pages by having a gh-pages branch pointing to the desired live version of the page project.\nThe following have been completed:\n etch-a-sketch Rock-Paper-Scissors  I'll admit, it's not a terribly impressive list. But in my defense, work has come up with a way to keep me thoroughly engaged by giving me the capacity to pitch, prioritize, and execute upon projects of my own to improve processes and practices. What this has boiled down to is:\n 4 projects pitched in August/September 4 approved projects 1 active project in early development / preparation stage (described below) 3 projects scheduled / prioritized for execution following active project 1 developer (me)  Given the constraint of me as Project Manager, Lead Developer, and Architect, I've been pretty busy taking on one of the heaviest lifts I've ever engaged in professionally: bring a year-old React app up to date, with a database backend (from scratch), and analytics/live dashboards. You might think that this seems pretty easy, so let me explain further the current state-of-affairs.\nThe Active Project The react app is served statically from a generic managed LAMP server, which I have no access to. The server's resources are behind the UW's NetID authentication, but the actual React site has nothing to do with this, you simply must be authenticated in order to load the app at all.\nThere is no backend in any sense that I've ever thought of as a \u0026ldquo;backend\u0026rdquo;. It's Google Sheets. All of it. Dynamic elements of the React site are all fields set up in various tabs of a single Google Sheets spreadsheet. Changing values in particular cells of the spreadsheet change the values displayed in the React app. The React app then uses those values to modify forms and dynamically change so that users can submit information about classroom checks (problems and details about them, or everything is fine). This information populates a \u0026ldquo;form responses\u0026rdquo; tab in that same spreadsheet, which then is checked by a formula in another tab, and that subsequently updates the React app.\nWhat black magic makes all this data go around without collapsing? Firebase. And Google Apps Scripts.\nSo Google Apps Scripts allow you to use Javascript to modify Google Docs, Sheets, etc. with custom functions and the like (such as macros). These scripts (can) have triggers, which can be time based (I'd say, cron-adjacent) or based on actions that happen with the associated Google Sheet or Doc. Firebase is a no-sql document object store with some really fancy real-time update functionality built into their libraries that provide some attractive quality-of-life features to app developers - specifically, updates to date in Firebase are automagically synced to all online clients, and any offline changes from clients are cached locally and later synced when that client goes back online.\nIn our case, Google Sheets holds all the data, any change to any part of the document triggers an Apps Scripts function that uploads all te watched cells to Firebase. Those changes are live-synced to any clients watching (clients being those with the main React app loaded). These triggers are dependent on user interaction. The opposite direction data-flow (the React app to Google Sheets) is not realtime. Or not quite. Form data filled out in the React app is uploaded to Firebase into a sort-of update queue. This is where data lives until cleared out by another Apps Script attached to this Google Sheet. This function has a time-based trigger that runs as often as Google allows: every minute of every day forever. Each minute (or so), this script checks the Firebase database (\u0026lsquo;queue\u0026rsquo;) to see if there's been a change to the queue, and if so, it pulls that data and populates a new row of the form responses tab, then clears that data from the Firebase queue, which causes another update to Firebase, because that new incoming data changed the Sheets, which means the Firebase data needs updating, which will cause an update to any listening React clients.\nIt sounds complicated, and it is. Learning how everything was put together in the first place took some time to learn, and then maintaining it is another battle entirely. The whole apparatus depends on triggers associated with the Google Apps Scripts, which are directly attached to their relevant Google Apps document.\nSome more background: Google Apps scripts can be either standalone, or they can be attached to a document. If they are standalone, then they exist and behave much like other Google documents or spreadsheets (they show up like another ). They are little files that exist somewhere. If they are attached to a document, then those scripts do not actually have any independent identity. They are intrinsically linked to whatever document that they were created in. This means that they are not easily found for shared documents, like the ones that we are dealing with here. They also have a peculiar association with other Google Cloud applications and functions, in that they appear to be projects, and have similar properties, but you cannot see the engine or virtual machine they run within, nor are they cloud functions per se, and their API usage doesn't necessarily show up in the Cloud Console. (This could just be my misunderstanding how these things work, but this has been my observation thus far). To edit scripts, you need to find them in the Google Apps Scripts developers console (G Suite Developer Hub), or by first finding the document that they're attached to and then opening that document's scripts.\nNow, about those triggers: until very recently there was no way (that I could find) to find the triggers associated with a shared document/spreadsheet attached apps script, unless you were the user that created that trigger. Now in the last couple of weeks (since November 20th or so) they allow other users that can see the shared document/script to see that there are triggers, but not who that user is or a whole lot of detail about the trigger parameters. (YMMV - this is taken from the perspective of a user within an organization (the UW) where documents are shared amongst many users (within one of our Team Drives)). When I first began looking into taking over this project, I couldn't see any triggers attached to any of the functions that were associated with our master Google spreadsheet, so to me, it was clear that these functions were happening, but I couldn't see how. This led to a failure for our crew one night, since (for reasons unknown) the magical invisible triggers just stopped working. Now, this probably wasn't the case, but since I could never see the triggers responsible for all the data moving back and forth, I also couldn't see that they weren't there or if they were broken in some other way. I had anticipated this, and the resolution was just to re-create these triggers under my own UW Google account.\nGoing forward, this is fine, but it brings me back to another fundamental flaw in this kind of system: Transitioning these resources over to a new team member/user. Triggers can't be moved/transferred/shared/etc. And similarly Scripts that are associated within a Googel Doc or Sheet are intrinsically tied to it. They can't move away from that document, and if that document is deleted, then the Script associated with it goes too. Making your scripts separately from your G Suite applications seems like a wise decision from the outset, but that wasn't immediately clear when these scripts were initially made. Of course copy-paste makes moving their content to local files or to independent Scripts on Google would ostensibly be a valid way of breaking this interdependency, it's still no Git. And of course today I used this very functionality to create two Slack Custom Integrations that run within this script framework with daily triggers to create some automated notifications for staff. These scripts are trivial so I feel no particular attachment to making sure they don't disappear suddenly, but on the other hand, for something more mission-critical, it's more concerning.\nLong term I'm going to work out a way of potentially deploying these Apps Scripts programmatically, so that they can live somewhere else and be deployed through some kind of automated system? (looking at you Github Actions\u0026hellip;)\nOngoing Development Ok, so here's a small list of the things that need to happen (completely out of order, some already completed): Write a project charter, and try to anticipate the speed at which decisions will be made within a bureaucratic government organization during the holiday season). Update Create-React-App (and React), all their dependencies, the Google Firebase library, and any other utility libraries. Audit the list of packages in package.json and determine what can stay and what is extraneous. Add eslint and settle on some rules to bring some order to the code base. Update React components to be more modular - break out what needs separation, cull redundancy, and prepare the app that is using Firebase as it's backend ready to switch over to a SQL database for a backend. Build out the architecture of the SQL database. Build an API between the database and the app. Build a Tableau dashboard and report set that pull from the SQL database. Build in Business Logic everywhere to ensure the core data in the SQL database is and remains valid into the future. Clean up and transition legacy data over to the SQL Database. Decide on what flavor of Database to have in the first place. Figure out how to get the app to be integrated into the UW NetID (Shibboleth) SSO, and use information about the users to control what users can and cannot do, including who can manage data in the SQL database (via a manager view within the app). Build a manager interface within the app.\nIt's an ambitious project, but one that I feel like I have all the pieces for, and the actual plumbing of all the parts and figuring out how they all go together is the most fun and enjoyable part of all of this.\n","permalink":"https://chasesawyer.dev/post/2018/10/long-time-update-new-projects/","tags":["post","update","uw","cloud"],"title":"Long Time Update; New Projects"},{"categories":null,"contents":"When developing an application or project with Node.js, debugging is an important aspect to getting to the bottom of issues. If you use an IDE (integrated development environment) or code editor that has built-in or supported debugging capacities, such as Visual Studio Code, then you have that ability already. But if you are someone who's developing with a more basic code editor - such as Sublime Text, then you can still debug your Node.js application, using Chrome Developer Tools as your debugger, as if you were debugging a live website.\nBackground - Node Debugging Node creates a server and runs your application. When debugging, you need a client to talk to the server to tell it when to stop execution, catch errors, and to set/catch breakpoints in the code. This is called an inspector client, and that interface is what is presented to you in the Chrome Developer Tools as well as when using the debugger mode in IDEs.\nSet up For this, you can use any Node server app. We'll be launching the app from the command line, and then open up Chrome and connect to the running Node server, which will be running in debug mode. Initiating the app in this way, it won't actually begin execution until the inspector client (Chrome) connects to Node server.\nStart Node in Debug mode If you normally start your Node project server with something like\n1  node app.js   Open a terminal in your Node project folder, and start it with the command\n1  node --inspect-brk app.js   You should then be greeted with a message like this:\nDebugger listening on ws://127.0.0.1:9229/9b3ec4f2-b590-4372-b34c-1s4affc3a345 For help see https://nodejs.org/en/docs/inspector Then open chrome to the address chrome://inspect\nFrom here, you should see your Node instance listed, including the file path to the app that's running. Click the \u0026ldquo;inspect\u0026rdquo; link below the file directory shown, and Chrome DevTools will open up. In the console, you should now see\nDebugger attached. At this point, your code still hasn't actually begun running yet. DevTools has attached to the Node debugging instance, and has frozen your code at the very beginning of app.js. DevTools will now let you explore your code, and insert breakpoints - places where you want code to halt - before it starts executing your code.\nFrom here you can begin to debug your code to your heart's content, and dig into pesky bugs that might be plaguing you. Using an interactive debugger is also very useful for catching the state of your app when it crashes (runs into an exception) as the debugger will halt at that point, and you can inspect the state of your variables and dig deeper into what may have gone wrong to cause the exception.\nFor a more in-depth guide on debugging and how to navigate Chrome's DevTools, check out the links below:\n https://developers.google.com/web/tools/chrome-devtools/ https://nodejs.org/en/docs/guides/debugging-getting-started/  For VS Code:\n https://code.visualstudio.com/Docs/editor/debugging  As a side note, most modern browsers have debugging tools, and they all behave pretty similarly, with the exception of mobile browsers - mobile browsers (to my knowledge) don't come with the ability to debug the code running on them. But if you are debugging a mobile page, then you can run a page as if in a mobile browser from within Chrome or Firefox by opening the DevTools and entering Responsive Design mode. Check out https://developer.mozilla.org/en-US/docs/Tools/Responsive_Design_Mode for Mozilla's implementation of this tool, and how it can be helpful in quickly testing your site's reaction to small screens.\n","permalink":"https://chasesawyer.dev/post/2018/06/debugging-node-applications/","tags":["post","blog","node","javascript","debugging"],"title":"Debugging Node Applications"},{"categories":null,"contents":"This week I had an interesting discussion with another new developer who was getting started working on an Express-based project and was frustrated by their static files working one day, and seemingly without provocation, not working the next. I knew from experiencing the same feeling when dealing with static files both in Django and Express that a static file loading problem is difficult to resolve, and how often the problem is often a simple one that is nonetheless opaque to a new developer unfamiliar with file systems and path resolution.\nWhen static file loaders can't or don't load something, it's often not called out as an error because when you tell the computer where the static files are, it doesn't make any assumptions about the contents there. And later on when you're trying to load your custom CSS or images and getting nothing loaded, and quiet browser 404 errors for the resource, but not the page overall, the result can be a maddening series of digging into documentation that doesn't often make clear the particulars of path names and how a single dot or slash can make all the difference in the world between success and failure.\nFile Paths A path to a file on a computer, and to an extent, a path to a resource on the internet looks like this: /rootDirectory/subDirectory/file with the file part usually having some kind of type after a dot, such as .txt or .exe. For Windows users, the beginning will have a drive letter at the beginning, most commonly C: C:\\rootDirectory\\subDirectory\\file.\nWhat happened to me, and I'm sure has happened to others - such as the individual I was talking to this week - is that nothing in the documentation about static paths goes deeply into path resolution. I guess it's just kind of assumed that you as the developer in control of things should know what's happening in the background, but many do not.\nPath Resolution When you give Express.js (Node) a path to load, it's generally going to be in a relative format. The documentation gives the most basic setup as something like app.use(express.static('public')). The mechanics behind this statement are a bit more complicated than it might seem at first glance.\nWhen you give Node a path such as 'public', a function goes through and makes a best guess as to where you mean to point it. Why doesn't it just know that you mean the folder called \u0026lsquo;public\u0026rsquo; (or \u0026lsquo;static\u0026rsquo; or \u0026lsquo;img\u0026rsquo; or whatever)? Because Node needs the absolute path on the file system to that directory, and thus the files and folders inside. And this function is pretty lenient about what it will accept in terms of file paths, and will assume that you know what you're doing.\nSo, what is the difference between 'public', '/public', and './public'?\nThe first and the last actually resolve to the same destination: relative to the current working directory, find a directory called \u0026lsquo;public\u0026rsquo; and serve the contents as static files. However, the second example resolves to something else entirely: find a directory called \u0026lsquo;public\u0026rsquo; at the root of the file system. Or more explicitly: instead of resolving to /rootDirectory/subDirectory/project/public/ it will resolve to /public/. Or in Windows, C:\\public\\\nWhen path.js (in Node) runs through resolve() for a given path name, there's this comment in the function:\n1 2 3 4 5 6 7  /* Lines 201 - 206 of path.js */ if (code === 47/*/*/ || code === 92/*\\*/) { // Possible UNC root  // If we started with a separator, we know we at least have an  // absolute path of some kind (UNC or otherwise)  isAbsolute = true;   Which if you walk through debugger with the above three options, it becomes clear that the middle option '/public' will resolve to an absolute path, rather than a relative path to the current directory where you're working, and that will make all the difference in the world.\nNow, if you do want to give an absolute path from the root of the drive you're working on, then you have a way of doing that by starting the path off with a \u0026lsquo;/\u0026rsquo; - otherwise you'll need to use relative paths with either a \u0026lsquo;./\u0026rsquo; or just the folder/file you're looking for without anything else.\nI hope that this helps someone out there understand file paths and relieves some frustration around what app.use(express.static('public')) actually means.\n","permalink":"https://chasesawyer.dev/post/2018/06/file-paths-in-node/","tags":["post","blog","express","javascript","node","filesystem","debugging"],"title":"File Paths in Node"},{"categories":null,"contents":"This project involved setting up a couple MongoDB collections and then building a responsive frontend for user interaction. This app allows adding users by name, then using that user's ID to add activities. The API allows for querying for user activities by user ID as well.\nThe UI uses a pug template and bootstrap for most styling, and some custom css rules to fine tune elements.\nThis was actually the first project that I completed out of the new Free Code Camp curriculum, and replaced the previous Google Image Search Abstraction project that I had already completed previously.\nCode View on GitHub¬† 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155  const mongoObjectId = require(\u0026#39;mongodb\u0026#39;).ObjectID const trackerCollection = \u0026#39;exerciseTracker\u0026#39; const trackerUserCollection = \u0026#39;exerciseTrackerUsers\u0026#39; // Challenge 4v2 - Exercise Tracker - UI page exports.uiPage = function (req, res) { res.render(\u0026#39;exercise/index\u0026#39;) } exports.addUser = function (req, res) { // lookup username; insert and return ID if not taken; otherwise return error message.  const collection = req.app.dbConn.getDB().collection(trackerUserCollection) const lookup = collection.find({ username: req.body.username }).toArray(function (err, documents) { if (err) { console.log(err) res.status(500).send(\u0026#39;Unable to complete action.\u0026#39;) } else { if (documents.length \u0026gt; 0) { // username was found  res.status(400).send(\u0026#39;Username \\\u0026#39;\u0026#39; + req.body.username + \u0026#39;\\\u0026#39; already taken.\u0026#39;) } else { collection.insertOne({\u0026#39;username\u0026#39;: req.body.username}, function (err, result) { if (err) { console.log(err) res.status(500).send(\u0026#39;500 server error; unable to save username.\u0026#39;) } else { res.json({ \u0026#39;username\u0026#39;: req.body.username, \u0026#39;_id\u0026#39;: result.insertedId }) } }) } } }) } exports.addActivity = function (req, res) { // given the userID, activity description, duration, and optionally, date  // look up the userID - if it exists, enter a new data entry about the activity to the database  // if the user id doesn\u0026#39;t exist, respond with a 400 user not found message  var dateEpoch = new Date().getTime() if (req.body.date) { dateEpoch = new Date(req.body.date).getTime() } const collection = req.app.dbConn.getDB().collection(trackerUserCollection) const lookup = collection.find({ \u0026#39;_id\u0026#39;: mongoObjectId(req.body.userId) }).toArray(function (err, documents) { if (err) { console.log(err) res.status(500).send(\u0026#39;Unable to complete action.\u0026#39;) } else { if (documents.length \u0026gt; 0) { // user found  const updateCollection = req.app.dbConn.getDB().collection(trackerCollection) const update = updateCollection.insertOne({ userId: documents[0]._id, description: req.body.description, duration: req.body.duration, date: dateEpoch }, function (err, result) { if (err) { console.log(err) res.status(500).send(\u0026#39;500 server error: unable to add entry.\u0026#39;) } else { // save successful  res.json({ \u0026#39;username\u0026#39;: documents[0].username, \u0026#39;description\u0026#39;: req.body.description, \u0026#39;duration\u0026#39;: req.body.duration, \u0026#39;_id\u0026#39;: documents[0]._id, \u0026#39;date\u0026#39;: new Date(dateEpoch).toDateString() }) } }) } else { res.status(400).send(\u0026#39;Unable to locate userID: \u0026#39; + req.body.userId) } } }) } exports.getUserLog = function (req, res) { // return user exercise data given the user\u0026#39;s id.  // Check for optional parameters and validate them:  var opts = {from: null, to: null, limit: 0} var queryParams = { \u0026#39;userId\u0026#39;: mongoObjectId(req.query.userId) } if (req.query.from \u0026amp;\u0026amp; req.query.to) { if (!isNaN(Date.parse(req.query.from)) \u0026amp;\u0026amp; !isNaN(Date.parse(req.query.to))) { queryParams[\u0026#39;date\u0026#39;] = { $gte: Date.parse(req.query.from), $lte: Date.parse(req.query.to) } } } else if (req.query.to) { if (!isNaN(Date.parse(req.query.to))) { queryParams[\u0026#39;date\u0026#39;] = { $lte: Date.parse(req.query.to) } } } else if (req.query.from) { if (!isNaN(Date.parse(req.query.from))) { queryParams[\u0026#39;date\u0026#39;] = { $gte: Date.parse(req.query.from) } } } if (req.query.limit \u0026amp;\u0026amp; !isNaN(req.query.limit)) { opts.limit = Math.ceil(Math.abs(req.query.limit)) } if (!req.query.userId) { res.status(400).send(\u0026#39;Must query for an ID.\u0026#39;) } else { const usersDb = req.app.dbConn.getDB().collection(trackerUserCollection) const activitiesDb = req.app.dbConn.getDB().collection(trackerCollection) const lookupUser = usersDb.find({ \u0026#39;_id\u0026#39;: mongoObjectId(req.query.userId) }).toArray(function (err, userDocs) { if (err) { console.log(err) res.status(500).send(\u0026#39;Unable to query user.\u0026#39;) } else { if (userDocs.length \u0026gt; 0) { // user found, can find their stuff.  var activitiesCursor = activitiesDb.find(queryParams) if (opts.limit) { activitiesCursor = activitiesCursor.sort(\u0026#39;date\u0026#39;, 1).limit(opts.limit) } activitiesCursor.toArray(function (err, activitiesDocs) { if (err) { console.log(err) res.status(500).send(\u0026#39;Unable to query user activities.\u0026#39;) } else { var resData = { _id: req.query.userId, username: userDocs[0].username, count: activitiesDocs.length, logData: [] } for (var i = 0; i \u0026lt; activitiesDocs.length; i++) { resData.logData.push({ \u0026#39;description\u0026#39;: activitiesDocs[i].description, \u0026#39;duration\u0026#39;: activitiesDocs[i].duration, \u0026#39;date\u0026#39;: new Date(activitiesDocs[i].date).toDateString() }) } res.json(resData) } }) } else { res.status(400).send(\u0026#39;Unable to locate userID: \u0026#39; + req.query.userId) } } }) } }   Demo View this code live here: https://fcc-challenges.herokuapp.com/exerciseTracker/\n","permalink":"https://chasesawyer.dev/post/2018/06/exercise-tracker-full-stack-app/","tags":["sample","javascript","freecodecamp","mongodb","express","pug"],"title":"Exercise Tracker Full-Stack App"},{"categories":null,"contents":"Word processors are great for creating documents with a lot of rich formatting, but all that stuff can be a huge distraction. Code editors on the other hand are amazingly good for writing - the writing where the only thing that actually matters is the content, and not the format.\nWhile it's probably been hashed out and written about a lot by other people, the one thing, the killer aspect of it for me, is the ability to scroll the page down, so that whenever you want, you can always return to that feeling of having a blank canvas to write upon. I really love the blank page, and I love writing on something fresh, and so far, code editors are the only things that I've seen do this. The word processors - think Google Docs, Microsoft Word, etc. - all stop you at the bottom of the current virtual piece of paper you're writing on and won't let you progress until you flow over onto the next page. Now, of course there's hacks around this, including just inserting a page break or two at the end of whatever it is you're working on. However, this has always felt really artificial to me. Plus there's always those page breaks and virtual pieces of paper that physically break apart your writing. It can be distracting.\nBut to be fair to word processors, I don't think anyone writes their blog posts in them, and I don't think that anyone writes their papers that are meant to be printed in a blog editor. There are always exceptions, of course, such as those that use LaTex for all their typesetting needs (I don't really know much about LaTex other than that it exists and that it is very useful for document formatting, and printing math equations).\n","permalink":"https://chasesawyer.dev/post/2018/06/code-editors-vs.-word-processors/","tags":["post","blog","nonesense"],"title":"Code Editors vs. Word Processors"},{"categories":null,"contents":"I got my back-end (now API and Microservice) cert today from Free Code Camp! https://www.freecodecamp.org/certification/shadowimmage/apis-and-microservices Hooray!\nIt's been a year since I started working through the Free Code Camp curriculum, and it feels good to finally have finished the API work. I feel like I have a pretty solid grasp now on how to build and arrange an API, which I hope will help me in the future when I try to do more API work.\nOver the time that I've spent on getting this certificate I've learned a lot of extra things on top of just how to build an API or microservice, which is also part of the reason that it took so long to get here. From the beginning of the projects on Free Code Camp, I decided that I wanted to wrap everything up together into a cohesive portfolio - something that I could showcase and present to the world that had everything packaged and ready to consume. It also added a whole layer of complexity on top of the challenges that required more research and more work, putting separate parts together into a single application and then designing and building the architecture of the portfolio where it'd be showcased. This actually led to two separate projects, the presentation site (this) where I could write up details in longer form and document my work over time, and the project site where all the projects and samples would live together in one application that could be expanded upon as I completed more challenges and coding solutions. I think this played to each platform's strengths as well, since Github Pages is great for static sites and is always online, and Heroku projects can be almost anything you want them to be and won't cost any money (at least for the free tier) - with the only downside being that the first visit to a sleeping \u0026lsquo;Dyno\u0026rsquo; is delayed by a few seconds as the server instance spins up.\nIn the end what really slowed me down in getting this certificate wasn't the actual challenges - each one really only took a few hours of work. It was the external factors, life factors, that made the difference and introduced the largest delays and distance from the work. Things like buying a house, renovating and moving into said house, the relentless autumn + winter holiday madness, and changes in demands at work and at home. Suddenly a year passes, your kid is older, you're older, and you've neglected your repos for so long that you don't remember why you made this branch or what wasn't working or done last time you committed your code.\nNot having time to dedicate to the work has made it harder to progress than I thought when I first started out. And I'm hoping that now that I have momentum and am starting to tell people about the things that I'm putting out there (rather than just working in obscurity) that I can maintain that energy and continue to get things done, become more confident, and make some cooler and bigger things.\n","permalink":"https://chasesawyer.dev/post/2018/06/new-free-code-camp-cert/","tags":["post","blog","update","certificate"],"title":"New Free Code Camp Cert"},{"categories":null,"contents":"Repair Task Tracker RTT is a full stack app that addresses the needs of a computer hardware management process, allowing the tracking and resolution of issues/problems with the hardware, as well as the configuration and components of each major hardware item. RTT is meant to be a back-of-house tool, replacing paper tickets and spreadsheets. The goal of this project was to implement a GraphQL app, with useful data, allowing a seamless user experience as they operate through the app, and data is downloaded and uploaded in the background.\nDevelopement Status This project is ongoing development. Progress details are in the project readme.\nTechnologies RTT is a single-page-app built in Vue.js and served from a Python/Django server running within a Heroku dyno (server instance). The data records are stored in a PostgreSQL database, retrieved with a GraphQL implementation using Apollo and Graphene.\nDemo The backend database schema has been completed; the frontend is still under development. The latest snapshot is live on heroku.\n   Display of all open issues in the database\n      Detail page and update fields for selected issue\n    ","permalink":"https://chasesawyer.dev/project/rttapp/","tags":["project","python","heroku","vue","javascript","postgresql","graphene","apollo","graphql"],"title":"RTTApp"},{"categories":null,"contents":"The Keys App was my first major project that sought to solve a problem with managing key checkouts without requiring a cumbersome customer database / sign up form. This would target an institution that mostly catered to internal customers, such as a university's AV department and it's instructors. It largely replicates a paper-form-based system, with an added layer of data validation and control (emails, phone numbers must be in a valid format; keys can only be returned by their original owners; keys can't be checked out twice; etc.). Each checkout and check-in is recorded and tracked. All changes in the database leave a history.\nThis project was built entirely upon Django, with some Bootstrap for front-end styling. The app runs as a multi-part form with several steps and each is validated before progress is allowed. The backend leverages the built-in Django admin console and allows navigating and editing all records in the system, while also maintaining data integrity.\nTechnologies The Keys App is a full stack project built with\n Python/Django PostgreSQL Bootstrap FontAwesome  Database Schema [To come]\nDemo Hosted on Heroku: https://chase-sawyer-demos.herokuapp.com/keysApp/\n    ","permalink":"https://chasesawyer.dev/project/keysapp/","tags":["project","demo","django","python","postgresql","heroku"],"title":"KeysApp"},{"categories":null,"contents":"I spent my sick day today designing a new logo for the site using Inkscape. I knew that it had to be something that was unique, so I went online looking around for resources on designing and producing a logo. I know that SVGs are great, because you will never have scaling issues - if you need a larger version, you can simply export it at higher resolution. Or lower. Or whatever. I found some sites for inspiration as well, since I couldn't think of exactly what I was going for initially.\nThe new design: logobook.com has been really helpful in looking at what kinds of designs you can do with different features or lettering and Logo Design Love was really helpful in getting my head into a good space for designing a logo. Other than that, the first few iterations of designs were just pen-and-paper drawings in my notebook.\nTools Designing a vector graphic is relatively easy if you have a vector graphic editing program. I have known about Inkscape for a while, and this is what I ultimately used. Another tool I found while looking around for vector graphic tools is a browser-based (or downloadable) tool called Vectr. I didn't use Vectr much, but it looks useful if you need really simple tools and don't want to get something as heavyweight as Inkscape. That being said, using Inkscape and it's exceptional grid and guide snapping tools really made designing this logo easy.\nGetting it online In order to fully implement use of the logo, there's several modifications that I had to make, including converting the .svg file to an .ico or .png format that would be browser friendly. Modern browsers support .svg files for page graphics, but most still don't support the format in the favicon space. Only Internet Explorer requires the format be a .ico for the favicon, and most support .png, but in different levels of quality, depending on the browser and age, and if it's a mobile or a desktop browser (not to mention things like consoles with browsers baked in). I didn't know any of that information starting out though, and instead got a good amount of information from this stackoverflow question.\nChanges to Head 1 2 3 4 5 6 7 8 9  \u0026lt;!--new favicon --\u0026gt; \u0026lt;link rel=\u0026#34;apple-touch-icon\u0026#34; sizes=\u0026#34;180x180\u0026#34; href=\u0026#34;{{ \u0026#34;img/apple-touch-icon.png\u0026#34; | absURL }}\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;icon\u0026#34; type=\u0026#34;image/png\u0026#34; sizes=\u0026#34;32x32\u0026#34; href=\u0026#34;{{ \u0026#34;img/favicon-32x32.png\u0026#34; | absURL }}\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;icon\u0026#34; type=\u0026#34;image/png\u0026#34; sizes=\u0026#34;16x16\u0026#34; href=\u0026#34;{{ \u0026#34;img/favicon-16x16.png\u0026#34; | absURL }}\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;manifest\u0026#34; href=\u0026#34;{{ \u0026#34;img/site.webmanifest\u0026#34; | absURL }}\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;mask-icon\u0026#34; href=\u0026#34;{{ \u0026#34;img/safari-pinned-tab.svg\u0026#34; | absURL }}\u0026#34; color=\u0026#34;#5bbad5\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;msapplication-TileColor\u0026#34; content=\u0026#34;#da532c\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;theme-color\u0026#34; content=\u0026#34;#ffffff\u0026#34;\u0026gt; \u0026lt;!--end favicon --\u0026gt;   This required some minor modifications to the Hugo theme that I've been building off of by changing part of the navbar to include the new monogram svg. Getting the sizing just right required some custom css, which also had to be included in the html head. I also changed the Hugo params in the config file to reflect that the avatar is an avatar, not an icon, and instead use the icon parameter for the file location of the monogram.\nChanges to Nav 1 2 3 4 5 6 7 8 9 10 11 12  \u0026lt;div class=\u0026#34;navbar-header\u0026#34;\u0026gt; + \u0026lt;a href=\u0026#34;{{ \u0026#34;\u0026#34; | absLangURL }}\u0026#34;\u0026gt; + \u0026lt;img class=\u0026#34;navbar-brand\u0026#34; id=\u0026#34;site-logo\u0026#34; src=\u0026#34;{{ .Site.Params.logo | absURL }}\u0026#34; alt=\u0026#34;{{ .Site.Title }}\u0026#34; /\u0026gt; + \u0026lt;/a\u0026gt;  \u0026lt;button type=\u0026#34;button\u0026#34; class=\u0026#34;navbar-toggle\u0026#34; data-toggle=\u0026#34;collapse\u0026#34; data-target=\u0026#34;#main-navbar\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;sr-only\u0026#34;\u0026gt;{{ i18n \u0026#34;toggleNavigation\u0026#34; }}\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026#34;icon-bar\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026#34;icon-bar\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026#34;icon-bar\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;/button\u0026gt; \u0026lt;a class=\u0026#34;navbar-brand\u0026#34; href=\u0026#34;{{ \u0026#34;\u0026#34; | absLangURL }}\u0026#34;\u0026gt;{{ .Site.Title }}\u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt;   ","permalink":"https://chasesawyer.dev/post/2018/05/logo-design/","tags":["post","hugo","update"],"title":"Logo Design"},{"categories":null,"contents":"Full-stack microservice (really basic frontend in Pug/HTML) that takes a FormData object from a file upload form and returns the file size in bytes as part of a JSON response.\nUser Stories  I can submit a FormData object that includes a file upload When I submit something, I will receive the file size in bytes within the JSON response  Code View on GitHub¬† 1 2 3 4 5 6 7 8 9 10 11 12  // upload page exports.upload = function (req, res) { res.render(\u0026#39;filesize/upload\u0026#39;) } // File Metadata Microservice - file upload result exports.result = function (req, res) { res.json({ \u0026#39;filename\u0026#39;: req.file.originalname, \u0026#39;size\u0026#39;: req.file.size }) }   Demo View this code live on Heroku\n","permalink":"https://chasesawyer.dev/post/2018/05/file-metadata-microservice/","tags":["sample","freecodecamp","javascript","demo"],"title":"File Metadata Microservice"},{"categories":null,"contents":"This microservice creates an abstraction layer (and history) between the user and the Google Images search API.\nUser Stories  User Story: I can get the image URLs, alt text and page urls for a set of images relating to a given search string. User Story: I can paginate through the responses by adding a ?offset=2 parameter to the URL. User Story: I can get a list of the most recently submitted search strings.  Code View on GitHub¬† 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86  const https = require(\u0026#39;https\u0026#39;) const imgSearchCollection = \u0026#39;imgSearches\u0026#39; // Challenge 4 - Image Search Abstraction Layer (search) exports.query = function (req, res) { const resultsPerQuery = 10 var localData = { searchTerm: \u0026#39;\u0026#39;, pagination: 1, } if (!req.params.q) { res.json({\u0026#39;error\u0026#39;: \u0026#39;search query required\u0026#39;}) } else { if (req.query.offset) { var offset_tmp = Number(req.query.offset) if (!isNaN(offset_tmp)) { localData.pagination = offset_tmp } } localData.searchTerm = req.params.q var options = { host: \u0026#39;www.googleapis.com\u0026#39;, port: 443, path: \u0026#39;/customsearch/v1?\u0026#39;, method: \u0026#39;GET\u0026#39;, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39; } } options.path += \u0026#39;searchType=image\u0026#39; options.path += \u0026#39;\u0026amp;safe=medium\u0026#39; options.path += \u0026#39;\u0026amp;fields=kind,items(title,link,snippet,image/contextLink,image/thumbnailLink)\u0026#39; options.path += \u0026#39;\u0026amp;key=\u0026#39; + process.env.G_SEARCH_API_KEY options.path += \u0026#39;\u0026amp;cx=\u0026#39; + process.env.G_CSE_ID options.path += \u0026#39;\u0026amp;q=\u0026#39; + localData.searchTerm options.path += \u0026#39;\u0026amp;start=\u0026#39; + Math.max(localData.pagination * resultsPerQuery, 1) const imgReq = https.request(options, function(imgRes) { var output = \u0026#39;\u0026#39; imgRes.setEncoding(\u0026#39;utf8\u0026#39;) imgRes.on(\u0026#39;data\u0026#39;, function (chunk) { output += chunk }) imgRes.on(\u0026#39;end\u0026#39;, function () { localData.imgJSON = JSON.parse(output) var collection = req.app.dbConn.getDB().collection(imgSearchCollection) var lastDoc = collection.find().sort({ index: -1 }).limit(1) lastDoc.project({_id: 0, index: 1}).toArray(function (err, documents) { if (err) console.error(err) var insertIndex = 1 if (documents.length \u0026gt; 0) { insertIndex += documents[0].index } collection.insertOne({ index: insertIndex, query: localData.searchTerm }, function(err, r) { if (err) console.error(err) res.json(localData) }) }) }) }) imgReq.on(\u0026#39;error\u0026#39;, function (err) { res.send(\u0026#39;error: \u0026#39; + err.message) }) imgReq.end() } } // Challenge 4 - Image Search Abstraction Layer (recent searches) exports.latest = function (req, res) { var collection = req.app.dbConn.getDB().collection(imgSearchCollection) var lastSearches = collection.find().sort({ index: -1 }).limit(10) lastSearches.project({ _id: 0, query: 1 }).toArray(function (err, documents) { if (err) console.error(err.message) res.json(documents) }) }   Demo View this code live on Heroku, usage:\nSearch for images by replacing {query} with your query, and paginate through results with {page}.\n https://fcc-challenges.herokuapp.com/api/imagesearch/q/{query}?offset={page}  Show recent queries at the endpoint:\n https://fcc-challenges.herokuapp.com/api/imagesearch/latest ","permalink":"https://chasesawyer.dev/post/2018/05/image-search-abstraction-layer/","tags":["sample","freecodecamp","javascript","mongodb","demo"],"title":"Image Search Abstraction Layer"},{"categories":null,"contents":"The challenge here was to create a URL Shortener microservice. It uses a database to associate a short url ID with the original url, and once created, the microservice will redirect visitors of the short URL to the original URL.\nExample creation input:\n https://fcc-challenges.herokuapp.com/shortener/new/https://www.google.com https://fcc-challenges.herokuapp.com/shortener/new/http://foo.com:80  Example creation output:\n1 2 3 4  { \u0026#34;original_url\u0026#34;:\u0026#34;http://foo.com:80\u0026#34;, \u0026#34;short_url\u0026#34;:\u0026#34;https://fcc-challenges.herokuapp.com/shortener/8170\u0026#34; }   Usage:\n https://fcc-challenges.herokuapp.com/shortener/2871  Will redirect to:\n https://www.google.com/  Demo View this code live on Heroku at fcc-challenges.herokuapp.com/shortener/\u0026hellip;\n URL Creation: new/https://www.google.com Retrieval: 2871  Code View on GitHub¬† Shortener 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35  // URL Shortener (part1) - Short URL Creator exports.new = function (req, res) { var resData = { original_url: \u0026#39;invalid URL\u0026#39;, short_url: null } resData.short_url = req.hostname + \u0026#39;/shortener/\u0026#39; // console.log(req.url)  var url = req.url.slice(5) // console.log(req.url.slice(5))  if (validUrl.isUri(url)) { resData.original_url = url var collection = req.app.dbConn.getDB().collection(shortUrlCollection) var lastDoc = collection.find().sort({ index: -1 }).limit(1) lastDoc.project({_id: 0, index: 1}).toArray(function (err, documents) { if (err) console.error(err) var insertIndex = 1 if (documents.length \u0026gt; 0) { // console.log(documents[0].index);  insertIndex += documents[0].index } collection.insertOne({ index: insertIndex, url: resData.original_url }, function(err, r) { if (err) console.error(err) resData.short_url += insertIndex res.json(resData) }) }) } else { //end valid url section  res.json(resData) } }   Resolver 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  // URL Shortener (part 2) - Short URL resolver/redirector exports.getId = function (req, res) { if (req.params.id) { var collection = req.app.dbConn.getDB().collection(shortUrlCollection) var shortDestDoc = collection.find({ index: parseInt(req.params.id) }).project({ _id: 0, url: 1 }).toArray(function (err, documents) { if (err) console.error(err) if (documents.length \u0026gt; 0) { res.redirect(documents[0].url) } else { res.end(\u0026#39;Invalid short URL id.\u0026#39;) } }) } else { res.end(JSON.stringify({\u0026#39;error\u0026#39;:\u0026#39;invalid URL\u0026#39;})) } }   ","permalink":"https://chasesawyer.dev/post/2018/05/url-shortener-microservice/","tags":["sample","freecodecamp","javascript","mongodb","demo"],"title":"Url Shortener Microservice"},{"categories":null,"contents":"The goal for this one is to get return to the user the IP address, language, and operating system of the browser/user making the request.\nCode View on GitHub¬† 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  // whoami.js // FreeCodeCamp // Backend Challenge 2 - Get requesting client IP Address exports.who = function (req, res) { var resData = { ipaddress: null, language: null, software: null } resData.ipaddress = req.ip if (req.header(\u0026#39;accept-language\u0026#39;)) { resData.language = req.header(\u0026#39;accept-language\u0026#39;).split(\u0026#39;,\u0026#39;)[0] } if (req.header(\u0026#39;user-agent\u0026#39;)) { var userAgent = req.header(\u0026#39;user-agent\u0026#39;) var lParenIndex = userAgent.indexOf(\u0026#39;(\u0026#39;) var rParenIndex = userAgent.indexOf(\u0026#39;)\u0026#39;) if (lParenIndex \u0026gt; -1 \u0026amp;\u0026amp; rParenIndex \u0026gt; -1) { resData.software = userAgent.substr(lParenIndex+1, rParenIndex-lParenIndex) } } res.json(resData) }   Demo View this code live on Heroku at fcc-challenges.herokuapp.com/api/whoami\n","permalink":"https://chasesawyer.dev/post/2018/05/request-header-parser-microservice/","tags":["sample","javascript","freecodecamp","demo"],"title":"Request Header Parser Microservice"},{"categories":null,"contents":"The goal is to create a microservice that will take a date string or a unix timestamp and make a JSON response with both versions of the given timestamp / date.\nCode View on GitHub¬† 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  // timestamp.js // Challenge 1 - Timestamp conversion UNIX \u0026lt;--\u0026gt; Standard  exports.convert = function (req, res) { var timestamp = req.params.timestamp var resData = { unix: null, natural: null } if (!timestamp) { res.json(resData) } else { const months = [\u0026#39;January\u0026#39;, \u0026#39;February\u0026#39;, \u0026#39;March\u0026#39;, \u0026#39;April\u0026#39;, \u0026#39;May\u0026#39;, \u0026#39;June\u0026#39;, \u0026#39;July\u0026#39;, \u0026#39;August\u0026#39;, \u0026#39;September\u0026#39;, \u0026#39;October\u0026#39;, \u0026#39;November\u0026#39;, \u0026#39;December\u0026#39;] if (isNaN(parseInt(timestamp))) { // is a string  var date = new Date(timestamp) resData.natural = months[date.getUTCMonth()] + \u0026#39; \u0026#39; + date.getUTCDate() + \u0026#39;, \u0026#39; + date.getUTCFullYear() resData.unix = Math.floor(date.getTime() / 1000) } else { // is a number (expect unix time)  var unixDate = new Date(timestamp * 1000) resData.natural = months[unixDate.getUTCMonth()] + \u0026#39; \u0026#39; + unixDate.getUTCDate() + \u0026#39;, \u0026#39; + unixDate.getUTCFullYear() resData.unix = timestamp } res.json(resData) } }   Demo View this code live on Heroku at fcc-challenges.herokuapp.com/\u0026hellip;\n Unix-style input: api/timestamp/1450137600 Timestamp input: api/timestamp/December%2015,%202015 ","permalink":"https://chasesawyer.dev/post/2018/05/timestamp-microservice/","tags":["post","sample","freecodecamp","demo"],"title":"Timestamp Microservice"},{"categories":null,"contents":"Gotchas In order of the issue being found:\nIs Strange (Hugo) (Template Logic) Hugo templates use strange logic - for conditional statements, Hugo uses Polish or prefix notation for the operators. This meas that instead of writing if this and that, you have to write if and this that. For more complex arrangements of logical conditions, say for a situation in which you need to check three conditions, you have to write it as: if and (first) ( and (second) (third)) which, in a infix notation style, would have been written if first and second and third.\nHugo uses blackfriday Hugo uses blackfriday as it's markdown engine. This is probably totally fine for most situations, except for when you want to make a list with more than 2 levels. For instance,\n first  second  third      (in markdown:)\n1 2 3  - first - second - third   In the above case, blackfriday normally will collapse the second and third levels both into the second level. I say normally because most linters and stye guides specify an indent of 2 spaces for markdown [citation needed]. If you come from github markdown style, then this is likely the case for you. Unfortunately there's an annoying bug in blackfriday that requires that the indent for each level be 4 spaces. If you don't notice this when scanning through the Hugo documentation on content formats, you'll likely run into a situation where you're confused about your list ending up somewhat flattened.\nFortunately, most linters can be configured to have different indent settings. For Visual Studio Code (my current editor of choice), markdownlint can be configured with a .markdownlint.json file in the root of your project directory or in your .vscode\\settings.json workspace settings file. This isn't a show stopper, but for me, wasted some time while I had to dig through to find the issue report in blackfriday and then more time to read the markdownlint documentation and figure out how to reconfigure the indent setting to make sure that my lists in my Hugo site markdown will pass.\nQuestion for later:\nCan I catch this / use markdownlint in CircleCI builds?\nUpdate January 2020 Hugo has grown since\nHugo Supports Emoji in Markdown This wasn't super hard to figure out, but could be easier to find info on (I actually think that the Hugo documentation could use a way better table of contents, and better search). I nearly added an emoji css library to my site head in order to support emoji via \u0026lt;i\u0026gt; tags inline in the markdown file, but fortunately found that emoji are natively supported by Hugo, but you have to enable it with the enableEmoji = true setting in the site's root configuration. Once you enable emoji support for content files with that setting, you can easily insert emoji with their names between colons (eg üò©).\nProtip: want larger emoji on your page? Have a header with an emoji rather than plain text. The emoji will scale to the predefined scale of the header. (Note: this will also anger the markdownlint gods, as the trailing \u0026lsquo;:\u0026rsquo; isn't allowed)\nüòÑ ","permalink":"https://chasesawyer.dev/post/2018/05/hugo-gotchas/","tags":["post","fyi","note","help"],"title":"Hugo Gotchas"},{"categories":null,"contents":"I woke up late today, so I didn't have time to make coffee. And instead of going to my usual place where I know there's going to be great coffee available, I decided to get a doughnut and some coffee from the doughnut shop. The first warning that this wasn't going to go well was when I got my travel mug back and there was coffee all over the outside of the mug. All day since then I've been drinking it slowly (8 hours) and it's just been terrible. It's really made me sad, because I paid money for this coffee, and this is Seattle. I could have gone to the Starbucks down the block from this doughnut shop, but instead decided not to in favor of making one stop, rather than two on my way into work.\nI should have just waited longer and gone to my usual place.\n","permalink":"https://chasesawyer.dev/post/2018/05/bad-coffee/","tags":["blog","coffee","fwp","short"],"title":"Bad Coffee"},{"categories":null,"contents":"I took some time this week to look into all the code that I've been collecting and producing over the last year or more and began to assess where everything was being stored and hosted. Since I build a lot of demos and try out new technologies on a pretty consistent (if not sporadic) basis, this has led me to a state where I have several projects hosted on Heroku, on AWS, and offline. So I've been looking for an efficient way to bring all these projects, ideas, code samples, demos, etc into one central hub or site.\nWith the redesign of this site, I have the opportunity to collect and organize everything together in a much more cohesive format that will hopefully be easy to navigate for visitors and myself. I'm hoping that this can effectively become something of a portfolio as well as a digital notebook that I can maintain and grow.\nStructure Proposed structure for the site arrangement and layout:\n Profile / home page (https://chasesawyer.dev)  Projects/Ideas  MyApps (Heroku: App Demos)  keysApp rttApp   Work stuff  SlackR25Bot     Samples  FreeCodeCamp challenges, etc (Heroku: fcc-challenges)   About Me  Traditional Resume (?) Contact info  social contact links (footer)     Architecture notes as a separate section (?)    Aside:\n Turns out that blackfriday, the markdown engine for Hugo has a bug (#329) in nested lists that requires 4 space indentation per level, rather than the usual 2 that most other markdown engines use. This threw me off for a long time until I was able to track down the issue. I'm beginning to think that I need to write up a \u0026lsquo;gotchas\u0026rsquo; page for Hugo Here is the gotchas page\n üò© ","permalink":"https://chasesawyer.dev/post/2018/05/new-site-structure/","tags":["post","update"],"title":"New Site Structure"},{"categories":null,"contents":"Built a new site, leaving Jekyll for Hugo, for my github.io page.\nChanges Moved all my old Jekyll files to a new subdirectory in order to maintain access to the old code and posts, and then transition all the content over to the main Hugo site as settings come together. One major advantage of Hugo is that the build process is super fast, so I've been looking at how to integrate CircleCI with Hugo builds. Fortunately the CircleCI engineering blog has a post on how to get set up doing automated builds via CircleCI.\nCircleCI One of the caveats of using Github Pages for hosting my static content is that I have to have the site content in the master branch. I had already branched my project to start working with Hugo to the hugo branch, with the intent that this branch would eventually merge back with master. After reading a bit more about how Hugo works, it seems that the hugo branch is going to be a permanent feature, with master being overwritten with the content of the /public output from the Hugo build process. The hugo branch will remain indefinitely as the new default branch on the site repository.\nOn the other hand, this will work out fine, as it will let me ser CircleCI up to pull the current hugo branch when updated, and then upon successful build, will be able to push the resulting build back up to the repository's master branch. This automated build process is perfect for what I'm hoping to achieve, and will let me get more experience with CircleCI, which I haven't touched in months!\n","permalink":"https://chasesawyer.dev/post/2018/05/new-static-site-with-hugo/","tags":["post","Hugo","update"],"title":"New Static Site with Hugo"},{"categories":null,"contents":"This project was built for Classroom Technologies \u0026amp; Events at the University of Washington.\nView on GitHub¬†   Background The University has been updating their course scheduling from an onsite instance of R25 (unable to link because documentation doesn't exist on the internet anymore) - a product from CollegeNET. Part of that process involved moving the scheduling database off-site. This allowed for a new integration to our Slack instance that could contact the R25 web service, now hosted on the CollegeNET servers, rather than secured locally on one of the UW's servers.\nTimeline I came up with the idea for this project around April 18th, 2018, after talking to some colleagues about transitioning other internal tools that leveraged the local SQL database over to the R25 web service after the system transitioned to the cloud. Our department had recently been granted access to the web service, and I had spent the earlier part of the month translating Python scripts that had been digesting local SQL results to talking to the R25 web service and digesting XML instead. I spent about a week researching the possible integrations between Slack and external apps, and decided to use a Slack Slash Command for the integration's implementation. The first version of the app was released on May 4th.\nImplementation Full details on the README\nTo quickly get up and running I started out with AWS Lambda and getting up to speed with what was necessary to get an AWS Lambda function working on the web. This led me to Serverless as a way of accelerating development and management of AWS resources.\nThe first version of the integration used only one Lambda function, but in order to echo commands back to the user in a channel and also respond later asynchronously, two Lambda functions are required. This is because each Lambda function can only reply to the request / trigger that started it once. If the Lambda function replies to Slack to acknowledge receipt of the command, then it won't be able to reply later once the R25 data has been retrieved and processed. If the Lambda function waits until it's gathered and processed all the R25 data, then it might miss the 3-second window to acknowledge the Slack command, and even if it does reply within the time frame, the confirmation of the command happens after the results are returned to Slack, making the confirmation show up not only late, but out of order. Thus two Lambda functions are required: one to parse and return acknowledge the command and another to query and process the R25 web service data.\nScreenshots Invoking    Invocation\n   Help    Help\n   ","permalink":"https://chasesawyer.dev/project/slackr25bot/","tags":["project","aws","lambda","sns","slack","UW"],"title":"SlackR25Bot - UW"},{"categories":null,"contents":"I've been making a lot of progress on the python-LEDSerialController project. There's been a lot to learn about how to run the original command line script with a GUI frontend. I chose to use Tk since it's baked into Python already, and there's nothing to configure to get it working. It doesn't look nearly as nice as something that would come out of using a more advanced UI toolkit, but it's also had a lower bar to entry, despite some drawbacks with Tk's documentation. Googling around for solutions to problems as they arise has proved to be effective though.\nGetting this project to run smoothly has been a challenge because I'm using one thread to accomplish everything, which comes with the restriction that nothing can be blocking (at least not for long) without causing UI lag (bad) or causing responsiveness on the LED controller (Arduino) to lag. This is further complicated by the way that I've built up the Arduino code to allow it to perform animations with the LED strips and check in on the serial buffer.\nAll of this combines to create the following conditions:\n The Arduino only checks in with the computer when it's ready for a new command in between running pattern animations, which depends on the interval setting for the last command that was sent The UI only updates while the Python thread is not blocked or doing anything long-running Updating the Arduino requires checking the computer's serial buffer to see if the Arduino has signaled that it's ready for the next command.  Disadvantages of doing things this way:\n A pattern animation, such as the animated rainbow, will only run for a single cycle, and then stop. More to the point, without any serial input from the host computer, the Arduino will cease to update any LEDs at all. The host computer must continually update the Arduino (controller) with what to do next, including, telling it to do the same thing over again.  Advantages of doing things this way:\n Any updates to state on the Python host program can update the controller with new information as soon as it's ready UI to Controller update delay is minimal UI can remain responsive while the controller is busy and not ready for communication  Timing on the Tk application can be tricky in this situation: checking the serial input as often as possible can push CPU usage to 100%, and accomplishes nothing productive since the Arduino won't be ready for updates that frequently. On the other hand, checking too infrequently will lead to stuttering in continuous patterns, but will leave more time on the host PC for keeping the UI responsive.\nThe way I solved this was to ensure that any blocking actions are only executed when absolutely necessary. In this instance, I can use the pyserial in_waiting property to know when the Arduino has sent data that needs to be checked:\n1 2 3  def serial_has_waiting(self): \u0026#34;\u0026#34;\u0026#34;Return true if there is serial data in the input buffer - non-Blocking\u0026#34;\u0026#34;\u0026#34; return self.cmdMessenger.comm.in_waiting != 0   Using that method allows me to avoid going into my incoming data handling code before there's anything in the input buffer to read:\n1 2 3 4 5 6 7 8 9 10 11  def getCommandSet(self, src): receivedCmdSet = None logging.debug(src + \u0026#39;: getCommand...\u0026#39;) while (self.cmdMessenger.comm.in_waiting == 0): # blocking - here as a final check before self.c.receive() time.sleep(0.1) receivedCmdSet = self.c.receive() logging.debug(src + \u0026#39;: getCommand complete.\u0026#39;) if (receivedCmdSet[0] == \u0026#34;CMDERROR\u0026#34;): logging.error(\u0026#34;CMDERROR: \u0026#34; + receivedCmdSet[1][0]) logging.debug(receivedCmdSet) return receivedCmdSet   The final piece is making a method that will be called for every cycle of the Tk mainloop() - first in main:\n1 2 3 4 5 6 7 8  if __name__ == \u0026#39;__main__\u0026#39;: try: if setup(): pre_run_commands() app.after(500, update_controller) # HERE app.mainloop() except KeyboardInterrupt: # Called when user ends process with CTRL+C stop()   And within update_controller():\n1 2 3 4 5  def update_controller(): \u0026#34;\u0026#34;\u0026#34;Check the LED Controller, and issue, or re-issue a command as needed\u0026#34;\u0026#34;\u0026#34; if LEDController.serial_has_waiting(): LEDController.repeat() app.after(75, update_controller) # HERE   The important part here is the amount of time (in milliseconds) that the app.after() is given for the next check. Initially from main, I have it set at 500ms, since we want to get things going and allow a little time for the UI to get started before we start the serial communication with the Arduino in earnest. Later on, it's reduced to 75ms in update_controller() so that we can have enough time to update the UI, not overburden the CPU, and also be sure to catch input from the Arduino relatively quickly (within about 75ms, which is pretty fast). This balance is fast enough that animations on the Arduino don't perceptibly have a delay in between iterations.\nI might tune these values more, but for now things seem to be running well enough that I can focus on further development of the UI and implementing more advanced actions through the Tk GUI, and eventually (long term) start building out connections to other applications and APIs that would allow the LED strip to react to events from other applications or web services.\n","permalink":"https://chasesawyer.dev/post/2017/06/python-tk-ui-notes-project-update/","tags":["python","gui","update"],"title":"Python Tk UI Notes, Project Update"},{"categories":null,"contents":"First Post!\nLearning how to set up Jekyll on GitHub Pages is actually a little harder than I was expecting from the outset. Mostly because most of the things that you need to set up Jekyll for local development, and a lot of the things that come prepackaged with it aren't actually necessary for running it on GitHub Pages.\nHere is the Gemfile content for this page when I first started and got things working:\n1 2 3 4 5 6 7 8 9 10 11 12  source \u0026#34;https://rubygems.org\u0026#34; ruby RUBY_VERSION # ... gem \u0026#34;jekyll\u0026#34;, \u0026#34;~\u0026gt; 3.3\u0026#34; gem \u0026#34;jekyll-theme-slate\u0026#34; # If you have any plugins, put them here! group :jekyll_plugins do gem \u0026#34;jekyll-feed\u0026#34; gem \u0026#34;github-pages\u0026#34; end   This is actually a mix of what you end up with when you create a site from scratch on GitHub Pages using the theme chooser with an empty repository. And it's also different from the one you'd get by starting locally using the Jekyll Quick-start guide. While the Quick-start mentions this helpful guide to getting started using Jekyll on GitHub Pages (which I worked through and found to bve educational in getting things set up), it doesn't make use of the nice (an easy to use) templates / themes that you can select through the GitHub Pages repository settings. To do that, you need to use the template HTML / CSS from the theme baked into GitHub Pages by copying default.html according to this article on GitHub Help.\nOnce you have your theme working, and you have a working website that you can reach, and your content is showing up the way you expect, you can begin customizing and adding in plugins, custom CSS, etc.\nOne final issue I ran into was a dependency issue with the Gemfile.lock vs. the Gemfile when trying to set up this site to use CircleCI (more for fun getting to know CircleCI than anything else). The issue was that when I created a local Jekyll deployment for testing with the same repository as the one I was using for GitHub Pages, I had been making a lot of ignorant changes to my Gemfile. There were conflicting entries between the two and my resolution (since there wasn't much here to break yet) was to rename Gemfile.lock to Gemfile.lock.old left my Gemfile as it was, and then reran bundle install, which will regenerate Gemfile.lock, without any conflicts between them.\nOnce that was ironed out, CircleCI was able to complete without errors, but without a circle.yml, it also didn't have any tests to run in order to say anything about the state of the project. So I put together a basic configuration:\n1 2 3 4 5 6 7 8 9 10 11 12  machine: ruby: version: 2.4.0 dependencies: post: - bundle exec jekyll build test: post: - bundle exec htmlproofer ./_site --check-html --disable-external   The above is basically lifted from the CircleCI page in the Jekyll docs, plus specifying the ruby version that's currently being used by GitHub Pages.\nAnother issue that I ran into trying to get this page to build on CircleCI was this Jekyll issue (#2938). So I had to add \u0026ldquo;vendor\u0026rdquo; to the exclude list - this was not immediately clear (since I hardly know YAML at all) because it's listed on the fixes as\n 1  exclude: [vendor]    But if you want it in a YAML list, it needs to be noted as\n1 2 3  exclude: - vendor - .... other excludes ....    So after all that, do I have a passing build? No.\nhtmlproofer found some \u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt; placeholder tags and failed.\nBut at least now it's failing because of something I wrote, rather than because of a configuration issue!\nSuccess and progress.\n","permalink":"https://chasesawyer.dev/post/2017/05/learning-jekyll-and-getting-started-with-github-pages/","tags":["jekyll","update","learning"],"title":"Learning Jekyll and Getting Started with Github Pages"},{"categories":null,"contents":"We have a hammer drill that has a vacuum collector attachment, and we wanted a collector that could use different kinds of filters. I took the existing dust collector and designed an exact replacement that would take any kind of 3M respirator filter as the filter element. This allows using different kinds of filters and for the filters to be replaced. The parts involved were manually reverse-engineered on paper with the device and a set of good digital calipers. The replacement parts were designed in Autodesk Fusion360 and 3D printed on a Prusa i3 MK2.\nReverse Engineering a Device Digital calipers are infinitely helpful when trying to reverse engineer or copy a physical object with 3D CAD and 3D Printing. Dimension all the parts on a drawing and then use those measurements to create the 3D model in CAD. This was a challenging project, and one of the most complex models that I've ever had to engineer. The result was a device that was a perfect 1:1 fit in the overall vacuum body.\nChallenges The original filter device didn't need to have any moving parts, and the filter was build into the wall between the chamber and the outlet. For my design I had to get a round filter to attach to a rectangular shaft, and also position the connection point somewhere where the filter could be folded around into the rectangular space of the dust chamber. Using a lofted profile allowed me to blend the rectangular outlet port into a circular tube to connect with the filter body. This worked well and maintained equal or greater volume between the filter connection point and the outlet port (no pinch point for the airflow).\nThe other challenge was designing a sliding joint that would hopefully be tight enough to seal, keeping the dust contained within the chamber and maintaining vacuum pressure. It took several test prints with small cut-away sections of the sliding joint between the two halves to get the tolerances just right to be tight without being loose or impossible to slide. This was definitely helped by making sure that the 3D printer was well calibrated and consistent throughout the entire print.\nDrawings    Reference measurements taken from the original device\n      Reference measurements taken from a sample filter scan\n    Results Below is a rendering of the 3D model from Fusion360. Unfortunately I neglected to take any photos of the finished 3D print.\n    ","permalink":"https://chasesawyer.dev/project/dustcollector/","tags":["project","3d printing","uw","fusion360"],"title":"Dust Collector - UW"},{"categories":null,"contents":"Summary   Hello   I'm a software engineer living in Seattle, Washington, currently working for Academic Technologies at the University of Washington.\nMy interests span across industries and disciplines, from art to computer science, and games to business intelligence. I've built my skill set from a position of curiosity, using an analytical approach to problems and their possible solutions, always learning whenever I have the opportunity. I love to learn. I solve difficult problems by combining my learning and experience into novel approaches and solutions leveraging the latest technologies and tools.\nI'm really interested in security, privacy, and ensuring that data is protected. Encryption is fascinating and I think that it should be accessible and usable by everyone to protect themselves and their data. I also love diving into data and using it to build automation and business intelligence tools. I use a combination of data tools and tech to quickly prototype solutions before building them into a service or microservice, designing the whole tool from the ground up: from the database to the frontend and everything in between.\nTraditional Resume Download Download pdf\nüõ† Skills Languages, Tools, \u0026amp; Operating Systems Python   Java  Javascript  Node   git  virtualenv  Linux  Windows  Arduino    Databases MySQL  MongoDB   SQL Server  PostgreSQL    Frameworks Vue   React   Django    BI Tools Tableau   Knime    Architecture Serverless   IP Networking  RESTful APIs  GraphQL   Docker    Cloud Platforms \u0026amp; Technologies AWS   Google Cloud Platform   Heroku   Continuous Integration / Continuous Delivery    Experience  Senior Computer Specialist  University of Washington   May 2016 - Present  Seattle, WA     Create business intelligence solutions and tools, informing service improvement. Develop software tools to automate processes, saving hundreds of hours of staff time. Manage communications and outreach campaigns with wide customer base. Manage and maintain 400+ desktop and laptop computers.     Program Support Supervisor  University of Washington   Aug 2013 - May 2016  Seattle, WA     Managed a team of help desk analysts, including hiring and training. Performed Tier 2 and 3 support and maintenance of HD AV Systems and infrastructure while providing excellent customer support and service. Certified in Biamp Audio DSPs and Crestron DigitalMedia.     Lead Help Desk Analyst  University of Washington   Apr 2009 - Jun 2013  Seattle, WA         Education  Bachelor\u0026#39;s Degree  University of Washington   2008 - 2013  Seattle, WA     Undergraduate degree culminating in a synthesis of computer science and art in the form of live drawing robots within the gallery space and finished works displayed in the gallery. Coursework involved (but was not limited to) CS fundamentals, data structures and algorithms, networking, databases, studio art, sculpture, and acoustics.     Certificates  APIs and Microservices  Free Code Camp   2018       Experience building APIs and Microservices. Check out my samples and projects sections for examples.  Free Code Camp Certificate    Additional Details Check me out on Linkedin¬†\n","permalink":"https://chasesawyer.dev/page/about/","tags":null,"title":"Chase Sawyer"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","permalink":"https://chasesawyer.dev/search/","tags":null,"title":"Search Results"}]