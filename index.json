[{"categories":null,"contents":" I\u0026rsquo;m building a web infrastructure project that\u0026rsquo;s based around the project verbose-equals-true (referred to as VET from now on), which sets out to create a set of services to support modern web apps, using several Docker based images to collect everything into separate concerns. I like the philosophy behind the project, and it looks well thought out, however, as things are always changing in this landscape and nobody has the same development environment, there\u0026rsquo;s always going to be stumbling blocks. It\u0026rsquo;s also my goal to use this other project as more of a framework or set of guidelines that should work, and then branch out from there, changing things as I go along so that it works for me.\nSo with credit to the original creator of this project, Brian Caffey, here are some of my notes on what I found difficult, what problems I ran into, and the things I changed.\nOperating Systems The project is built in a Linux environment, which has excellent support for Docker, but it\u0026rsquo;s not my primary environment to be working in. I use Windows primarily, and all of my computers are Windows boxes. However, I have some Linux environments set up in Hyper-V already, so I\u0026rsquo;m using Linux in Windows to implement this Docker project (what could possibly go wrong? Networking, mostly).\nDNS This is in the Docker documentation here, but not in the VET documentation: (quoted below from docs.docker.com)\n Troubleshoting for Linux users\n\u0026hellip;\nDNS settings\nDNS misconfigurations can generate problems with pip. You need to set your own DNS server address to make pip work properly. You might want to change the DNS settings of the Docker daemon. You can edit (or create) the configuration file at /etc/docker/daemon.json with the dns key, as following:\n{ \u0026quot;dns\u0026quot;: [\u0026quot;your_dns_address\u0026quot;, \u0026quot;8.8.8.8\u0026quot;] }\n[\u0026hellip;]\nBefore proceeding, save daemon.json and restart the docker service. sudo service docker restart\n The example they give above has you set the IP address(s) for your local DNS provider, with a fallback address to Google\u0026rsquo;s public DNS. I set my DNS address list to use my local DNS first, falling back to 1.1.1.1 (Cloudflare) and finally 8.8.8.8 for Google\u0026rsquo;s public DNS.\nThe line in the VET docs that gave me trouble was when it says to first run this line in the terminal:\nsudo docker-compose run backend django-admin.py startproject backend .\nThis line will spin up a docker container that will try to run pip install for the required packages in requirements.txt which failed for me because the docker container didn\u0026rsquo;t know where to look for DNS resolution, and so pip couldn\u0026rsquo;t find and install any requisite packages.\nAfter setting up my DNS setting for the Docker daemon as specified above and restarting the docker service, pip install worked as expected ðŸ™Œ.\nCI The VET docs on have you use the GitLab built-in CI offering, which, if I used GitLab at all, would probably be really easy! But I don\u0026rsquo;t use GitLab and don\u0026rsquo;t really want to get set up with another code repository when I already have GitHub set up and have integration with CircleCI already set up. So here\u0026rsquo;s my modifications to the project to get set up with GitHub and CircleCI.\nNot really knowing anything about how GitLab\u0026rsquo;s CI systems work, it\u0026rsquo;s hard to translate what it\u0026rsquo;s doing to what needs to be done with CircleCI, so this is maybe not the best solution, but it does work, and I believe that it does the thing it\u0026rsquo;s supposed to do.\nWorking Version of CircleCI Config Here\u0026rsquo;s the first version of the CircleCI configuration file that successfully passed all tests and was able to store testing data in a way that CircleCI could store and parse in results.\n#.circleci/config.ymlversion:2.1# CircleCIjobs:lint_test_coverage:working_directory:~/project# this is the defaultdocker:# The first image listed is the primary image and runs all commands-image:circleci/python:3.6environment:TEST_DATABASE_URL:postgresql://postgres@localhost/circle_test?sslmode=disableDJANGO_SETTINGS_MODULE:backend.settings-circleci# Subsequent images listed run on a common network with the primary image-image:circleci/postgres:9.6.9environment:# these settings affect how the test database is going to be set up - use to connect laterPOSTGRES_USER:postgresPOSTGRES_DB:circle_testPOSTGRES_PASSWORD:\u0026#34;\u0026#34;steps:-checkout-run:mkdirtest-reports-restore_cache:key:deps1-{{.Branch}}-{{checksum\u0026#34;backend/requirements.txt\u0026#34;}}-run:name:InstallPythonDependenciescommand:| python3 -m venv .venv..venv/bin/activatepipinstall-rbackend/requirements.txt# Save installed environment dependencies in cache for later steps/jobs-save_cache:key:deps1-{{.Branch}}-{{checksum\u0026#34;backend/requirements.txt\u0026#34;}}paths:-\u0026#34;.venv\u0026#34;-run:name:installdockerizecommand:wgethttps://github.com/jwilder/dockerize/releases/download/$DOCKERIZE_VERSION/dockerize-linux-amd64-$DOCKERIZE_VERSION.tar.gz\u0026amp;\u0026amp;sudotar-C/usr/local/bin-xzvfdockerize-linux-amd64-$DOCKERIZE_VERSION.tar.gz\u0026amp;\u0026amp;rmdockerize-linux-amd64-$DOCKERIZE_VERSION.tar.gzenvironment:DOCKERIZE_VERSION:v0.3.0-run:name:Waitfordbcommand:dockerize-waittcp://localhost:5432-timeout1m-run:name:Lintingtestcommand:| . .venv/bin/activatecdbackendflake8--output-file=../test-reports/flake8.txt-run:name:coveragetestcommand:| . .venv/bin/activatecdbackendpytest--cov--junitxml=../test-reports/pytest-store_artifacts:path:test-reports/destination:tr1-store_test_results:path:test-reports/workflows:version:2lint_test:jobs:-lint_test_coverage:filters:branches:only:master Issue 1: DB Credentials Need to make sure that the postgres configuration passed to the secondary docker image sets up a user and database name that django is later going to connect to. In this case this is called out here:\n-image:circleci/postgres:9.6.9environment:POSTGRES_USER:postgresPOSTGRES_DB:circle_testPOSTGRES_PASS:\u0026#34;\u0026#34; Which needs to be loaded into django\u0026rsquo;s settings from settings-circleci.py\nfrom .settings import * # noqa DATABASES = { \u0026#39;default\u0026#39;: { \u0026#39;ENGINE\u0026#39;: \u0026#39;django.db.backends.postgresql_psycopg2\u0026#39;, \u0026#39;NAME\u0026#39;: \u0026#39;circle_test\u0026#39;, \u0026#39;USER\u0026#39;: \u0026#39;postgres\u0026#39;, \u0026#39;PASSWORD\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;HOST\u0026#39;: \u0026#39;localhost\u0026#39;, \u0026#39;PORT\u0026#39;: \u0026#39;5432\u0026#39;, }, } Which are loaded when pytest runs because of the environment variable on the main docker image that is running the tests. I also declared the TEST_DATABASE_URL environment variable there, but decided not to make use of it in my settings-circleci.py file, finding that it seems to work better to declare the individual parts, rather than to use the URL string.\nIssue #2: Difference in Caching Methods The source documentation for GitLab lets you declare a cache with a path. With CircleCI it\u0026rsquo;s a little more complicated. For caching things like python dependencies, you need to add steps bracketing the dependency installation, with the same naming scheme for the cache. CircleCI leys you define the naming convention to use as the \u0026ldquo;key\u0026rdquo; for the cache file, so for any project make sure the restore_cache and save_cache use the same key generation method. In this case (from the circleci examples) we use the git branch name ({{ .Branch }}) and the checksum of the requirements file ({{ checksum \u0026quot;backend/requirements.txt\u0026quot; }}) so that we can maintain separate caches based on the required dependencies, and what git branch is actively being tested.\nIssue #3: Translating before_script For GitLab, this runs the pip installation of project requirements. This is basically the same, only in this case, we need to define a virtual environment (that can later be backed up) with venv, then install the dependencies there, much like what we would achieve with local development.\nThese lines:\n-run:name:InstallPythonDependenciescommand:| python3 -m venv .venv..venv/bin/activatepipinstall-rbackend/requirements.txt achieve the goal of installing project dependencies, which in the next step are cached for use on the next run (potentially).\nIssue #4: Database not Ready In preliminary test runs, I ran into the issue that django was complaining about the database not being set up. To solve this, and to ensure that the database will always be ready, I install dockerize and use it to wait for port 5432 on localhost to be ready (with a max time limit of 1 minute) in order to be sure that our postgres container is running and accepting connections. This could also be achieved with a curl command, but this works fine too.\nIssue #5: Remember where you\u0026rsquo;re working I ran into a lot of failed runs with CircleCI simply because I wasn\u0026rsquo;t calling commands in the right location, trying to change into directories that didn\u0026rsquo;t exist (because I was already there) or tried to save things into directories that were incorrectly named or not in the places I expected them to be in.\nWorking Direcory CircleCI defaults to ~/project for the current working directory. This is where code gets checked out to and is where the console is pointing when it starts any - run: stanza. You can change the working directory to something else that makes sense if you want to. Some trouble I ran into was trying to cd in to project (which doesn\u0026rsquo;t exist as a subdirectory of project for this project).\nSaving Test Results Early on I create the folder test-reports (line is - run: mkdir test-reports) right after the checkout step. This folder is at ~project/test-reports/, right at the top level of the project folder, alongside all my top level code folders and files when they\u0026rsquo;re checked out.\nDuring my Linting step and Test step, I tried to save the results to ./test-reports/{whatever} - but if you look closely, in those two steps, I\u0026rsquo;ve cd\u0026rsquo;d down to the backend folder (~/project/backend/) which does not contain a test-reports folder! So my tests failed because they could not write to that direcory. After testing I realized my mistake and changed ./test-reports/{whatever} to ../test-reports/{whatever}.\nA single dot, but an important one.\nIssue #6: pytest Won\u0026rsquo;t Set Up Django This was completely missed in the VET documentation, and I\u0026rsquo;m pretty sure I didn\u0026rsquo;t miss anything in there that I was supposed to do, but installing pytest as part of the requirements.txt alone is insufficient to get pytest to get Django working properly to run tests against. For that you need to also add pytest-django to the project requirements. After I aded that package, Django would come up, connect to the test database and run the simple test that I have defined. Success!\nHere\u0026rsquo;s my full requirements.txt after that change:\nDjango==2.2.1 psycopg2==2.8.2 flake8==3.7 pytest pytest-django # \u0026lt;- Add this pytest-cov Conclusion of Project Setup That\u0026rsquo;s it for this post, and ends the alterations I needed to make for the Project Setup phase of the Verbose Equals True clone I\u0026rsquo;m making. Up next is the Backend API additions to Django, which will also include my first deep dive into Django REST Framework - since in the past I\u0026rsquo;ve only really used plain responses, and Apollo/GraphQl for my Django APIs.\n","permalink":"https://chasesawyer.dev/post/2019/05/docker-infrastructure-project/","tags":["post","docker","django","python","infrastructure"],"title":"Docker Infrastructure Project"},{"categories":null,"contents":" I had to set up an Ubuntu VM (Virtual Machine) on my machine recently and because I also use Docker, I had to learn exactly how Hyper-V handles networking tasks between the host machine and the virtual machines you set up. Hyper-V, as far as I understand, has 3 main networking methods called virtual switches. There\u0026rsquo;s actually a 4th option that comes out of the box, which is something of a hybrid between an \u0026ldquo;Internal Switch\u0026rdquo; and an \u0026ldquo;External Switch\u0026rdquo;. The default switch is interesting, and not particularly intuitive, but after getting used to it, I find it really convenient, if a bit tedious to deal with. It is absolutely not something I recommend running anything in for long periods of time, or for anything upon which other people rely. It is, however, good for getting something set up quick and dirty - so long as you understand what it\u0026rsquo;s doing.\nFor the length of this article, it will be good to understand what a switch is, so read this Wikipedia article if you are unfamiliar.\nSo with that, here\u0026rsquo;s a quick overview of what the Hyper-V switches are and can do.\nFirst, there\u0026rsquo;s the Internal Switch. This is a virtual switch that allows Host\u0026lt;-\u0026gt;Guest connections with static IP addressing allowing inter-machine communication. The downside of this, however, is that the Guest OS is isolated from the wider internet that the Host may be connected to.\nSecond, there\u0026rsquo;s the External Switch. This is a virtual switch that is not connected to the Host directly, and consumes a physical host network interface. This interface is given over to the exclusive use of any VM set up to use the External Switch. The benefit of this is that all VMs on that switch can talk to each other, and can communicate to whatever is connected to that physical network, which may also be the same physical network as your Host (via an external switch, routing, etc. and separate physical network interface). If you have two network interfaces for your computer, then this may be a good solution for long term use. Other benefits include: Static IP addressing of your guest virtual machines, the VMs behave exactly as if they had their own wired (or wireless) network connection to the rest of your physical network, and internet connections are thus possible to and from the Guest machines. Basically, this is just like having real physically separate computers sitting on your desk, with the caveat that you need more than 1 physical network interface (which I don\u0026rsquo;t currently have).\nLastly, there\u0026rsquo;s the Private Switch. This is an interesting situation, and I can only think of a couple uses for it, but what it does is only allow VM\u0026lt;-\u0026gt;VM communication. There\u0026rsquo;s no way for Host\u0026lt;-\u0026gt;VM or Internet\u0026lt;-\u0026gt;VM connections with this switch. I think the only situation you\u0026rsquo;d want to use this is for VM-VM relay communication, where each VM has more than 1 virtual network connection enabled, such as a Router VM, Firewall VM, and other(s), where 1 VM is connected to the internet, relays traffic to an isolated Firewall VM, which then allows traffic through to even deeper VM(s) that are thus protected via layers of networking, without the need for a rack full of hardware. This might theoretically be a useful application for a clustered network simulation or something where everything is isolated from external interference, but that\u0026rsquo;s beyond my interest or skill.\nThe \u0026ldquo;Default Switch\u0026rdquo; - Getting Up and Running Without Configuring a Bunch of Stuff The thing that makes the Default Switch work well is that it is the only version of virtual switch that Hyper-V has that has NAT or Network Address Translation. The default switch behaves somewhat like a home router, allowing Guest VMs internet access, as well as Host\u0026lt;-\u0026gt;VM communication, without the hardware requirements or setup needs of an External Switch. It also does all the work for you, but also doesn\u0026rsquo;t allow you to take control of it. What this means is that you cannot set static IPs for your VMs. Hyper-V gives them DHCP addresses in a limited range, which is randomly selected in the 192.168.xxx.xxx/28 space. The Host can communicate with services running on a Guest at the given IP address that the VM is given, and the Guest(s) can communicate with the Host at it\u0026rsquo;s external IP address via NAT. Guests will get new random IP addresses every (or almost every) time the VM is rebooted, and each time the Host is rebooted, Hyper-V will choose a new random local address space to set up it\u0026rsquo;s DHCP range in, before passing those off to the guest VMs. You also can\u0026rsquo;t disable or remove the Default Switch (as far as I can tell) - but it won\u0026rsquo;t hurt anything being there. If you don\u0026rsquo;t use it, just ignore it.\nSo, how to get things talking to each other?\nStep 1: Gather information. You need to retrieve 2 things: Your Host computer\u0026rsquo;s IP address. This is what you computer is addressed according to your company / home network if another physical computer on that network were to ping it. For me, it\u0026rsquo;s 192.168.1.10 at home and 10.155.43.82 at work. Then you need to get the IP of your VM. In this case I just opened the ethernet settings in Ubuntu and checked the assigned IP address. You can also get this information from the Hyper-V manager window to see what IP your VM got assigned. At the time of taking notes, this was 192.168.18.187.\nStep 2: Set up services on VM: Say you want to run a test web server on your VM and access it from your Host\u0026rsquo;s browser. You\u0026rsquo;ll need to tell your server to bind to the VM IP at whatever port you want to use, for instance 192.168.18.187:8000. For a Django server, I\u0026rsquo;d run: python manage.py runserver 192.168.18.187:8000. Then (if it successfully starts up) you can go to your Host browser and go to that address and the Default Switch should route your traffic to the VM running that webserver.\nFrom the VM, if you want to communicate with the Host services (say, a Docker image running PgAdmin; or your PostgreSQL server that\u0026rsquo;s running on the Host), you can address them from the VM by using your Host\u0026rsquo;s external IP address. For the above example, my Django web server running in my Ubuntu VM, would communicate with my Host\u0026rsquo;s PostgreSQL database server at 10.155.43.82:5432.\nNext time you reboot things, just check your IP addresses, and reconfigure your settings before starting up services again.\n","permalink":"https://chasesawyer.dev/post/2019/05/hyper-v-local-networking/","tags":["post","hyperv","networking","virtualization"],"title":"Hyper-V Local Networking"},{"categories":null,"contents":" At the end of February 2019, Google released general access to the .dev top level domain. I had heard about this happening about a year ago, and am now the happy owner of two .dev domains! One of these is chasesawyer.dev and will soon be the new home of this site!\nBut how to get it set up? When you buy a domain name, nothing is really set up for them - going to those addresses doesn\u0026rsquo;t point to anything Firefox and Google will just say \u0026ldquo;Server not found.\u0026rdquo; So here\u0026rsquo;s how to change that.\nTheres a few things I want to set up for these domains, and I\u0026rsquo;m going to go through the steps for each. This is also part one of a series of posts on how to accomplish these tasks. Other parts will be linked when they\u0026rsquo;ve been figured out and done!\n Custom domain setup for this site, while maintaining hosting via Github Pages Email forwarding to ProtonMail Custom domain setup for my apps on Heroku  Some Quick Assumptions I\u0026rsquo;m going to be making some assumptions throughout this article, most of which can probably be translated to other platforms, but your mileage may vary.\n You\u0026rsquo;re using a .dev domain - part of the HSTS list (this will come up later) You bought your domain, are administering your domain through Google Domains (rather than godaddy or namecheap or something else - those are probably fine, I just went with Google Domains for expediency when they launched .dev)  GitHub Pages and Custom Domains This first one is pretty straightforward - you need to do two things:\n Determine if you want your GitHub Pages site to be at the root of your custom domain (ie, going to chasesawyer.dev without any prefixes) or if you want it to be at a subdomain (ie, blog.chasesawyer.dev or pages.chasesawyer.dev) Create a the appropriate DNS record on google domains (domains.google.com/m/registrar/{yourdomainname}/dns) under \u0026ldquo;Custom resource records.  For the former case (Apex domain), see setting up an apex domain. For this use case, the way that worked best for me was not by using forwarding, but by using A records in the Google Domains DNS settings for my domain name. For the latter case (subdomain), see setting up a custom subdomain  Update the custom domain option on your GitHub Pages repository to match the domain that you\u0026rsquo;re using. If you don\u0026rsquo;t do this, then your site won\u0026rsquo;t load properly, and, since the .dev domain can only be used via HTTPS, you\u0026rsquo;ll have security problems with your site! (more below) Wait for a little while - your DNS settings need some time to propagate through the DNS system, so that requests to {yourdomain}.dev will properly redirect to your GitHub Pages site. GitHub Pages also has to have time to set up a certificate for your new site.  Security, HTTPS, Certificates If, like me, you enjoy using GitHub Pages for hosting your site, you\u0026rsquo;ve enjoyed having your site having an https address, and all the security baked into GitHub Pages. When you are setting up sites on a .dev domain since it\u0026rsquo;s part of the HSTS preload list in most modern browsers, your whatever.dev site can only be loaded via https. This means that you\u0026rsquo;ll need a security certificate for the site you set up on .dev! But how do you do this if you\u0026rsquo;re using GitHub Pages? You don\u0026rsquo;t have access to their webserver, so the Let\u0026rsquo;s Encrypt certbot won\u0026rsquo;t work for you. But you need a security certificate for your domain!\nWhat isn\u0026rsquo;t really explained anywhere where I can see, is that this process of getting a security certificate through Let\u0026rsquo;s Encrypt for your GitHub Pages site at your custom domain is completely automated! That\u0026rsquo;s right! GitHub Pages just does it for you. This is why step 4 above takes a little while to complete - whatever mechanism at GitHub that serves up pages from your github.io pages site that gets set up on {your custom domain} needs to have a security certificate set up, and that system takes care of getting one through Let\u0026rsquo;s Encrypt.\nThis is also why I decided to go through with setting up this site with A DNS records, because for whatever reason, DNS forwarding didn\u0026rsquo;t work properly, and didn\u0026rsquo;t trigger whatever automated system that sets up a security certificate for your custom domain served through GitHub Pages.\nWill this cause problems later on? I am not sure yet. I haven\u0026rsquo;t set up subdomain records on Google Domains yet (like {something}.chasesawyer.dev) but that is gonna be the subject of my next post!\n","permalink":"https://chasesawyer.dev/post/2019/03/google-.dev-domains-github-pages-and-heroku-apps/","tags":["post","blog","dev","heroku","google","domains","dns","security","hsts"],"title":"Google .dev Domains, GitHub Pages, and Heroku Apps"},{"categories":null,"contents":" I have been working on the backend for a project that I\u0026rsquo;ve written about {{previously\u0026ndash;link}}. The established tools server that will be supporting my new React frontend app(s) will be using a backend built on Python 2.7 and Django 1.11, and thus I\u0026rsquo;ve had to remember how to get a development environment set up that will appropriately support the project running locally on my machine. I have a personal site that runs on the same version of Django, but with Python 3.6 as the underlying code base. Having used these tools previously, I decided that it was time to bump my personal Django instance up to something more modern and learn what it would take to get my apps-demos server (available at chase-sawyer-demos.herokuapp.com) up to Django 2.1 with Python 3.7 backing it up. I also wanted to keep some notes here on what the experience has been like setting up a legacy development environment from scratch after not having worked on a project for some time or picking up someone else\u0026rsquo;s project.\nBelow are some of my notes and experiences on having gone through this process.\nThe Work Environment and Long File Names I had a further complication with this project in that some of the dependencies required were projects maintained by other groups here, were clearly developed on a MacOS or Linux environment as the file names and directories for files within these projects were too long for Windows to handle. This meant that I could not successfully clone the codebase on to my usual development computer (Windows 10 based). I dug through some documentation for Windows that stated that it can technically support longer than 260 or so characters, and that the NTFS filesystem has no technical reason to restrict these files from existing. However, many programs and utilities baked into Windows still use the maximum path length limit (like explorer.exe - the Windows file manager). I decided that instead of trying to hack Windows into working so that I could get a working copy of the backend server that I am using to host the API and database connections for this project, I could just use a VM. I am already using Hyper-V as part of having Docker running on my dev environment, and Hyper-V has a nearly-one-click solution to installing an Ubuntu 18.04 LTS VM on Windows, so I set that up as my designated Python 2.7 + Django 1.11 development instance so that I could handle the long paths required for this project to run successfully.\nHaving set up an Ubuntu 18.04 LTS VM on my main Windows machine, I installed git, vscode, and virtualenv for Python, along with Python27 for Ubuntu so that I could get a working virtual environment set up for this project before I cloned in the existing backend project with all it\u0026rsquo;s dependencies and get things up and running before starting my new backend app within that framework.\nBut then I remembered something - Django is built around app reusability and so I went back to my app structure and moved it out into a super basic from-scratch Django project which allows me to work on any machine to build the app, without any dependence on the existing older project structure. This approach uses Django 1.11 to match the work environment\u0026rsquo;s Django version, but uses the Python 3.7 version that I have installed on most all of my development machines - this shouldn\u0026rsquo;t be too problematic, as Django 1.11 supports Python 3 and I have been taking steps to ensure that the coding I\u0026rsquo;m doing is Python 2 and 3 compatible. I\u0026rsquo;ll edit here if this turns out to be a bad idea.\n","permalink":"https://chasesawyer.dev/post/2019/02/maintaining-older-django-and-python-projects/","tags":["post","django","python","virtualenv"],"title":"Maintaining Older Django and Python Projects"},{"categories":null,"contents":" Author\u0026rsquo;s Note: this post has taken a long time to get written - so long, that I already have my 2018 Hactoberfest t-shirt and stickers! It\u0026rsquo;s content spans mid-October through December. I\u0026rsquo;ve done my best to make it a cohesive whole.\n Happy Hacktoberfest! This has been the second year that I\u0026rsquo;ve participated, and I\u0026rsquo;m looking forward to getting my t-shirt and stickers when they\u0026rsquo;re available from DigitalOcean. Really briefly, here\u0026rsquo;s a list of things that I\u0026rsquo;ve been working on and doing in the last few months since my last update.\nI\u0026rsquo;ve started working through the project prompts on The Odin Project - Still finding it difficult to stick with these self-directed learning sites. The advantage I think that The Odin Project has over Free Code Camp is that their projects are more easily shown off to other people. So far I\u0026rsquo;ve managed a couple project solutions without having to set up anything other than the GitHub repository where my fork\u0026rsquo;s code lives. The project code is all served from GitHub pages by having a gh-pages branch pointing to the desired live version of the page project.\nThe following have been completed:\n etch-a-sketch Rock-Paper-Scissors  I\u0026rsquo;ll admit, it\u0026rsquo;s not a terribly impressive list. But in my defense, work has come up with a way to keep me thoroughly engaged by giving me the capacity to pitch, prioritize, and execute upon projects of my own to improve processes and practices. What this has boiled down to is:\n 4 projects pitched in August/September 4 approved projects 1 active project in early development / preparation stage (described below) 3 projects scheduled / prioritized for execution following active project 1 developer (me)  Given the constraint of me as Project Manager, Lead Developer, and Architect, I\u0026rsquo;ve been pretty busy taking on one of the heaviest lifts I\u0026rsquo;ve ever engaged in professionally: bring a year-old React app up to date, with a database backend (from scratch), and analytics/live dashboards. You might think that this seems pretty easy, so let me explain further the current state-of-affairs.\nThe Active Project The react app is served statically from a generic managed LAMP server, which I have no access to. The server\u0026rsquo;s resources are behind the UW\u0026rsquo;s NetID authentication, but the actual React site has nothing to do with this, you simply must be authenticated in order to load the app at all.\nThere is no backend in any sense that I\u0026rsquo;ve ever thought of as a \u0026ldquo;backend\u0026rdquo;. It\u0026rsquo;s Google Sheets. All of it. Dynamic elements of the React site are all fields set up in various tabs of a single Google Sheets spreadsheet. Changing values in particular cells of the spreadsheet change the values displayed in the React app. The React app then uses those values to modify forms and dynamically change so that users can submit information about classroom checks (problems and details about them, or everything is fine). This information populates a \u0026ldquo;form responses\u0026rdquo; tab in that same spreadsheet, which then is checked by a formula in another tab, and that subsequently updates the React app.\nWhat black magic makes all this data go around without collapsing? Firebase. And Google Apps Scripts.\nSo Google Apps Scripts allow you to use Javascript to modify Google Docs, Sheets, etc. with custom functions and the like (such as macros). These scripts (can) have triggers, which can be time based (I\u0026rsquo;d say, cron-adjacent) or based on actions that happen with the associated Google Sheet or Doc. Firebase is a no-sql document object store with some really fancy real-time update functionality built into their libraries that provide some attractive quality-of-life features to app developers - specifically, updates to date in Firebase are automagically synced to all online clients, and any offline changes from clients are cached locally and later synced when that client goes back online.\nIn our case, Google Sheets holds all the data, any change to any part of the document triggers an Apps Scripts function that uploads all te watched cells to Firebase. Those changes are live-synced to any clients watching (clients being those with the main React app loaded). These triggers are dependent on user interaction. The opposite direction data-flow (the React app to Google Sheets) is not realtime. Or not quite. Form data filled out in the React app is uploaded to Firebase into a sort-of update queue. This is where data lives until cleared out by another Apps Script attached to this Google Sheet. This function has a time-based trigger that runs as often as Google allows: every minute of every day forever. Each minute (or so), this script checks the Firebase database (\u0026lsquo;queue\u0026rsquo;) to see if there\u0026rsquo;s been a change to the queue, and if so, it pulls that data and populates a new row of the form responses tab, then clears that data from the Firebase queue, which causes another update to Firebase, because that new incoming data changed the Sheets, which means the Firebase data needs updating, which will cause an update to any listening React clients.\nIt sounds complicated, and it is. Learning how everything was put together in the first place took some time to learn, and then maintaining it is another battle entirely. The whole apparatus depends on triggers associated with the Google Apps Scripts, which are directly attached to their relevant Google Apps document.\nSome more background: Google Apps scripts can be either standalone, or they can be attached to a document. If they are standalone, then they exist and behave much like other Google documents or spreadsheets (they show up like another ). They are little files that exist somewhere. If they are attached to a document, then those scripts do not actually have any independent identity. They are intrinsically linked to whatever document that they were created in. This means that they are not easily found for shared documents, like the ones that we are dealing with here. They also have a peculiar association with other Google Cloud applications and functions, in that they appear to be projects, and have similar properties, but you cannot see the engine or virtual machine they run within, nor are they cloud functions per se, and their API usage doesn\u0026rsquo;t necessarily show up in the Cloud Console. (This could just be my misunderstanding how these things work, but this has been my observation thus far). To edit scripts, you need to find them in the Google Apps Scripts developers console (G Suite Developer Hub), or by first finding the document that they\u0026rsquo;re attached to and then opening that document\u0026rsquo;s scripts.\nNow, about those triggers: until very recently there was no way (that I could find) to find the triggers associated with a shared document/spreadsheet attached apps script, unless you were the user that created that trigger. Now in the last couple of weeks (since November 20th or so) they allow other users that can see the shared document/script to see that there are triggers, but not who that user is or a whole lot of detail about the trigger parameters. (YMMV - this is taken from the perspective of a user within an organization (the UW) where documents are shared amongst many users (within one of our Team Drives)). When I first began looking into taking over this project, I couldn\u0026rsquo;t see any triggers attached to any of the functions that were associated with our master Google spreadsheet, so to me, it was clear that these functions were happening, but I couldn\u0026rsquo;t see how. This led to a failure for our crew one night, since (for reasons unknown) the magical invisible triggers just stopped working. Now, this probably wasn\u0026rsquo;t the case, but since I could never see the triggers responsible for all the data moving back and forth, I also couldn\u0026rsquo;t see that they weren\u0026rsquo;t there or if they were broken in some other way. I had anticipated this, and the resolution was just to re-create these triggers under my own UW Google account.\nGoing forward, this is fine, but it brings me back to another fundamental flaw in this kind of system: Transitioning these resources over to a new team member/user. Triggers can\u0026rsquo;t be moved/transferred/shared/etc. And similarly Scripts that are associated within a Googel Doc or Sheet are intrinsically tied to it. They can\u0026rsquo;t move away from that document, and if that document is deleted, then the Script associated with it goes too. Making your scripts separately from your G Suite applications seems like a wise decision from the outset, but that wasn\u0026rsquo;t immediately clear when these scripts were initially made. Of course copy-paste makes moving their content to local files or to independent Scripts on Google would ostensibly be a valid way of breaking this interdependency, it\u0026rsquo;s still no Git. And of course today I used this very functionality to create two Slack Custom Integrations that run within this script framework with daily triggers to create some automated notifications for staff. These scripts are trivial so I feel no particular attachment to making sure they don\u0026rsquo;t disappear suddenly, but on the other hand, for something more mission-critical, it\u0026rsquo;s more concerning.\nLong term I\u0026rsquo;m going to work out a way of potentially deploying these Apps Scripts programmatically, so that they can live somewhere else and be deployed through some kind of automated system? (looking at you Github Actions\u0026hellip;)\nOngoing Development Ok, so here\u0026rsquo;s a small list of the things that need to happen (completely out of order, some already completed): Write a project charter, and try to anticipate the speed at which decisions will be made within a bureaucratic government organization during the holiday season). Update Create-React-App (and React), all their dependencies, the Google Firebase library, and any other utility libraries. Audit the list of packages in package.json and determine what can stay and what is extraneous. Add eslint and settle on some rules to bring some order to the code base. Update React components to be more modular - break out what needs separation, cull redundancy, and prepare the app that is using Firebase as it\u0026rsquo;s backend ready to switch over to a SQL database for a backend. Build out the architecture of the SQL database. Build an API between the database and the app. Build a Tableau dashboard and report set that pull from the SQL database. Build in Business Logic everywhere to ensure the core data in the SQL database is and remains valid into the future. Clean up and transition legacy data over to the SQL Database. Decide on what flavor of Database to have in the first place. Figure out how to get the app to be integrated into the UW NetID (Shibboleth) SSO, and use information about the users to control what users can and cannot do, including who can manage data in the SQL database (via a manager view within the app). Build a manager interface within the app.\nIt\u0026rsquo;s an ambitious project, but one that I feel like I have all the pieces for, and the actual plumbing of all the parts and figuring out how they all go together is the most fun and enjoyable part of all of this.\n","permalink":"https://chasesawyer.dev/post/2018/10/long-time-update-new-projects/","tags":["post","update","uw","cloud"],"title":"Long Time Update; New Projects"},{"categories":null,"contents":" When developing an application or project with Node.js, debugging is an important aspect to getting to the bottom of issues. If you use an IDE (integrated development environment) or code editor that has built-in or supported debugging capacities, such as Visual Studio Code, then you have that ability already. But if you are someone who\u0026rsquo;s developing with a more basic code editor - such as Sublime Text, then you can still debug your Node.js application, using Chrome Developer Tools as your debugger, as if you were debugging a live website.\nBackground - Node Debugging Node creates a server and runs your application. When debugging, you need a client to talk to the server to tell it when to stop execution, catch errors, and to set/catch breakpoints in the code. This is called an inspector client, and that interface is what is presented to you in the Chrome Developer Tools as well as when using the debugger mode in IDEs.\nSet up For this, you can use any Node server app. We\u0026rsquo;ll be launching the app from the command line, and then open up Chrome and connect to the running Node server, which will be running in debug mode. Initiating the app in this way, it won\u0026rsquo;t actually begin execution until the inspector client (Chrome) connects to Node server.\nStart Node in Debug mode If you normally start your Node project server with something like\nnode app.js Open a terminal in your Node project folder, and start it with the command\nnode --inspect-brk app.js You should then be greeted with a message like this:\nDebugger listening on ws://127.0.0.1:9229/9b3ec4f2-b590-4372-b34c-1s4affc3a345 For help see https://nodejs.org/en/docs/inspector Then open chrome to the address chrome://inspect\nFrom here, you should see your Node instance listed, including the file path to the app that\u0026rsquo;s running. Click the \u0026ldquo;inspect\u0026rdquo; link below the file directory shown, and Chrome DevTools will open up. In the console, you should now see\nDebugger attached. At this point, your code still hasn\u0026rsquo;t actually begun running yet. DevTools has attached to the Node debugging instance, and has frozen your code at the very beginning of app.js. DevTools will now let you explore your code, and insert breakpoints - places where you want code to halt - before it starts executing your code.\nFrom here you can begin to debug your code to your heart\u0026rsquo;s content, and dig into pesky bugs that might be plaguing you. Using an interactive debugger is also very useful for catching the state of your app when it crashes (runs into an exception) as the debugger will halt at that point, and you can inspect the state of your variables and dig deeper into what may have gone wrong to cause the exception.\nFor a more in-depth guide on debugging and how to navigate Chrome\u0026rsquo;s DevTools, check out the links below:\n https://developers.google.com/web/tools/chrome-devtools/ https://nodejs.org/en/docs/guides/debugging-getting-started/  For VS Code:\n https://code.visualstudio.com/Docs/editor/debugging  As a side note, most modern browsers have debugging tools, and they all behave pretty similarly, with the exception of mobile browsers - mobile browsers (to my knowledge) don\u0026rsquo;t come with the ability to debug the code running on them. But if you are debugging a mobile page, then you can run a page as if in a mobile browser from within Chrome or Firefox by opening the DevTools and entering Responsive Design mode. Check out https://developer.mozilla.org/en-US/docs/Tools/Responsive_Design_Mode for Mozilla\u0026rsquo;s implementation of this tool, and how it can be helpful in quickly testing your site\u0026rsquo;s reaction to small screens.\n","permalink":"https://chasesawyer.dev/post/2018/06/debugging-node-applications/","tags":["post","blog","node","javascript","debugging"],"title":"Debugging Node Applications"},{"categories":null,"contents":" This week I had an interesting discussion with another new developer who was getting started working on an Express-based project and was frustrated by their static files working one day, and seemingly without provocation, not working the next. I knew from experiencing the same feeling when dealing with static files both in Django and Express that a static file loading problem is difficult to resolve, and how often the problem is often a simple one that is nonetheless opaque to a new developer unfamiliar with file systems and path resolution.\nWhen static file loaders can\u0026rsquo;t or don\u0026rsquo;t load something, it\u0026rsquo;s often not called out as an error because when you tell the computer where the static files are, it doesn\u0026rsquo;t make any assumptions about the contents there. And later on when you\u0026rsquo;re trying to load your custom CSS or images and getting nothing loaded, and quiet browser 404 errors for the resource, but not the page overall, the result can be a maddening series of digging into documentation that doesn\u0026rsquo;t often make clear the particulars of path names and how a single dot or slash can make all the difference in the world between success and failure.\nFile Paths A path to a file on a computer, and to an extent, a path to a resource on the internet looks like this: /rootDirectory/subDirectory/file with the file part usually having some kind of type after a dot, such as .txt or .exe. For Windows users, the beginning will have a drive letter at the beginning, most commonly C: C:\\rootDirectory\\subDirectory\\file.\nWhat happened to me, and I\u0026rsquo;m sure has happened to others - such as the individual I was talking to this week - is that nothing in the documentation about static paths goes deeply into path resolution. I guess it\u0026rsquo;s just kind of assumed that you as the developer in control of things should know what\u0026rsquo;s happening in the background, but many do not.\nPath Resolution When you give Express.js (Node) a path to load, it\u0026rsquo;s generally going to be in a relative format. The documentation gives the most basic setup as something like app.use(express.static('public')). The mechanics behind this statement are a bit more complicated than it might seem at first glance.\nWhen you give Node a path such as 'public', a function goes through and makes a best guess as to where you mean to point it. Why doesn\u0026rsquo;t it just know that you mean the folder called \u0026lsquo;public\u0026rsquo; (or \u0026lsquo;static\u0026rsquo; or \u0026lsquo;img\u0026rsquo; or whatever)? Because Node needs the absolute path on the file system to that directory, and thus the files and folders inside. And this function is pretty lenient about what it will accept in terms of file paths, and will assume that you know what you\u0026rsquo;re doing.\nSo, what is the difference between 'public', '/public', and './public'?\nThe first and the last actually resolve to the same destination: relative to the current working directory, find a directory called \u0026lsquo;public\u0026rsquo; and serve the contents as static files. However, the second example resolves to something else entirely: find a directory called \u0026lsquo;public\u0026rsquo; at the root of the file system. Or more explicitly: instead of resolving to /rootDirectory/subDirectory/project/public/ it will resolve to /public/. Or in Windows, C:\\public\\\nWhen path.js (in Node) runs through resolve() for a given path name, there\u0026rsquo;s this comment in the function:\n/* Lines 201 - 206 of path.js */ if (code === 47/*/*/ || code === 92/*\\*/) { // Possible UNC root  // If we started with a separator, we know we at least have an  // absolute path of some kind (UNC or otherwise)  isAbsolute = true;  Which if you walk through debugger with the above three options, it becomes clear that the middle option '/public' will resolve to an absolute path, rather than a relative path to the current directory where you\u0026rsquo;re working, and that will make all the difference in the world.\nNow, if you do want to give an absolute path from the root of the drive you\u0026rsquo;re working on, then you have a way of doing that by starting the path off with a \u0026lsquo;/\u0026rsquo; - otherwise you\u0026rsquo;ll need to use relative paths with either a \u0026lsquo;./\u0026rsquo; or just the folder/file you\u0026rsquo;re looking for without anything else.\nI hope that this helps someone out there understand file paths and relieves some frustration around what app.use(express.static('public')) actually means.\n","permalink":"https://chasesawyer.dev/post/2018/06/file-paths-in-node/","tags":["post","blog","express","javascript","node","filesystem","debugging"],"title":"File Paths in Node"},{"categories":null,"contents":"This project involved setting up a couple MongoDB collections and then building a responsive frontend for user interaction. This app allows adding users by name, then using that user\u0026rsquo;s ID to add activities. The API allows for querying for user activities by user ID as well.\nThe UI uses a pug template and bootstrap for most styling, and some custom css rules to fine tune elements.\nThis was actually the first project that I completed out of the new Free Code Camp curriculum, and replaced the previous Google Image Search Abstraction project that I had already completed previously.\nCode View on GitHub\u0026nbsp;\u0026nbsp; const mongoObjectId = require(\u0026#39;mongodb\u0026#39;).ObjectID const trackerCollection = \u0026#39;exerciseTracker\u0026#39; const trackerUserCollection = \u0026#39;exerciseTrackerUsers\u0026#39; // Challenge 4v2 - Exercise Tracker - UI page exports.uiPage = function (req, res) { res.render(\u0026#39;exercise/index\u0026#39;) } exports.addUser = function (req, res) { // lookup username; insert and return ID if not taken; otherwise return error message.  const collection = req.app.dbConn.getDB().collection(trackerUserCollection) const lookup = collection.find({ username: req.body.username }).toArray(function (err, documents) { if (err) { console.log(err) res.status(500).send(\u0026#39;Unable to complete action.\u0026#39;) } else { if (documents.length \u0026gt; 0) { // username was found  res.status(400).send(\u0026#39;Username \\\u0026#39;\u0026#39; + req.body.username + \u0026#39;\\\u0026#39; already taken.\u0026#39;) } else { collection.insertOne({\u0026#39;username\u0026#39;: req.body.username}, function (err, result) { if (err) { console.log(err) res.status(500).send(\u0026#39;500 server error; unable to save username.\u0026#39;) } else { res.json({ \u0026#39;username\u0026#39;: req.body.username, \u0026#39;_id\u0026#39;: result.insertedId }) } }) } } }) } exports.addActivity = function (req, res) { // given the userID, activity description, duration, and optionally, date  // look up the userID - if it exists, enter a new data entry about the activity to the database  // if the user id doesn\u0026#39;t exist, respond with a 400 user not found message  var dateEpoch = new Date().getTime() if (req.body.date) { dateEpoch = new Date(req.body.date).getTime() } const collection = req.app.dbConn.getDB().collection(trackerUserCollection) const lookup = collection.find({ \u0026#39;_id\u0026#39;: mongoObjectId(req.body.userId) }).toArray(function (err, documents) { if (err) { console.log(err) res.status(500).send(\u0026#39;Unable to complete action.\u0026#39;) } else { if (documents.length \u0026gt; 0) { // user found  const updateCollection = req.app.dbConn.getDB().collection(trackerCollection) const update = updateCollection.insertOne({ userId: documents[0]._id, description: req.body.description, duration: req.body.duration, date: dateEpoch }, function (err, result) { if (err) { console.log(err) res.status(500).send(\u0026#39;500 server error: unable to add entry.\u0026#39;) } else { // save successful  res.json({ \u0026#39;username\u0026#39;: documents[0].username, \u0026#39;description\u0026#39;: req.body.description, \u0026#39;duration\u0026#39;: req.body.duration, \u0026#39;_id\u0026#39;: documents[0]._id, \u0026#39;date\u0026#39;: new Date(dateEpoch).toDateString() }) } }) } else { res.status(400).send(\u0026#39;Unable to locate userID: \u0026#39; + req.body.userId) } } }) } exports.getUserLog = function (req, res) { // return user exercise data given the user\u0026#39;s id.  // Check for optional parameters and validate them:  var opts = {from: null, to: null, limit: 0} var queryParams = { \u0026#39;userId\u0026#39;: mongoObjectId(req.query.userId) } if (req.query.from \u0026amp;\u0026amp; req.query.to) { if (!isNaN(Date.parse(req.query.from)) \u0026amp;\u0026amp; !isNaN(Date.parse(req.query.to))) { queryParams[\u0026#39;date\u0026#39;] = { $gte: Date.parse(req.query.from), $lte: Date.parse(req.query.to) } } } else if (req.query.to) { if (!isNaN(Date.parse(req.query.to))) { queryParams[\u0026#39;date\u0026#39;] = { $lte: Date.parse(req.query.to) } } } else if (req.query.from) { if (!isNaN(Date.parse(req.query.from))) { queryParams[\u0026#39;date\u0026#39;] = { $gte: Date.parse(req.query.from) } } } if (req.query.limit \u0026amp;\u0026amp; !isNaN(req.query.limit)) { opts.limit = Math.ceil(Math.abs(req.query.limit)) } if (!req.query.userId) { res.status(400).send(\u0026#39;Must query for an ID.\u0026#39;) } else { const usersDb = req.app.dbConn.getDB().collection(trackerUserCollection) const activitiesDb = req.app.dbConn.getDB().collection(trackerCollection) const lookupUser = usersDb.find({ \u0026#39;_id\u0026#39;: mongoObjectId(req.query.userId) }).toArray(function (err, userDocs) { if (err) { console.log(err) res.status(500).send(\u0026#39;Unable to query user.\u0026#39;) } else { if (userDocs.length \u0026gt; 0) { // user found, can find their stuff.  var activitiesCursor = activitiesDb.find(queryParams) if (opts.limit) { activitiesCursor = activitiesCursor.sort(\u0026#39;date\u0026#39;, 1).limit(opts.limit) } activitiesCursor.toArray(function (err, activitiesDocs) { if (err) { console.log(err) res.status(500).send(\u0026#39;Unable to query user activities.\u0026#39;) } else { var resData = { _id: req.query.userId, username: userDocs[0].username, count: activitiesDocs.length, logData: [] } for (var i = 0; i \u0026lt; activitiesDocs.length; i++) { resData.logData.push({ \u0026#39;description\u0026#39;: activitiesDocs[i].description, \u0026#39;duration\u0026#39;: activitiesDocs[i].duration, \u0026#39;date\u0026#39;: new Date(activitiesDocs[i].date).toDateString() }) } res.json(resData) } }) } else { res.status(400).send(\u0026#39;Unable to locate userID: \u0026#39; + req.query.userId) } } }) } }  Demo View this code live here: https://fcc-challenges.herokuapp.com/exerciseTracker/\n","permalink":"https://chasesawyer.dev/post/2018/06/exercise-tracker-full-stack-app/","tags":["sample","javascript","freecodecamp","mongodb","express","pug"],"title":"Exercise Tracker Full-Stack App"},{"categories":null,"contents":"Word processors are great for creating documents with a lot of rich formatting, but all that stuff can be a huge distraction. Code editors on the other hand are amazingly good for writing - the writing where the only thing that actually matters is the content, and not the format.\nWhile it\u0026rsquo;s probably been hashed out and written about a lot by other people, the one thing, the killer aspect of it for me, is the ability to scroll the page down, so that whenever you want, you can always return to that feeling of having a blank canvas to write upon. I really love the blank page, and I love writing on something fresh, and so far, code editors are the only things that I\u0026rsquo;ve seen do this. The word processors - think Google Docs, Microsoft Word, etc. - all stop you at the bottom of the current virtual piece of paper you\u0026rsquo;re writing on and won\u0026rsquo;t let you progress until you flow over onto the next page. Now, of course there\u0026rsquo;s hacks around this, including just inserting a page break or two at the end of whatever it is you\u0026rsquo;re working on. However, this has always felt really artificial to me. Plus there\u0026rsquo;s always those page breaks and virtual pieces of paper that physically break apart your writing. It can be distracting.\nBut to be fair to word processors, I don\u0026rsquo;t think anyone writes their blog posts in them, and I don\u0026rsquo;t think that anyone writes their papers that are meant to be printed in a blog editor. There are always exceptions, of course, such as those that use LaTex for all their typesetting needs (I don\u0026rsquo;t really know much about LaTex other than that it exists and that it is very useful for document formatting, and printing math equations).\n","permalink":"https://chasesawyer.dev/post/2018/06/code-editors-vs.-word-processors/","tags":["post","blog","nonesense"],"title":"Code Editors vs. Word Processors"},{"categories":null,"contents":"I got my back-end (now API and Microservice) cert today from Free Code Camp! https://www.freecodecamp.org/certification/shadowimmage/apis-and-microservices Hooray!\nIt\u0026rsquo;s been a year since I started working through the Free Code Camp curriculum, and it feels good to finally have finished the API work. I feel like I have a pretty solid grasp now on how to build and arrange an API, which I hope will help me in the future when I try to do more API work.\nOver the time that I\u0026rsquo;ve spent on getting this certificate I\u0026rsquo;ve learned a lot of extra things on top of just how to build an API or microservice, which is also part of the reason that it took so long to get here. From the beginning of the projects on Free Code Camp, I decided that I wanted to wrap everything up together into a cohesive portfolio - something that I could showcase and present to the world that had everything packaged and ready to consume. It also added a whole layer of complexity on top of the challenges that required more research and more work, putting separate parts together into a single application and then designing and building the architecture of the portfolio where it\u0026rsquo;d be showcased. This actually led to two separate projects, the presentation site (this) where I could write up details in longer form and document my work over time, and the project site where all the projects and samples would live together in one application that could be expanded upon as I completed more challenges and coding solutions. I think this played to each platform\u0026rsquo;s strengths as well, since Github Pages is great for static sites and is always online, and Heroku projects can be almost anything you want them to be and won\u0026rsquo;t cost any money (at least for the free tier) - with the only downside being that the first visit to a sleeping \u0026lsquo;Dyno\u0026rsquo; is delayed by a few seconds as the server instance spins up.\nIn the end what really slowed me down in getting this certificate wasn\u0026rsquo;t the actual challenges - each one really only took a few hours of work. It was the external factors, life factors, that made the difference and introduced the largest delays and distance from the work. Things like buying a house, renovating and moving into said house, the relentless autumn + winter holiday madness, and changes in demands at work and at home. Suddenly a year passes, your kid is older, you\u0026rsquo;re older, and you\u0026rsquo;ve neglected your repos for so long that you don\u0026rsquo;t remember why you made this branch or what wasn\u0026rsquo;t working or done last time you committed your code.\nNot having time to dedicate to the work has made it harder to progress than I thought when I first started out. And I\u0026rsquo;m hoping that now that I have momentum and am starting to tell people about the things that I\u0026rsquo;m putting out there (rather than just working in obscurity) that I can maintain that energy and continue to get things done, become more confident, and make some cooler and bigger things.\n","permalink":"https://chasesawyer.dev/post/2018/06/new-free-code-camp-cert/","tags":["post","blog","update","certificate"],"title":"New Free Code Camp Cert"},{"categories":null,"contents":" Repair Task Tracker RTT is a full stack app that addresses the needs of a computer hardware management process, allowing the tracking and resolution of issues/problems with the hardware, as well as the configuration and components of each major hardware item. RTT is meant to be a back-of-house tool, replacing paper tickets and spreadsheets. The goal of this project was to implement a GraphQL app, with useful data, allowing a seamless user experience as they operate through the app, and data is downloaded and uploaded in the background.\nDevelopement Status This project is ongoing development. Progress details are in the project readme.\nTechnologies RTT is a single-page-app built in Vue.js and served from a Python/Django server running within a Heroku dyno (server instance). The data records are stored in a PostgreSQL database, retrieved with a GraphQL implementation using Apollo and Graphene.\nDemo The backend database schema has been completed; the frontend is still under development. The latest snapshot is live on heroku.\n   Display of all open issues in the database\n      Detail page and update fields for selected issue\n    ","permalink":"https://chasesawyer.dev/project/rttapp/","tags":["project","python","heroku","vue","javascript","postgresql","graphene","apollo","graphql"],"title":"RTTApp"},{"categories":null,"contents":" The Keys App was my first major project that sought to solve a problem with managing key checkouts without requiring a cumbersome customer database / sign up form. This would target an institution that mostly catered to internal customers, such as a university\u0026rsquo;s AV department and it\u0026rsquo;s instructors. It largely replicates a paper-form-based system, with an added layer of data validation and control (emails, phone numbers must be in a valid format; keys can only be returned by their original owners; keys can\u0026rsquo;t be checked out twice; etc.). Each checkout and check-in is recorded and tracked. All changes in the database leave a history.\nThis project was built entirely upon Django, with some Bootstrap for front-end styling. The app runs as a multi-part form with several steps and each is validated before progress is allowed. The backend leverages the built-in Django admin console and allows navigating and editing all records in the system, while also maintaining data integrity.\nTechnologies The Keys App is a full stack project built with\n Python/Django PostgreSQL Bootstrap FontAwesome  Database Schema [To come]\nDemo Hosted on Heroku: https://chase-sawyer-demos.herokuapp.com/keysApp/\n    ","permalink":"https://chasesawyer.dev/project/keysapp/","tags":["project","demo","django","python","postgresql","heroku"],"title":"KeysApp"},{"categories":null,"contents":" I spent my sick day today designing a new logo for the site using Inkscape. I knew that it had to be something that was unique, so I went online looking around for resources on designing and producing a logo. I know that SVGs are great, because you will never have scaling issues - if you need a larger version, you can simply export it at higher resolution. Or lower. Or whatever. I found some sites for inspiration as well, since I couldn\u0026rsquo;t think of exactly what I was going for initially.\nThe new design: logobook.com has been really helpful in looking at what kinds of designs you can do with different features or lettering and Logo Design Love was really helpful in getting my head into a good space for designing a logo. Other than that, the first few iterations of designs were just pen-and-paper drawings in my notebook.\nTools Designing a vector graphic is relatively easy if you have a vector graphic editing program. I have known about Inkscape for a while, and this is what I ultimately used. Another tool I found while looking around for vector graphic tools is a browser-based (or downloadable) tool called Vectr. I didn\u0026rsquo;t use Vectr much, but it looks useful if you need really simple tools and don\u0026rsquo;t want to get something as heavyweight as Inkscape. That being said, using Inkscape and it\u0026rsquo;s exceptional grid and guide snapping tools really made designing this logo easy.\nGetting it online In order to fully implement use of the logo, there\u0026rsquo;s several modifications that I had to make, including converting the .svg file to an .ico or .png format that would be browser friendly. Modern browsers support .svg files for page graphics, but most still don\u0026rsquo;t support the format in the favicon space. Only Internet Explorer requires the format be a .ico for the favicon, and most support .png, but in different levels of quality, depending on the browser and age, and if it\u0026rsquo;s a mobile or a desktop browser (not to mention things like consoles with browsers baked in). I didn\u0026rsquo;t know any of that information starting out though, and instead got a good amount of information from this stackoverflow question.\nChanges to Head \u0026lt;!-- new favicon --\u0026gt; \u0026lt;link rel=\u0026#34;apple-touch-icon\u0026#34; sizes=\u0026#34;180x180\u0026#34; href=\u0026#34;{{ \u0026#34;img/apple-touch-icon.png\u0026#34; | absURL }}\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;icon\u0026#34; type=\u0026#34;image/png\u0026#34; sizes=\u0026#34;32x32\u0026#34; href=\u0026#34;{{ \u0026#34;img/favicon-32x32.png\u0026#34; | absURL }}\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;icon\u0026#34; type=\u0026#34;image/png\u0026#34; sizes=\u0026#34;16x16\u0026#34; href=\u0026#34;{{ \u0026#34;img/favicon-16x16.png\u0026#34; | absURL }}\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;manifest\u0026#34; href=\u0026#34;{{ \u0026#34;img/site.webmanifest\u0026#34; | absURL }}\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;mask-icon\u0026#34; href=\u0026#34;{{ \u0026#34;img/safari-pinned-tab.svg\u0026#34; | absURL }}\u0026#34; color=\u0026#34;#5bbad5\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;msapplication-TileColor\u0026#34; content=\u0026#34;#da532c\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;theme-color\u0026#34; content=\u0026#34;#ffffff\u0026#34;\u0026gt; \u0026lt;!-- end favicon --\u0026gt; This required some minor modifications to the Hugo theme that I\u0026rsquo;ve been building off of by changing part of the navbar to include the new monogram svg. Getting the sizing just right required some custom css, which also had to be included in the html head. I also changed the Hugo params in the config file to reflect that the avatar is an avatar, not an icon, and instead use the icon parameter for the file location of the monogram.\nChanges to Nav \u0026lt;div class=\u0026#34;navbar-header\u0026#34;\u0026gt; + \u0026lt;a href=\u0026#34;{{ \u0026#34;\u0026#34; | absLangURL }}\u0026#34;\u0026gt; + \u0026lt;img class=\u0026#34;navbar-brand\u0026#34; id=\u0026#34;site-logo\u0026#34; src=\u0026#34;{{ .Site.Params.logo | absURL }}\u0026#34; alt=\u0026#34;{{ .Site.Title }}\u0026#34; /\u0026gt; + \u0026lt;/a\u0026gt;  \u0026lt;button type=\u0026#34;button\u0026#34; class=\u0026#34;navbar-toggle\u0026#34; data-toggle=\u0026#34;collapse\u0026#34; data-target=\u0026#34;#main-navbar\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;sr-only\u0026#34;\u0026gt;{{ i18n \u0026#34;toggleNavigation\u0026#34; }}\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026#34;icon-bar\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026#34;icon-bar\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026#34;icon-bar\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;/button\u0026gt; \u0026lt;a class=\u0026#34;navbar-brand\u0026#34; href=\u0026#34;{{ \u0026#34;\u0026#34; | absLangURL }}\u0026#34;\u0026gt;{{ .Site.Title }}\u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; ","permalink":"https://chasesawyer.dev/post/2018/05/logo-design/","tags":["post","hugo","update"],"title":"Logo Design"},{"categories":null,"contents":"Full-stack microservice (really basic frontend in Pug/HTML) that takes a FormData object from a file upload form and returns the file size in bytes as part of a JSON response.\nUser Stories  I can submit a FormData object that includes a file upload When I submit something, I will receive the file size in bytes within the JSON response  Code View on GitHub\u0026nbsp;\u0026nbsp; // upload page exports.upload = function (req, res) { res.render(\u0026#39;filesize/upload\u0026#39;) } // File Metadata Microservice - file upload result exports.result = function (req, res) { res.json({ \u0026#39;filename\u0026#39;: req.file.originalname, \u0026#39;size\u0026#39;: req.file.size }) }  Demo View this code live on Heroku\n","permalink":"https://chasesawyer.dev/post/2018/05/file-metadata-microservice/","tags":["sample","freecodecamp","javascript","demo"],"title":"File Metadata Microservice"},{"categories":null,"contents":"This microservice creates an abstraction layer (and history) between the user and the Google Images search API.\nUser Stories  User Story: I can get the image URLs, alt text and page urls for a set of images relating to a given search string. User Story: I can paginate through the responses by adding a ?offset=2 parameter to the URL. User Story: I can get a list of the most recently submitted search strings.  Code View on GitHub\u0026nbsp;\u0026nbsp; const https = require(\u0026#39;https\u0026#39;) const imgSearchCollection = \u0026#39;imgSearches\u0026#39; // Challenge 4 - Image Search Abstraction Layer (search) exports.query = function (req, res) { const resultsPerQuery = 10 var localData = { searchTerm: \u0026#39;\u0026#39;, pagination: 1, } if (!req.params.q) { res.json({\u0026#39;error\u0026#39;: \u0026#39;search query required\u0026#39;}) } else { if (req.query.offset) { var offset_tmp = Number(req.query.offset) if (!isNaN(offset_tmp)) { localData.pagination = offset_tmp } } localData.searchTerm = req.params.q var options = { host: \u0026#39;www.googleapis.com\u0026#39;, port: 443, path: \u0026#39;/customsearch/v1?\u0026#39;, method: \u0026#39;GET\u0026#39;, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39; } } options.path += \u0026#39;searchType=image\u0026#39; options.path += \u0026#39;\u0026amp;safe=medium\u0026#39; options.path += \u0026#39;\u0026amp;fields=kind,items(title,link,snippet,image/contextLink,image/thumbnailLink)\u0026#39; options.path += \u0026#39;\u0026amp;key=\u0026#39; + process.env.G_SEARCH_API_KEY options.path += \u0026#39;\u0026amp;cx=\u0026#39; + process.env.G_CSE_ID options.path += \u0026#39;\u0026amp;q=\u0026#39; + localData.searchTerm options.path += \u0026#39;\u0026amp;start=\u0026#39; + Math.max(localData.pagination * resultsPerQuery, 1) const imgReq = https.request(options, function(imgRes) { var output = \u0026#39;\u0026#39; imgRes.setEncoding(\u0026#39;utf8\u0026#39;) imgRes.on(\u0026#39;data\u0026#39;, function (chunk) { output += chunk }) imgRes.on(\u0026#39;end\u0026#39;, function () { localData.imgJSON = JSON.parse(output) var collection = req.app.dbConn.getDB().collection(imgSearchCollection) var lastDoc = collection.find().sort({ index: -1 }).limit(1) lastDoc.project({_id: 0, index: 1}).toArray(function (err, documents) { if (err) console.error(err) var insertIndex = 1 if (documents.length \u0026gt; 0) { insertIndex += documents[0].index } collection.insertOne({ index: insertIndex, query: localData.searchTerm }, function(err, r) { if (err) console.error(err) res.json(localData) }) }) }) }) imgReq.on(\u0026#39;error\u0026#39;, function (err) { res.send(\u0026#39;error: \u0026#39; + err.message) }) imgReq.end() } } // Challenge 4 - Image Search Abstraction Layer (recent searches) exports.latest = function (req, res) { var collection = req.app.dbConn.getDB().collection(imgSearchCollection) var lastSearches = collection.find().sort({ index: -1 }).limit(10) lastSearches.project({ _id: 0, query: 1 }).toArray(function (err, documents) { if (err) console.error(err.message) res.json(documents) }) }  Demo View this code live on Heroku, usage:\nSearch for images by replacing {query} with your query, and paginate through results with {page}.\n https://fcc-challenges.herokuapp.com/api/imagesearch/q/{query}?offset={page}  Show recent queries at the endpoint:\n https://fcc-challenges.herokuapp.com/api/imagesearch/latest ","permalink":"https://chasesawyer.dev/post/2018/05/image-search-abstraction-layer/","tags":["sample","freecodecamp","javascript","mongodb","demo"],"title":"Image Search Abstraction Layer"},{"categories":null,"contents":"The challenge here was to create a URL Shortener microservice. It uses a database to associate a short url ID with the original url, and once created, the microservice will redirect visitors of the short URL to the original URL.\nExample creation input:\n https://fcc-challenges.herokuapp.com/shortener/new/https://www.google.com https://fcc-challenges.herokuapp.com/shortener/new/http://foo.com:80  Example creation output:\n{ \u0026#34;original_url\u0026#34;:\u0026#34;http://foo.com:80\u0026#34;, \u0026#34;short_url\u0026#34;:\u0026#34;https://fcc-challenges.herokuapp.com/shortener/8170\u0026#34; } Usage:\n https://fcc-challenges.herokuapp.com/shortener/2871  Will redirect to:\n https://www.google.com/  Demo View this code live on Heroku at fcc-challenges.herokuapp.com/shortener/\u0026hellip;\n URL Creation: new/https://www.google.com Retrieval: 2871  Code View on GitHub\u0026nbsp;\u0026nbsp; Shortener // URL Shortener (part1) - Short URL Creator exports.new = function (req, res) { var resData = { original_url: \u0026#39;invalid URL\u0026#39;, short_url: null } resData.short_url = req.hostname + \u0026#39;/shortener/\u0026#39; // console.log(req.url)  var url = req.url.slice(5) // console.log(req.url.slice(5))  if (validUrl.isUri(url)) { resData.original_url = url var collection = req.app.dbConn.getDB().collection(shortUrlCollection) var lastDoc = collection.find().sort({ index: -1 }).limit(1) lastDoc.project({_id: 0, index: 1}).toArray(function (err, documents) { if (err) console.error(err) var insertIndex = 1 if (documents.length \u0026gt; 0) { // console.log(documents[0].index);  insertIndex += documents[0].index } collection.insertOne({ index: insertIndex, url: resData.original_url }, function(err, r) { if (err) console.error(err) resData.short_url += insertIndex res.json(resData) }) }) } else { //end valid url section  res.json(resData) } }  Resolver // URL Shortener (part 2) - Short URL resolver/redirector exports.getId = function (req, res) { if (req.params.id) { var collection = req.app.dbConn.getDB().collection(shortUrlCollection) var shortDestDoc = collection.find({ index: parseInt(req.params.id) }).project({ _id: 0, url: 1 }).toArray(function (err, documents) { if (err) console.error(err) if (documents.length \u0026gt; 0) { res.redirect(documents[0].url) } else { res.end(\u0026#39;Invalid short URL id.\u0026#39;) } }) } else { res.end(JSON.stringify({\u0026#39;error\u0026#39;:\u0026#39;invalid URL\u0026#39;})) } } ","permalink":"https://chasesawyer.dev/post/2018/05/url-shortener-microservice/","tags":["sample","freecodecamp","javascript","mongodb","demo"],"title":"Url Shortener Microservice"},{"categories":null,"contents":"The goal for this one is to get return to the user the IP address, language, and operating system of the browser/user making the request.\nCode View on GitHub\u0026nbsp;\u0026nbsp; // whoami.js // FreeCodeCamp // Backend Challenge 2 - Get requesting client IP Address exports.who = function (req, res) { var resData = { ipaddress: null, language: null, software: null } resData.ipaddress = req.ip if (req.header(\u0026#39;accept-language\u0026#39;)) { resData.language = req.header(\u0026#39;accept-language\u0026#39;).split(\u0026#39;,\u0026#39;)[0] } if (req.header(\u0026#39;user-agent\u0026#39;)) { var userAgent = req.header(\u0026#39;user-agent\u0026#39;) var lParenIndex = userAgent.indexOf(\u0026#39;(\u0026#39;) var rParenIndex = userAgent.indexOf(\u0026#39;)\u0026#39;) if (lParenIndex \u0026gt; -1 \u0026amp;\u0026amp; rParenIndex \u0026gt; -1) { resData.software = userAgent.substr(lParenIndex+1, rParenIndex-lParenIndex) } } res.json(resData) }  Demo View this code live on Heroku at fcc-challenges.herokuapp.com/api/whoami\n","permalink":"https://chasesawyer.dev/post/2018/05/request-header-parser-microservice/","tags":["sample","javascript","freecodecamp","demo"],"title":"Request Header Parser Microservice"},{"categories":null,"contents":"The goal is to create a microservice that will take a date string or a unix timestamp and make a JSON response with both versions of the given timestamp / date.\nCode View on GitHub\u0026nbsp;\u0026nbsp; // timestamp.js // Challenge 1 - Timestamp conversion UNIX \u0026lt;--\u0026gt; Standard  exports.convert = function (req, res) { var timestamp = req.params.timestamp var resData = { unix: null, natural: null } if (!timestamp) { res.json(resData) } else { const months = [\u0026#39;January\u0026#39;, \u0026#39;February\u0026#39;, \u0026#39;March\u0026#39;, \u0026#39;April\u0026#39;, \u0026#39;May\u0026#39;, \u0026#39;June\u0026#39;, \u0026#39;July\u0026#39;, \u0026#39;August\u0026#39;, \u0026#39;September\u0026#39;, \u0026#39;October\u0026#39;, \u0026#39;November\u0026#39;, \u0026#39;December\u0026#39;] if (isNaN(parseInt(timestamp))) { // is a string  var date = new Date(timestamp) resData.natural = months[date.getUTCMonth()] + \u0026#39; \u0026#39; + date.getUTCDate() + \u0026#39;, \u0026#39; + date.getUTCFullYear() resData.unix = Math.floor(date.getTime() / 1000) } else { // is a number (expect unix time)  var unixDate = new Date(timestamp * 1000) resData.natural = months[unixDate.getUTCMonth()] + \u0026#39; \u0026#39; + unixDate.getUTCDate() + \u0026#39;, \u0026#39; + unixDate.getUTCFullYear() resData.unix = timestamp } res.json(resData) } }  Demo View this code live on Heroku at fcc-challenges.herokuapp.com/\u0026hellip;\n Unix-style input: api/timestamp/1450137600 Timestamp input: api/timestamp/December%2015,%202015 ","permalink":"https://chasesawyer.dev/post/2018/05/timestamp-microservice/","tags":["post","sample","freecodecamp","demo"],"title":"Timestamp Microservice"},{"categories":null,"contents":" Gotchas In order of the issue being found:\nIs Strange (Hugo) (Template Logic) Hugo templates use strange logic - for conditional statements, Hugo uses Polish or prefix notation for the operators. This meas that instead of writing if this and that, you have to write if and this that. For more complex arrangements of logical conditions, say for a situation in which you need to check three conditions, you have to write it as: if and (first) ( and (second) (third)) which, in a infix notation style, would have been written if first and second and third.\nHugo uses blackfriday Hugo uses blackfriday as it\u0026rsquo;s markdown engine. This is probably totally fine for most situations, except for when you want to make a list with more than 2 levels. For instance,\n first  second  third    In the above case, blackfriday normally will collapse the second and third levels both into the second level. I say normally because most linters and stye guides specify an indent of 2 spaces for markdown [citation needed]. If you come from github markdown style, then this is likely the case for you. Unfortunately there\u0026rsquo;s an annoying bug in blackfriday that requires that the indent for each level be 4 spaces. If you don\u0026rsquo;t notice this when scanning through the Hugo documentation on content formats, you\u0026rsquo;ll likely run into a situation where you\u0026rsquo;re confused about your list ending up somewhat flattened.\nFortunately, most linters can be configured to have different indent settings. For Visual Studio Code (my current editor of choice), markdownlint can be configured with a .markdownlint.json file in the root of your project directory or in your .vscode\\settings.json workspace settings file. This isn\u0026rsquo;t a show stopper, but for me, wasted some time while I had to dig through to find the issue report in blackfriday and then more time to read the markdownlint documentation and figure out how to reconfigure the indent setting to make sure that my lists in my Hugo site markdown will pass.\nQuestion for later:\nCan I catch this / use markdownlint in CircleCI builds?\nHugo Supports Emoji in Markdown This wasn\u0026rsquo;t super hard to figure out, but could be easier to find info on (I actually think that the Hugo documentation could use a way better table of contents, and better search). I nearly added an emoji css library to my site head in order to support emoji via \u0026lt;i\u0026gt; tags inline in the markdown file, but fortunately found that emoji are natively supported by Hugo, but you have to enable it with the enableEmoji = true setting in the site\u0026rsquo;s root configuration. Once you enable emoji support for content files with that setting, you can easily insert emoji with their names between colons (eg ðŸ˜©).\nProtip: want larger emoji on your page? Have a header with an emoji rather than plain text. The emoji will scale to the predefined scale of the header. (Note: this will also anger the markdownlint gods, as the trailing \u0026lsquo;:\u0026rsquo; isn\u0026rsquo;t allowed)\nðŸ˜„ ","permalink":"https://chasesawyer.dev/post/2018/05/hugo-gotchas/","tags":["post","fyi","note","help"],"title":"Hugo Gotchas"},{"categories":null,"contents":"I woke up late today, so I didn\u0026rsquo;t have time to make coffee. And instead of going to my usual place where I know there\u0026rsquo;s going to be great coffee available, I decided to get a doughnut and some coffee from the doughnut shop. The first warning that this wasn\u0026rsquo;t going to go well was when I got my travel mug back and there was coffee all over the outside of the mug. All day since then I\u0026rsquo;ve been drinking it slowly (8 hours) and it\u0026rsquo;s just been terrible. It\u0026rsquo;s really made me sad, because I paid money for this coffee, and this is Seattle. I could have gone to the Starbucks down the block from this doughnut shop, but instead decided not to in favor of making one stop, rather than two on my way into work.\nI should have just waited longer and gone to my usual place.\n","permalink":"https://chasesawyer.dev/post/2018/05/bad-coffee/","tags":["blog","coffee","fwp","short"],"title":"Bad Coffee"},{"categories":null,"contents":" Built a new site, leaving Jekyll for Hugo, for my github.io page.\nChanges Moved all my old Jekyll files to a new subdirectory in order to maintain access to the old code and posts, and then transition all the content over to the main Hugo site as settings come together. One major advantage of Hugo is that the build process is super fast, so I\u0026rsquo;ve been looking at how to integrate CircleCI with Hugo builds. Fortunately the CircleCI engineering blog has a post on how to get set up doing automated builds via CircleCI.\nCircleCI One of the caveats of using Github Pages for hosting my static content is that I have to have the site content in the master branch. I had already branched my project to start working with Hugo to the hugo branch, with the intent that this branch would eventually merge back with master. After reading a bit more about how Hugo works, it seems that the hugo branch is going to be a permanent feature, with master being overwritten with the content of the /public output from the Hugo build process. The hugo branch will remain indefinitely as the new default branch on the site repository.\nOn the other hand, this will work out fine, as it will let me ser CircleCI up to pull the current hugo branch when updated, and then upon successful build, will be able to push the resulting build back up to the repository\u0026rsquo;s master branch. This automated build process is perfect for what I\u0026rsquo;m hoping to achieve, and will let me get more experience with CircleCI, which I haven\u0026rsquo;t touched in months!\n","permalink":"https://chasesawyer.dev/post/2018/05/new-static-site-with-hugo/","tags":["post","Hugo","update"],"title":"New Static Site with Hugo"},{"categories":null,"contents":"This project was built for Classroom Technologies \u0026amp; Events at the University of Washington.\nView on GitHub\u0026nbsp;\u0026nbsp;   Background The University has been updating their course scheduling from an onsite instance of R25 (unable to link because documentation doesn\u0026rsquo;t exist on the internet anymore) - a product from CollegeNET. Part of that process involved moving the scheduling database off-site. This allowed for a new integration to our Slack instance that could contact the R25 web service, now hosted on the CollegeNET servers, rather than secured locally on one of the UW\u0026rsquo;s servers.\nTimeline I came up with the idea for this project around April 18th, 2018, after talking to some colleagues about transitioning other internal tools that leveraged the local SQL database over to the R25 web service after the system transitioned to the cloud. Our department had recently been granted access to the web service, and I had spent the earlier part of the month translating Python scripts that had been digesting local SQL results to talking to the R25 web service and digesting XML instead. I spent about a week researching the possible integrations between Slack and external apps, and decided to use a Slack Slash Command for the integration\u0026rsquo;s implementation. The first version of the app was released on May 4th.\nImplementation Full details on the README\nTo quickly get up and running I started out with AWS Lambda and getting up to speed with what was necessary to get an AWS Lambda function working on the web. This led me to Serverless as a way of accelerating development and management of AWS resources.\nThe first version of the integration used only one Lambda function, but in order to echo commands back to the user in a channel and also respond later asynchronously, two Lambda functions are required. This is because each Lambda function can only reply to the request / trigger that started it once. If the Lambda function replies to Slack to acknowledge receipt of the command, then it won\u0026rsquo;t be able to reply later once the R25 data has been retrieved and processed. If the Lambda function waits until it\u0026rsquo;s gathered and processed all the R25 data, then it might miss the 3-second window to acknowledge the Slack command, and even if it does reply within the time frame, the confirmation of the command happens after the results are returned to Slack, making the confirmation show up not only late, but out of order. Thus two Lambda functions are required: one to parse and return acknowledge the command and another to query and process the R25 web service data.\nScreenshots Invoking    Invocation\n   Help    Help\n   ","permalink":"https://chasesawyer.dev/project/slackr25bot/","tags":["project","aws","lambda","sns","slack","UW"],"title":"SlackR25Bot - UW"},{"categories":null,"contents":"I\u0026rsquo;ve been making a lot of progress on the python-LEDSerialController project. There\u0026rsquo;s been a lot to learn about how to run the original command line script with a GUI frontend. I chose to use Tk since it\u0026rsquo;s baked into Python already, and there\u0026rsquo;s nothing to configure to get it working. It doesn\u0026rsquo;t look nearly as nice as something that would come out of using a more advanced UI toolkit, but it\u0026rsquo;s also had a lower bar to entry, despite some drawbacks with Tk\u0026rsquo;s documentation. Googling around for solutions to problems as they arise has proved to be effective though.\nGetting this project to run smoothly has been a challenge because I\u0026rsquo;m using one thread to accomplish everything, which comes with the restriction that nothing can be blocking (at least not for long) without causing UI lag (bad) or causing responsiveness on the LED controller (Arduino) to lag. This is further complicated by the way that I\u0026rsquo;ve built up the Arduino code to allow it to perform animations with the LED strips and check in on the serial buffer.\nAll of this combines to create the following conditions:\n The Arduino only checks in with the computer when it\u0026rsquo;s ready for a new command in between running pattern animations, which depends on the interval setting for the last command that was sent The UI only updates while the Python thread is not blocked or doing anything long-running Updating the Arduino requires checking the computer\u0026rsquo;s serial buffer to see if the Arduino has signaled that it\u0026rsquo;s ready for the next command.  Disadvantages of doing things this way:\n A pattern animation, such as the animated rainbow, will only run for a single cycle, and then stop. More to the point, without any serial input from the host computer, the Arduino will cease to update any LEDs at all. The host computer must continually update the Arduino (controller) with what to do next, including, telling it to do the same thing over again.  Advantages of doing things this way:\n Any updates to state on the Python host program can update the controller with new information as soon as it\u0026rsquo;s ready UI to Controller update delay is minimal UI can remain responsive while the controller is busy and not ready for communication  Timing on the Tk application can be tricky in this situation: checking the serial input as often as possible can push CPU usage to 100%, and accomplishes nothing productive since the Arduino won\u0026rsquo;t be ready for updates that frequently. On the other hand, checking too infrequently will lead to stuttering in continuous patterns, but will leave more time on the host PC for keeping the UI responsive.\nThe way I solved this was to ensure that any blocking actions are only executed when absolutely necessary. In this instance, I can use the pyserial in_waiting property to know when the Arduino has sent data that needs to be checked:\ndef serial_has_waiting(self): \u0026#34;\u0026#34;\u0026#34;Return true if there is serial data in the input buffer - non-Blocking\u0026#34;\u0026#34;\u0026#34; return self.cmdMessenger.comm.in_waiting != 0 Using that method allows me to avoid going into my incoming data handling code before there\u0026rsquo;s anything in the input buffer to read:\ndef getCommandSet(self, src): receivedCmdSet = None logging.debug(src + \u0026#39;: getCommand...\u0026#39;) while (self.cmdMessenger.comm.in_waiting == 0): # blocking - here as a final check before self.c.receive() time.sleep(0.1) receivedCmdSet = self.c.receive() logging.debug(src + \u0026#39;: getCommand complete.\u0026#39;) if (receivedCmdSet[0] == \u0026#34;CMDERROR\u0026#34;): logging.error(\u0026#34;CMDERROR: \u0026#34; + receivedCmdSet[1][0]) logging.debug(receivedCmdSet) return receivedCmdSet The final piece is making a method that will be called for every cycle of the Tk mainloop() - first in main:\nif __name__ == \u0026#39;__main__\u0026#39;: try: if setup(): pre_run_commands() app.after(500, update_controller) # HERE app.mainloop() except KeyboardInterrupt: # Called when user ends process with CTRL+C stop() And within update_controller():\ndef update_controller(): \u0026#34;\u0026#34;\u0026#34;Check the LED Controller, and issue, or re-issue a command as needed\u0026#34;\u0026#34;\u0026#34; if LEDController.serial_has_waiting(): LEDController.repeat() app.after(75, update_controller) # HERE The important part here is the amount of time (in milliseconds) that the app.after() is given for the next check. Initially from main, I have it set at 500ms, since we want to get things going and allow a little time for the UI to get started before we start the serial communication with the Arduino in earnest. Later on, it\u0026rsquo;s reduced to 75ms in update_controller() so that we can have enough time to update the UI, not overburden the CPU, and also be sure to catch input from the Arduino relatively quickly (within about 75ms, which is pretty fast). This balance is fast enough that animations on the Arduino don\u0026rsquo;t perceptibly have a delay in between iterations.\nI might tune these values more, but for now things seem to be running well enough that I can focus on further development of the UI and implementing more advanced actions through the Tk GUI, and eventually (long term) start building out connections to other applications and APIs that would allow the LED strip to react to events from other applications or web services.\n","permalink":"https://chasesawyer.dev/post/2017/06/python-tk-ui-notes-project-update/","tags":["python","gui","update"],"title":"Python Tk UI Notes, Project Update"},{"categories":null,"contents":"First Post!\nLearning how to set up Jekyll on GitHub Pages is actually a little harder than I was expecting from the outset. Mostly because most of the things that you need to set up Jekyll for local development, and a lot of the things that come prepackaged with it aren\u0026rsquo;t actually necessary for running it on GitHub Pages.\nHere is the Gemfile content for this page when I first started and got things working:\nsource \u0026#34;https://rubygems.org\u0026#34; ruby RUBY_VERSION # ... gem \u0026#34;jekyll\u0026#34;, \u0026#34;~\u0026gt; 3.3\u0026#34; gem \u0026#34;jekyll-theme-slate\u0026#34; # If you have any plugins, put them here! group :jekyll_plugins do gem \u0026#34;jekyll-feed\u0026#34; gem \u0026#34;github-pages\u0026#34; end This is actually a mix of what you end up with when you create a site from scratch on GitHub Pages using the theme chooser with an empty repository. And it\u0026rsquo;s also different from the one you\u0026rsquo;d get by starting locally using the Jekyll Quick-start guide. While the Quick-start mentions this helpful guide to getting started using Jekyll on GitHub Pages (which I worked through and found to bve educational in getting things set up), it doesn\u0026rsquo;t make use of the nice (an easy to use) templates / themes that you can select through the GitHub Pages repository settings. To do that, you need to use the template HTML / CSS from the theme baked into GitHub Pages by copying default.html according to this article on GitHub Help.\nOnce you have your theme working, and you have a working website that you can reach, and your content is showing up the way you expect, you can begin customizing and adding in plugins, custom CSS, etc.\nOne final issue I ran into was a dependency issue with the Gemfile.lock vs. the Gemfile when trying to set up this site to use CircleCI (more for fun getting to know CircleCI than anything else). The issue was that when I created a local Jekyll deployment for testing with the same repository as the one I was using for GitHub Pages, I had been making a lot of ignorant changes to my Gemfile. There were conflicting entries between the two and my resolution (since there wasn\u0026rsquo;t much here to break yet) was to rename Gemfile.lock to Gemfile.lock.old left my Gemfile as it was, and then reran bundle install, which will regenerate Gemfile.lock, without any conflicts between them.\nOnce that was ironed out, CircleCI was able to complete without errors, but without a circle.yml, it also didn\u0026rsquo;t have any tests to run in order to say anything about the state of the project. So I put together a basic configuration:\nmachine:ruby:version:2.4.0dependencies:post:-bundleexecjekyllbuildtest:post:-bundleexechtmlproofer./_site--check-html--disable-external The above is basically lifted from the CircleCI page in the Jekyll docs, plus specifying the ruby version that\u0026rsquo;s currently being used by GitHub Pages.\nAnother issue that I ran into trying to get this page to build on CircleCI was this Jekyll issue (#2938). So I had to add \u0026ldquo;vendor\u0026rdquo; to the exclude list - this was not immediately clear (since I hardly know YAML at all) because it\u0026rsquo;s listed on the fixes as \u0026gt; yml \u0026gt; exclude: [vendor] \u0026gt; But if you want it in a YAML list, it needs to be noted as\nexclude:-vendor-....otherexcludes.... So after all that, do I have a passing build? No.\nhtmlproofer found some \u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt; placeholder tags and failed.\nBut at least now it\u0026rsquo;s failing because of something I wrote, rather than because of a configuration issue!\nSuccess and progress.\n","permalink":"https://chasesawyer.dev/post/2017/05/learning-jekyll-and-getting-started-with-github-pages/","tags":["jekyll","update","learning"],"title":"Learning Jekyll and Getting Started with Github Pages"},{"categories":null,"contents":"We have a hammer drill that has a vacuum collector attachment, and we wanted a collector that could use different kinds of filters. I took the existing dust collector and designed an exact replacement that would take any kind of 3M respirator filter as the filter element. This allows using different kinds of filters and for the filters to be replaced. The parts involved were manually reverse-engineered on paper with the device and a set of good digital calipers. The replacement parts were designed in Autodesk Fusion360 and 3D printed on a Prusa i3 MK2.\nReverse Engineering a Device Digital calipers are infinitely helpful when trying to reverse engineer or copy a physical object with 3D CAD and 3D Printing. Dimension all the parts on a drawing and then use those measurements to create the 3D model in CAD. This was a challenging project, and one of the most complex models that I\u0026rsquo;ve ever had to engineer. The result was a device that was a perfect 1:1 fit in the overall vacuum body.\nChallenges The original filter device didn\u0026rsquo;t need to have any moving parts, and the filter was build into the wall between the chamber and the outlet. For my design I had to get a round filter to attach to a rectangular shaft, and also position the connection point somewhere where the filter could be folded around into the rectangular space of the dust chamber. Using a lofted profile allowed me to blend the rectangular outlet port into a circular tube to connect with the filter body. This worked well and maintained equal or greater volume between the filter connection point and the outlet port (no pinch point for the airflow).\nThe other challenge was designing a sliding joint that would hopefully be tight enough to seal, keeping the dust contained within the chamber and maintaining vacuum pressure. It took several test prints with small cut-away sections of the sliding joint between the two halves to get the tolerances just right to be tight without being loose or impossible to slide. This was definitely helped by making sure that the 3D printer was well calibrated and consistent throughout the entire print.\nDrawings    Reference measurements taken from the original device\n      Reference measurements taken from a sample filter scan\n    Results Below is a rendering of the 3D model from Fusion360. Unfortunately I neglected to take any photos of the finished 3D print.\n    ","permalink":"https://chasesawyer.dev/project/dustcollector/","tags":["project","3d printing","uw","fusion360"],"title":"Dust Collector - UW"},{"categories":null,"contents":" Summary   Hello   I\u0026rsquo;m a software engineer living in Seattle, Washington, currently working for Academic Technologies at the University of Washington.\nMy interests span across industries and disciplines, from art to computer science, and games to business intelligence. I\u0026rsquo;ve built my skill set from a position of curiosity, using an analytical approach to problems and their possible solutions, always learning whenever I have the opportunity. I love to learn. I solve difficult problems by combining my learning and experience into novel approaches and solutions.\nI\u0026rsquo;m really interested in security, privacy, and ensuring that data is protected. Encryption is fascinating and I think that it should be accessible and usable by everyone to protect themselves and their data. I also love diving into data and using it to build automation and business intelligence tools.\nðŸ›  Skills Languages, Tools, \u0026amp; Operating Systems Python   Java  Javascript  Node   git  virtualenv  Linux  Windows  Arduino    Databases MySQL  MongoDB   SQL Server  PostgreSQL   GraphQL    Frameworks Vue   React   Django    BI Tools Tableau   Knime    Architecture Serverless   IP Networking  RESTful APIs  Docker    Cloud Platforms \u0026amp; Technologies AWS   Google Cloud Platform   Heroku   Continuous Integration / Continuous Delivery    Experience  Senior Computer Specialist  University of Washington   May 2016 - Present  Seattle, WA     Create business intelligence solutions and tools, informing service improvement. Develop software tools to automate processes, saving hundreds of hours of staff time. Manage communications and outreach campaigns with wide customer base. Manage and maintain 400+ desktop and laptop computers.     Program Support Supervisor  University of Washington   Aug 2013 - May 2016  Seattle, WA     Managed a team of help desk analysts, including hiring and training. Performed Tier 2 and 3 support and maintenance of HD AV Systems and infrastructure while providing excellent customer support and service. Certified in Biamp Audio DSPs and Crestron DigitalMedia.     Lead Help Desk Analyst  University of Washington   Apr 2009 - Jun 2013  Seattle, WA         Education  BFA, 3D4M  University of Washington School of Art   2008 - 2013  Seattle, WA     Undergraduate degree culminating in a synthesis of computer science and art in the form of live drawing robots within the gallery space and finished works displayed in the gallery. Coursework involved (but was not limited to) CS fundamentals, data structures and algorithms, networking, databases, studio art, sculpture, and acoustics.     Certificates  APIs and Microservices  Free Code Camp   2018       Experience building APIs and Microservices. Check out my samples and projects sections for examples.  Free Code Camp Certificate    Additional Details Check me out on Linkedin\u0026nbsp;\u0026nbsp;\n","permalink":"https://chasesawyer.dev/page/about/","tags":null,"title":"Chase Sawyer"},{"categories":null,"contents":" This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ` [outputs] home = [\u0026quot;HTML\u0026quot;, \u0026quot;JSON\u0026quot;] \\`\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ` ... \u0026quot;contents\u0026quot;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026quot;tags\u0026quot;:{{ .Params.tags | jsonify }}{{end}}, \u0026quot;categories\u0026quot; : {{ .Params.categories | jsonify }}, ... \\`\nEdit fuse.js options to Search static/js/search.js ` keys: [ \u0026quot;title\u0026quot;, \u0026quot;contents\u0026quot;, \u0026quot;tags\u0026quot;, \u0026quot;categories\u0026quot; ] \\`\n","permalink":"https://chasesawyer.dev/search/","tags":null,"title":"Search Results"}]